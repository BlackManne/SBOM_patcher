{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void core_tmr_drain_state_list(\n\tstruct se_device *dev,\n\tstruct se_cmd *prout_cmd,\n\tstruct se_node_acl *tmr_nacl,\n\tint tas,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_task_list);\n\tstruct se_session *sess;\n\tstruct se_cmd *cmd, *next;\n\tunsigned long flags;\n\tint rc;\n\n\t/*\n\t * Complete outstanding commands with TASK_ABORTED SAM status.\n\t *\n\t * This is following sam4r17, section 5.6 Aborting commands, Table 38\n\t * for TMR LUN_RESET:\n\t *\n\t * a) \"Yes\" indicates that each command that is aborted on an I_T nexus\n\t * other than the one that caused the SCSI device condition is\n\t * completed with TASK ABORTED status, if the TAS bit is set to one in\n\t * the Control mode page (see SPC-4). \"No\" indicates that no status is\n\t * returned for aborted commands.\n\t *\n\t * d) If the logical unit reset is caused by a particular I_T nexus\n\t * (e.g., by a LOGICAL UNIT RESET task management function), then \"yes\"\n\t * (TASK_ABORTED status) applies.\n\t *\n\t * Otherwise (e.g., if triggered by a hard reset), \"no\"\n\t * (no TASK_ABORTED SAM status) applies.\n\t *\n\t * Note that this seems to be independent of TAS (Task Aborted Status)\n\t * in the Control Mode Page.\n\t */\n\tspin_lock_irqsave(&dev->execute_task_lock, flags);\n\tlist_for_each_entry_safe(cmd, next, &dev->state_list, state_list) {\n\t\t/*\n\t\t * For PREEMPT_AND_ABORT usage, only process commands\n\t\t * with a matching reservation key.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Not aborting PROUT PREEMPT_AND_ABORT CDB..\n\t\t */\n\t\tif (prout_cmd == cmd)\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\trc = __target_check_io_state(cmd);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc)\n\t\t\tcontinue;\n\n\t\tlist_move_tail(&cmd->state_list, &drain_task_list);\n\t\tcmd->state_active = false;\n\t}\n\tspin_unlock_irqrestore(&dev->execute_task_lock, flags);\n\n\twhile (!list_empty(&drain_task_list)) {\n\t\tcmd = list_entry(drain_task_list.next, struct se_cmd, state_list);\n\t\tlist_del_init(&cmd->state_list);\n\n\t\tpr_debug(\"LUN_RESET: %s cmd: %p\"\n\t\t\t\" ITT/CmdSN: 0x%08llx/0x%08x, i_state: %d, t_state: %d\"\n\t\t\t\"cdb: 0x%02x\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", cmd,\n\t\t\tcmd->tag, 0,\n\t\t\tcmd->se_tfo->get_cmd_state(cmd), cmd->t_state,\n\t\t\tcmd->t_task_cdb[0]);\n\t\tpr_debug(\"LUN_RESET: ITT[0x%08llx] - pr_res_key: 0x%016Lx\"\n\t\t\t\" -- CMD_T_ACTIVE: %d\"\n\t\t\t\" CMD_T_STOP: %d CMD_T_SENT: %d\\n\",\n\t\t\tcmd->tag, cmd->pr_res_key,\n\t\t\t(cmd->transport_state & CMD_T_ACTIVE) != 0,\n\t\t\t(cmd->transport_state & CMD_T_STOP) != 0,\n\t\t\t(cmd->transport_state & CMD_T_SENT) != 0);\n\n\t\t/*\n\t\t * If the command may be queued onto a workqueue cancel it now.\n\t\t *\n\t\t * This is equivalent to removal from the execute queue in the\n\t\t * loop above, but we do it down here given that\n\t\t * cancel_work_sync may block.\n\t\t */\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\tcore_tmr_handle_tas_abort(tmr_nacl, cmd, tas);\n\t\ttarget_put_sess_cmd(cmd);\n\t}\n}\n\nstatic void core_tmr_drain_tmr_list(\n\tstruct se_device *dev,\n\tstruct se_tmr_req *tmr,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_tmr_list);\n\tstruct se_session *sess;\n\tstruct se_tmr_req *tmr_p, *tmr_pp;\n\tstruct se_cmd *cmd;\n\tunsigned long flags;\n\tbool rc;\n\t/*\n\t * Release all pending and outgoing TMRs aside from the received\n\t * LUN_RESET tmr..\n\t */\n\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &dev->dev_tmr_list, tmr_list) {\n\t\t/*\n\t\t * Allow the received TMR to return with FUNCTION_COMPLETE.\n\t\t */\n\t\tif (tmr_p == tmr)\n\t\t\tcontinue;\n\n\t\tcmd = tmr_p->task_cmd;\n\t\tif (!cmd) {\n\t\t\tpr_err(\"Unable to locate struct se_cmd for TMR\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * If this function was called with a valid pr_res_key\n\t\t * parameter (eg: for PROUT PREEMPT_AND_ABORT service action\n\t\t * skip non registration key matching TMRs.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\tspin_lock(&cmd->t_state_lock);\n\t\tif (!(cmd->transport_state & CMD_T_ACTIVE)) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (cmd->t_state == TRANSPORT_ISTATE_PROCESSING) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tcmd->transport_state |= CMD_T_ABORTED;\n\t\tspin_unlock(&cmd->t_state_lock);\n\n\t\trc = kref_get_unless_zero(&cmd->cmd_kref);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc) {\n\t\t\tprintk(\"LUN_RESET TMR: non-zero kref_get_unless_zero\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tlist_move_tail(&tmr_p->tmr_list, &drain_tmr_list);\n\t}\n\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &drain_tmr_list, tmr_list) {\n\t\tlist_del_init(&tmr_p->tmr_list);\n\t\tcmd = tmr_p->task_cmd;\n\n\t\tpr_debug(\"LUN_RESET: %s releasing TMR %p Function: 0x%02x,\"\n\t\t\t\" Response: 0x%02x, t_state: %d\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", tmr_p,\n\t\t\ttmr_p->function, tmr_p->response, cmd->t_state);\n\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\ttransport_cmd_finish_abort(cmd, 1);\n\t\ttarget_put_sess_cmd(cmd);\n\t}\n}\n\nstatic void core_tmr_handle_tas_abort(\n\tstruct se_node_acl *tmr_nacl,\n\tstruct se_cmd *cmd,\n\tint tas)\n{\n\tbool remove = true;\n\t/*\n\t * TASK ABORTED status (TAS) bit support\n\t */\n\tif ((tmr_nacl && (tmr_nacl != cmd->se_sess->se_node_acl)) && tas) {\n\t\tremove = false;\n\t\ttransport_send_task_abort(cmd);\n\t}\n\n\ttransport_cmd_finish_abort(cmd, remove);\n}\n\nint core_tmr_lun_reset(\n        struct se_device *dev,\n        struct se_tmr_req *tmr,\n        struct list_head *preempt_and_abort_list,\n        struct se_cmd *prout_cmd)\n{\n\tstruct se_node_acl *tmr_nacl = NULL;\n\tstruct se_portal_group *tmr_tpg = NULL;\n\tint tas;\n        /*\n\t * TASK_ABORTED status bit, this is configurable via ConfigFS\n\t * struct se_device attributes.  spc4r17 section 7.4.6 Control mode page\n\t *\n\t * A task aborted status (TAS) bit set to zero specifies that aborted\n\t * tasks shall be terminated by the device server without any response\n\t * to the application client. A TAS bit set to one specifies that tasks\n\t * aborted by the actions of an I_T nexus other than the I_T nexus on\n\t * which the command was received shall be completed with TASK ABORTED\n\t * status (see SAM-4).\n\t */\n\ttas = dev->dev_attrib.emulate_tas;\n\t/*\n\t * Determine if this se_tmr is coming from a $FABRIC_MOD\n\t * or struct se_device passthrough..\n\t */\n\tif (tmr && tmr->task_cmd && tmr->task_cmd->se_sess) {\n\t\ttmr_nacl = tmr->task_cmd->se_sess->se_node_acl;\n\t\ttmr_tpg = tmr->task_cmd->se_sess->se_tpg;\n\t\tif (tmr_nacl && tmr_tpg) {\n\t\t\tpr_debug(\"LUN_RESET: TMR caller fabric: %s\"\n\t\t\t\t\" initiator port %s\\n\",\n\t\t\t\ttmr_tpg->se_tpg_tfo->get_fabric_name(),\n\t\t\t\ttmr_nacl->initiatorname);\n\t\t}\n\t}\n\tpr_debug(\"LUN_RESET: %s starting for [%s], tas: %d\\n\",\n\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\tdev->transport->name, tas);\n\n\tcore_tmr_drain_tmr_list(dev, tmr, preempt_and_abort_list);\n\tcore_tmr_drain_state_list(dev, prout_cmd, tmr_nacl, tas,\n\t\t\t\tpreempt_and_abort_list);\n\n\t/*\n\t * Clear any legacy SPC-2 reservation when called during\n\t * LOGICAL UNIT RESET\n\t */\n\tif (!preempt_and_abort_list &&\n\t     (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)) {\n\t\tspin_lock(&dev->dev_reservation_lock);\n\t\tdev->dev_reserved_node_acl = NULL;\n\t\tdev->dev_reservation_flags &= ~DRF_SPC2_RESERVATIONS;\n\t\tspin_unlock(&dev->dev_reservation_lock);\n\t\tpr_debug(\"LUN_RESET: SCSI-2 Released reservation\\n\");\n\t}\n\n\tatomic_long_inc(&dev->num_resets);\n\n\tpr_debug(\"LUN_RESET: %s for [%s] Complete\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\t\tdev->transport->name);\n\treturn 0;\n}\n\nvoid core_tmr_release_req(struct se_tmr_req *tmr)\n{\n\tstruct se_device *dev = tmr->tmr_dev;\n\tunsigned long flags;\n\n\tif (dev) {\n\t\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\t\tlist_del_init(&tmr->tmr_list);\n\t\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\t}\n\n\tkfree(tmr);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void core_tmr_drain_state_list(\n\tstruct se_device *dev,\n\tstruct se_cmd *prout_cmd,\n\tstruct se_session *tmr_sess,\n\tint tas,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_task_list);\n\tstruct se_session *sess;\n\tstruct se_cmd *cmd, *next;\n\tunsigned long flags;\n\tint rc;\n\n\t/*\n\t * Complete outstanding commands with TASK_ABORTED SAM status.\n\t *\n\t * This is following sam4r17, section 5.6 Aborting commands, Table 38\n\t * for TMR LUN_RESET:\n\t *\n\t * a) \"Yes\" indicates that each command that is aborted on an I_T nexus\n\t * other than the one that caused the SCSI device condition is\n\t * completed with TASK ABORTED status, if the TAS bit is set to one in\n\t * the Control mode page (see SPC-4). \"No\" indicates that no status is\n\t * returned for aborted commands.\n\t *\n\t * d) If the logical unit reset is caused by a particular I_T nexus\n\t * (e.g., by a LOGICAL UNIT RESET task management function), then \"yes\"\n\t * (TASK_ABORTED status) applies.\n\t *\n\t * Otherwise (e.g., if triggered by a hard reset), \"no\"\n\t * (no TASK_ABORTED SAM status) applies.\n\t *\n\t * Note that this seems to be independent of TAS (Task Aborted Status)\n\t * in the Control Mode Page.\n\t */\n\tspin_lock_irqsave(&dev->execute_task_lock, flags);\n\tlist_for_each_entry_safe(cmd, next, &dev->state_list, state_list) {\n\t\t/*\n\t\t * For PREEMPT_AND_ABORT usage, only process commands\n\t\t * with a matching reservation key.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Not aborting PROUT PREEMPT_AND_ABORT CDB..\n\t\t */\n\t\tif (prout_cmd == cmd)\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\trc = __target_check_io_state(cmd);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc)\n\t\t\tcontinue;\n\n\t\tlist_move_tail(&cmd->state_list, &drain_task_list);\n\t\tcmd->state_active = false;\n\t}\n\tspin_unlock_irqrestore(&dev->execute_task_lock, flags);\n\n\twhile (!list_empty(&drain_task_list)) {\n\t\tcmd = list_entry(drain_task_list.next, struct se_cmd, state_list);\n\t\tlist_del_init(&cmd->state_list);\n\n\t\tpr_debug(\"LUN_RESET: %s cmd: %p\"\n\t\t\t\" ITT/CmdSN: 0x%08llx/0x%08x, i_state: %d, t_state: %d\"\n\t\t\t\"cdb: 0x%02x\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", cmd,\n\t\t\tcmd->tag, 0,\n\t\t\tcmd->se_tfo->get_cmd_state(cmd), cmd->t_state,\n\t\t\tcmd->t_task_cdb[0]);\n\t\tpr_debug(\"LUN_RESET: ITT[0x%08llx] - pr_res_key: 0x%016Lx\"\n\t\t\t\" -- CMD_T_ACTIVE: %d\"\n\t\t\t\" CMD_T_STOP: %d CMD_T_SENT: %d\\n\",\n\t\t\tcmd->tag, cmd->pr_res_key,\n\t\t\t(cmd->transport_state & CMD_T_ACTIVE) != 0,\n\t\t\t(cmd->transport_state & CMD_T_STOP) != 0,\n\t\t\t(cmd->transport_state & CMD_T_SENT) != 0);\n\n\t\t/*\n\t\t * If the command may be queued onto a workqueue cancel it now.\n\t\t *\n\t\t * This is equivalent to removal from the execute queue in the\n\t\t * loop above, but we do it down here given that\n\t\t * cancel_work_sync may block.\n\t\t */\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\tcore_tmr_handle_tas_abort(tmr_sess, cmd, tas);\n\t\ttarget_put_sess_cmd(cmd);\n\t}\n}\n\nstatic void core_tmr_drain_tmr_list(\n\tstruct se_device *dev,\n\tstruct se_tmr_req *tmr,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_tmr_list);\n\tstruct se_session *sess;\n\tstruct se_tmr_req *tmr_p, *tmr_pp;\n\tstruct se_cmd *cmd;\n\tunsigned long flags;\n\tbool rc;\n\t/*\n\t * Release all pending and outgoing TMRs aside from the received\n\t * LUN_RESET tmr..\n\t */\n\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &dev->dev_tmr_list, tmr_list) {\n\t\t/*\n\t\t * Allow the received TMR to return with FUNCTION_COMPLETE.\n\t\t */\n\t\tif (tmr_p == tmr)\n\t\t\tcontinue;\n\n\t\tcmd = tmr_p->task_cmd;\n\t\tif (!cmd) {\n\t\t\tpr_err(\"Unable to locate struct se_cmd for TMR\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * If this function was called with a valid pr_res_key\n\t\t * parameter (eg: for PROUT PREEMPT_AND_ABORT service action\n\t\t * skip non registration key matching TMRs.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\tspin_lock(&cmd->t_state_lock);\n\t\tif (!(cmd->transport_state & CMD_T_ACTIVE)) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (cmd->t_state == TRANSPORT_ISTATE_PROCESSING) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tcmd->transport_state |= CMD_T_ABORTED;\n\t\tspin_unlock(&cmd->t_state_lock);\n\n\t\trc = kref_get_unless_zero(&cmd->cmd_kref);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc) {\n\t\t\tprintk(\"LUN_RESET TMR: non-zero kref_get_unless_zero\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tlist_move_tail(&tmr_p->tmr_list, &drain_tmr_list);\n\t}\n\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &drain_tmr_list, tmr_list) {\n\t\tlist_del_init(&tmr_p->tmr_list);\n\t\tcmd = tmr_p->task_cmd;\n\n\t\tpr_debug(\"LUN_RESET: %s releasing TMR %p Function: 0x%02x,\"\n\t\t\t\" Response: 0x%02x, t_state: %d\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", tmr_p,\n\t\t\ttmr_p->function, tmr_p->response, cmd->t_state);\n\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\ttransport_cmd_finish_abort(cmd, 1);\n\t\ttarget_put_sess_cmd(cmd);\n\t}\n}\n\nstatic void core_tmr_handle_tas_abort(\n\tstruct se_session *tmr_sess,\n\tstruct se_cmd *cmd,\n\tint tas)\n{\n\tbool remove = true;\n\t/*\n\t * TASK ABORTED status (TAS) bit support\n\t */\n\tif (tmr_sess && tmr_sess != cmd->se_sess && tas) {\n\t\tremove = false;\n\t\ttransport_send_task_abort(cmd);\n\t}\n\n\ttransport_cmd_finish_abort(cmd, remove);\n}\n\nint core_tmr_lun_reset(\n        struct se_device *dev,\n        struct se_tmr_req *tmr,\n        struct list_head *preempt_and_abort_list,\n        struct se_cmd *prout_cmd)\n{\n\tstruct se_node_acl *tmr_nacl = NULL;\n\tstruct se_portal_group *tmr_tpg = NULL;\n\tstruct se_session *tmr_sess = NULL;\n\tint tas;\n        /*\n\t * TASK_ABORTED status bit, this is configurable via ConfigFS\n\t * struct se_device attributes.  spc4r17 section 7.4.6 Control mode page\n\t *\n\t * A task aborted status (TAS) bit set to zero specifies that aborted\n\t * tasks shall be terminated by the device server without any response\n\t * to the application client. A TAS bit set to one specifies that tasks\n\t * aborted by the actions of an I_T nexus other than the I_T nexus on\n\t * which the command was received shall be completed with TASK ABORTED\n\t * status (see SAM-4).\n\t */\n\ttas = dev->dev_attrib.emulate_tas;\n\t/*\n\t * Determine if this se_tmr is coming from a $FABRIC_MOD\n\t * or struct se_device passthrough..\n\t */\n\tif (tmr && tmr->task_cmd && tmr->task_cmd->se_sess) {\n\t\ttmr_sess = tmr->task_cmd->se_sess;\n\t\ttmr_nacl = tmr_sess->se_node_acl;\n\t\ttmr_tpg = tmr_sess->se_tpg;\n\t\tif (tmr_nacl && tmr_tpg) {\n\t\t\tpr_debug(\"LUN_RESET: TMR caller fabric: %s\"\n\t\t\t\t\" initiator port %s\\n\",\n\t\t\t\ttmr_tpg->se_tpg_tfo->get_fabric_name(),\n\t\t\t\ttmr_nacl->initiatorname);\n\t\t}\n\t}\n\tpr_debug(\"LUN_RESET: %s starting for [%s], tas: %d\\n\",\n\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\tdev->transport->name, tas);\n\n\tcore_tmr_drain_tmr_list(dev, tmr, preempt_and_abort_list);\n\tcore_tmr_drain_state_list(dev, prout_cmd, tmr_sess, tas,\n\t\t\t\tpreempt_and_abort_list);\n\n\t/*\n\t * Clear any legacy SPC-2 reservation when called during\n\t * LOGICAL UNIT RESET\n\t */\n\tif (!preempt_and_abort_list &&\n\t     (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)) {\n\t\tspin_lock(&dev->dev_reservation_lock);\n\t\tdev->dev_reserved_node_acl = NULL;\n\t\tdev->dev_reservation_flags &= ~DRF_SPC2_RESERVATIONS;\n\t\tspin_unlock(&dev->dev_reservation_lock);\n\t\tpr_debug(\"LUN_RESET: SCSI-2 Released reservation\\n\");\n\t}\n\n\tatomic_long_inc(&dev->num_resets);\n\n\tpr_debug(\"LUN_RESET: %s for [%s] Complete\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\t\tdev->transport->name);\n\treturn 0;\n}\n\nvoid core_tmr_release_req(struct se_tmr_req *tmr)\n{\n\tstruct se_device *dev = tmr->tmr_dev;\n\tunsigned long flags;\n\n\tif (dev) {\n\t\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\t\tlist_del_init(&tmr->tmr_list);\n\t\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\t}\n\n\tkfree(tmr);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void core_tmr_drain_state_list(\n\tstruct se_device *dev,\n\tstruct se_cmd *prout_cmd,\n\tstruct se_node_acl *tmr_nacl,\n\tint tas,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_task_list);\n\tstruct se_session *sess;\n\tstruct se_cmd *cmd, *next;\n\tunsigned long flags;\n\tint rc;\n\n\t/*\n\t * Complete outstanding commands with TASK_ABORTED SAM status.\n\t *\n\t * This is following sam4r17, section 5.6 Aborting commands, Table 38\n\t * for TMR LUN_RESET:\n\t *\n\t * a) \"Yes\" indicates that each command that is aborted on an I_T nexus\n\t * other than the one that caused the SCSI device condition is\n\t * completed with TASK ABORTED status, if the TAS bit is set to one in\n\t * the Control mode page (see SPC-4). \"No\" indicates that no status is\n\t * returned for aborted commands.\n\t *\n\t * d) If the logical unit reset is caused by a particular I_T nexus\n\t * (e.g., by a LOGICAL UNIT RESET task management function), then \"yes\"\n\t * (TASK_ABORTED status) applies.\n\t *\n\t * Otherwise (e.g., if triggered by a hard reset), \"no\"\n\t * (no TASK_ABORTED SAM status) applies.\n\t *\n\t * Note that this seems to be independent of TAS (Task Aborted Status)\n\t * in the Control Mode Page.\n\t */\n\tspin_lock_irqsave(&dev->execute_task_lock, flags);\n\tlist_for_each_entry_safe(cmd, next, &dev->state_list, state_list) {\n\t\t/*\n\t\t * For PREEMPT_AND_ABORT usage, only process commands\n\t\t * with a matching reservation key.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Not aborting PROUT PREEMPT_AND_ABORT CDB..\n\t\t */\n\t\tif (prout_cmd == cmd)\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\trc = __target_check_io_state(cmd);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc)\n\t\t\tcontinue;\n\n\t\tlist_move_tail(&cmd->state_list, &drain_task_list);\n\t\tcmd->state_active = false;\n\t}\n\tspin_unlock_irqrestore(&dev->execute_task_lock, flags);\n\n\twhile (!list_empty(&drain_task_list)) {\n\t\tcmd = list_entry(drain_task_list.next, struct se_cmd, state_list);\n\t\tlist_del_init(&cmd->state_list);\n\n\t\tpr_debug(\"LUN_RESET: %s cmd: %p\"\n\t\t\t\" ITT/CmdSN: 0x%08x/0x%08x, i_state: %d, t_state: %d\"\n\t\t\t\"cdb: 0x%02x\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", cmd,\n\t\t\tcmd->se_tfo->get_task_tag(cmd), 0,\n\t\t\tcmd->se_tfo->get_cmd_state(cmd), cmd->t_state,\n\t\t\tcmd->t_task_cdb[0]);\n\t\tpr_debug(\"LUN_RESET: ITT[0x%08x] - pr_res_key: 0x%016Lx\"\n\t\t\t\" -- CMD_T_ACTIVE: %d\"\n\t\t\t\" CMD_T_STOP: %d CMD_T_SENT: %d\\n\",\n\t\t\tcmd->se_tfo->get_task_tag(cmd), cmd->pr_res_key,\n\t\t\t(cmd->transport_state & CMD_T_ACTIVE) != 0,\n\t\t\t(cmd->transport_state & CMD_T_STOP) != 0,\n\t\t\t(cmd->transport_state & CMD_T_SENT) != 0);\n\n\t\t/*\n\t\t * If the command may be queued onto a workqueue cancel it now.\n\t\t *\n\t\t * This is equivalent to removal from the execute queue in the\n\t\t * loop above, but we do it down here given that\n\t\t * cancel_work_sync may block.\n\t\t */\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\tcore_tmr_handle_tas_abort(tmr_nacl, cmd, tas);\n\t\ttarget_put_sess_cmd(cmd->se_sess, cmd);\n\t}\n}\n\nstatic void core_tmr_drain_tmr_list(\n\tstruct se_device *dev,\n\tstruct se_tmr_req *tmr,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_tmr_list);\n\tstruct se_session *sess;\n\tstruct se_tmr_req *tmr_p, *tmr_pp;\n\tstruct se_cmd *cmd;\n\tunsigned long flags;\n\tbool rc;\n\t/*\n\t * Release all pending and outgoing TMRs aside from the received\n\t * LUN_RESET tmr..\n\t */\n\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &dev->dev_tmr_list, tmr_list) {\n\t\t/*\n\t\t * Allow the received TMR to return with FUNCTION_COMPLETE.\n\t\t */\n\t\tif (tmr_p == tmr)\n\t\t\tcontinue;\n\n\t\tcmd = tmr_p->task_cmd;\n\t\tif (!cmd) {\n\t\t\tpr_err(\"Unable to locate struct se_cmd for TMR\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * If this function was called with a valid pr_res_key\n\t\t * parameter (eg: for PROUT PREEMPT_AND_ABORT service action\n\t\t * skip non regisration key matching TMRs.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\tspin_lock(&cmd->t_state_lock);\n\t\tif (!(cmd->transport_state & CMD_T_ACTIVE)) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (cmd->t_state == TRANSPORT_ISTATE_PROCESSING) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tcmd->transport_state |= CMD_T_ABORTED;\n\t\tspin_unlock(&cmd->t_state_lock);\n\n\t\trc = kref_get_unless_zero(&cmd->cmd_kref);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc) {\n\t\t\tprintk(\"LUN_RESET TMR: non-zero kref_get_unless_zero\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tlist_move_tail(&tmr_p->tmr_list, &drain_tmr_list);\n\t}\n\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &drain_tmr_list, tmr_list) {\n\t\tlist_del_init(&tmr_p->tmr_list);\n\t\tcmd = tmr_p->task_cmd;\n\n\t\tpr_debug(\"LUN_RESET: %s releasing TMR %p Function: 0x%02x,\"\n\t\t\t\" Response: 0x%02x, t_state: %d\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", tmr_p,\n\t\t\ttmr_p->function, tmr_p->response, cmd->t_state);\n\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\ttransport_cmd_finish_abort(cmd, 1);\n\t\ttarget_put_sess_cmd(cmd->se_sess, cmd);\n\t}\n}\n\nstatic void core_tmr_handle_tas_abort(\n\tstruct se_node_acl *tmr_nacl,\n\tstruct se_cmd *cmd,\n\tint tas)\n{\n\tbool remove = true;\n\t/*\n\t * TASK ABORTED status (TAS) bit support\n\t*/\n\tif ((tmr_nacl &&\n\t     (tmr_nacl != cmd->se_sess->se_node_acl)) && tas) {\n\t\tremove = false;\n\t\ttransport_send_task_abort(cmd);\n\t}\n\n\ttransport_cmd_finish_abort(cmd, remove);\n}\n\nint core_tmr_lun_reset(\n        struct se_device *dev,\n        struct se_tmr_req *tmr,\n        struct list_head *preempt_and_abort_list,\n        struct se_cmd *prout_cmd)\n{\n\tstruct se_node_acl *tmr_nacl = NULL;\n\tstruct se_portal_group *tmr_tpg = NULL;\n\tint tas;\n        /*\n\t * TASK_ABORTED status bit, this is configurable via ConfigFS\n\t * struct se_device attributes.  spc4r17 section 7.4.6 Control mode page\n\t *\n\t * A task aborted status (TAS) bit set to zero specifies that aborted\n\t * tasks shall be terminated by the device server without any response\n\t * to the application client. A TAS bit set to one specifies that tasks\n\t * aborted by the actions of an I_T nexus other than the I_T nexus on\n\t * which the command was received shall be completed with TASK ABORTED\n\t * status (see SAM-4).\n\t */\n\ttas = dev->dev_attrib.emulate_tas;\n\t/*\n\t * Determine if this se_tmr is coming from a $FABRIC_MOD\n\t * or struct se_device passthrough..\n\t */\n\tif (tmr && tmr->task_cmd && tmr->task_cmd->se_sess) {\n\t\ttmr_nacl = tmr->task_cmd->se_sess->se_node_acl;\n\t\ttmr_tpg = tmr->task_cmd->se_sess->se_tpg;\n\t\tif (tmr_nacl && tmr_tpg) {\n\t\t\tpr_debug(\"LUN_RESET: TMR caller fabric: %s\"\n\t\t\t\t\" initiator port %s\\n\",\n\t\t\t\ttmr_tpg->se_tpg_tfo->get_fabric_name(),\n\t\t\t\ttmr_nacl->initiatorname);\n\t\t}\n\t}\n\tpr_debug(\"LUN_RESET: %s starting for [%s], tas: %d\\n\",\n\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\tdev->transport->name, tas);\n\n\tcore_tmr_drain_tmr_list(dev, tmr, preempt_and_abort_list);\n\tcore_tmr_drain_state_list(dev, prout_cmd, tmr_nacl, tas,\n\t\t\t\tpreempt_and_abort_list);\n\n\t/*\n\t * Clear any legacy SPC-2 reservation when called during\n\t * LOGICAL UNIT RESET\n\t */\n\tif (!preempt_and_abort_list &&\n\t     (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)) {\n\t\tspin_lock(&dev->dev_reservation_lock);\n\t\tdev->dev_reserved_node_acl = NULL;\n\t\tdev->dev_reservation_flags &= ~DRF_SPC2_RESERVATIONS;\n\t\tspin_unlock(&dev->dev_reservation_lock);\n\t\tpr_debug(\"LUN_RESET: SCSI-2 Released reservation\\n\");\n\t}\n\n\tatomic_long_inc(&dev->num_resets);\n\n\tpr_debug(\"LUN_RESET: %s for [%s] Complete\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\t\tdev->transport->name);\n\treturn 0;\n}\n\nvoid core_tmr_release_req(\n\tstruct se_tmr_req *tmr)\n{\n\tstruct se_device *dev = tmr->tmr_dev;\n\tunsigned long flags;\n\n\tif (!dev) {\n\t\tkfree(tmr);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\tlist_del_init(&tmr->tmr_list);\n\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\n\tkfree(tmr);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void core_tmr_drain_state_list(\n\tstruct se_device *dev,\n\tstruct se_cmd *prout_cmd,\n\tstruct se_session *tmr_sess,\n\tint tas,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_task_list);\n\tstruct se_session *sess;\n\tstruct se_cmd *cmd, *next;\n\tunsigned long flags;\n\tint rc;\n\n\t/*\n\t * Complete outstanding commands with TASK_ABORTED SAM status.\n\t *\n\t * This is following sam4r17, section 5.6 Aborting commands, Table 38\n\t * for TMR LUN_RESET:\n\t *\n\t * a) \"Yes\" indicates that each command that is aborted on an I_T nexus\n\t * other than the one that caused the SCSI device condition is\n\t * completed with TASK ABORTED status, if the TAS bit is set to one in\n\t * the Control mode page (see SPC-4). \"No\" indicates that no status is\n\t * returned for aborted commands.\n\t *\n\t * d) If the logical unit reset is caused by a particular I_T nexus\n\t * (e.g., by a LOGICAL UNIT RESET task management function), then \"yes\"\n\t * (TASK_ABORTED status) applies.\n\t *\n\t * Otherwise (e.g., if triggered by a hard reset), \"no\"\n\t * (no TASK_ABORTED SAM status) applies.\n\t *\n\t * Note that this seems to be independent of TAS (Task Aborted Status)\n\t * in the Control Mode Page.\n\t */\n\tspin_lock_irqsave(&dev->execute_task_lock, flags);\n\tlist_for_each_entry_safe(cmd, next, &dev->state_list, state_list) {\n\t\t/*\n\t\t * For PREEMPT_AND_ABORT usage, only process commands\n\t\t * with a matching reservation key.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Not aborting PROUT PREEMPT_AND_ABORT CDB..\n\t\t */\n\t\tif (prout_cmd == cmd)\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\trc = __target_check_io_state(cmd);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc)\n\t\t\tcontinue;\n\n\t\tlist_move_tail(&cmd->state_list, &drain_task_list);\n\t\tcmd->state_active = false;\n\t}\n\tspin_unlock_irqrestore(&dev->execute_task_lock, flags);\n\n\twhile (!list_empty(&drain_task_list)) {\n\t\tcmd = list_entry(drain_task_list.next, struct se_cmd, state_list);\n\t\tlist_del_init(&cmd->state_list);\n\n\t\tpr_debug(\"LUN_RESET: %s cmd: %p\"\n\t\t\t\" ITT/CmdSN: 0x%08x/0x%08x, i_state: %d, t_state: %d\"\n\t\t\t\"cdb: 0x%02x\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", cmd,\n\t\t\tcmd->se_tfo->get_task_tag(cmd), 0,\n\t\t\tcmd->se_tfo->get_cmd_state(cmd), cmd->t_state,\n\t\t\tcmd->t_task_cdb[0]);\n\t\tpr_debug(\"LUN_RESET: ITT[0x%08x] - pr_res_key: 0x%016Lx\"\n\t\t\t\" -- CMD_T_ACTIVE: %d\"\n\t\t\t\" CMD_T_STOP: %d CMD_T_SENT: %d\\n\",\n\t\t\tcmd->se_tfo->get_task_tag(cmd), cmd->pr_res_key,\n\t\t\t(cmd->transport_state & CMD_T_ACTIVE) != 0,\n\t\t\t(cmd->transport_state & CMD_T_STOP) != 0,\n\t\t\t(cmd->transport_state & CMD_T_SENT) != 0);\n\n\t\t/*\n\t\t * If the command may be queued onto a workqueue cancel it now.\n\t\t *\n\t\t * This is equivalent to removal from the execute queue in the\n\t\t * loop above, but we do it down here given that\n\t\t * cancel_work_sync may block.\n\t\t */\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\tcore_tmr_handle_tas_abort(tmr_sess, cmd, tas);\n\t\ttarget_put_sess_cmd(cmd->se_sess, cmd);\n\t}\n}\n\nstatic void core_tmr_drain_tmr_list(\n\tstruct se_device *dev,\n\tstruct se_tmr_req *tmr,\n\tstruct list_head *preempt_and_abort_list)\n{\n\tLIST_HEAD(drain_tmr_list);\n\tstruct se_session *sess;\n\tstruct se_tmr_req *tmr_p, *tmr_pp;\n\tstruct se_cmd *cmd;\n\tunsigned long flags;\n\tbool rc;\n\t/*\n\t * Release all pending and outgoing TMRs aside from the received\n\t * LUN_RESET tmr..\n\t */\n\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &dev->dev_tmr_list, tmr_list) {\n\t\t/*\n\t\t * Allow the received TMR to return with FUNCTION_COMPLETE.\n\t\t */\n\t\tif (tmr_p == tmr)\n\t\t\tcontinue;\n\n\t\tcmd = tmr_p->task_cmd;\n\t\tif (!cmd) {\n\t\t\tpr_err(\"Unable to locate struct se_cmd for TMR\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * If this function was called with a valid pr_res_key\n\t\t * parameter (eg: for PROUT PREEMPT_AND_ABORT service action\n\t\t * skip non regisration key matching TMRs.\n\t\t */\n\t\tif (target_check_cdb_and_preempt(preempt_and_abort_list, cmd))\n\t\t\tcontinue;\n\n\t\tsess = cmd->se_sess;\n\t\tif (WARN_ON_ONCE(!sess))\n\t\t\tcontinue;\n\n\t\tspin_lock(&sess->sess_cmd_lock);\n\t\tspin_lock(&cmd->t_state_lock);\n\t\tif (!(cmd->transport_state & CMD_T_ACTIVE)) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (cmd->t_state == TRANSPORT_ISTATE_PROCESSING) {\n\t\t\tspin_unlock(&cmd->t_state_lock);\n\t\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tcmd->transport_state |= CMD_T_ABORTED;\n\t\tspin_unlock(&cmd->t_state_lock);\n\n\t\trc = kref_get_unless_zero(&cmd->cmd_kref);\n\t\tspin_unlock(&sess->sess_cmd_lock);\n\t\tif (!rc) {\n\t\t\tprintk(\"LUN_RESET TMR: non-zero kref_get_unless_zero\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\tlist_move_tail(&tmr_p->tmr_list, &drain_tmr_list);\n\t}\n\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\n\tlist_for_each_entry_safe(tmr_p, tmr_pp, &drain_tmr_list, tmr_list) {\n\t\tlist_del_init(&tmr_p->tmr_list);\n\t\tcmd = tmr_p->task_cmd;\n\n\t\tpr_debug(\"LUN_RESET: %s releasing TMR %p Function: 0x%02x,\"\n\t\t\t\" Response: 0x%02x, t_state: %d\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"\", tmr_p,\n\t\t\ttmr_p->function, tmr_p->response, cmd->t_state);\n\n\t\tcancel_work_sync(&cmd->work);\n\t\ttransport_wait_for_tasks(cmd);\n\n\t\ttransport_cmd_finish_abort(cmd, 1);\n\t\ttarget_put_sess_cmd(cmd->se_sess, cmd);\n\t}\n}\n\nstatic void core_tmr_handle_tas_abort(\n\tstruct se_session *tmr_sess,\n\tstruct se_cmd *cmd,\n\tint tas)\n{\n\tbool remove = true;\n\t/*\n\t * TASK ABORTED status (TAS) bit support\n\t*/\n\tif (tmr_sess && tmr_sess != cmd->se_sess && tas) {\n\t\tremove = false;\n\t\ttransport_send_task_abort(cmd);\n\t}\n\n\ttransport_cmd_finish_abort(cmd, remove);\n}\n\nint core_tmr_lun_reset(\n        struct se_device *dev,\n        struct se_tmr_req *tmr,\n        struct list_head *preempt_and_abort_list,\n        struct se_cmd *prout_cmd)\n{\n\tstruct se_node_acl *tmr_nacl = NULL;\n\tstruct se_portal_group *tmr_tpg = NULL;\n\tstruct se_session *tmr_sess = NULL;\n\tint tas;\n        /*\n\t * TASK_ABORTED status bit, this is configurable via ConfigFS\n\t * struct se_device attributes.  spc4r17 section 7.4.6 Control mode page\n\t *\n\t * A task aborted status (TAS) bit set to zero specifies that aborted\n\t * tasks shall be terminated by the device server without any response\n\t * to the application client. A TAS bit set to one specifies that tasks\n\t * aborted by the actions of an I_T nexus other than the I_T nexus on\n\t * which the command was received shall be completed with TASK ABORTED\n\t * status (see SAM-4).\n\t */\n\ttas = dev->dev_attrib.emulate_tas;\n\t/*\n\t * Determine if this se_tmr is coming from a $FABRIC_MOD\n\t * or struct se_device passthrough..\n\t */\n\tif (tmr && tmr->task_cmd && tmr->task_cmd->se_sess) {\n\t\ttmr_sess = tmr->task_cmd->se_sess;\n\t\ttmr_nacl = tmr_sess->se_node_acl;\n\t\ttmr_tpg = tmr_sess->se_tpg;\n\t\tif (tmr_nacl && tmr_tpg) {\n\t\t\tpr_debug(\"LUN_RESET: TMR caller fabric: %s\"\n\t\t\t\t\" initiator port %s\\n\",\n\t\t\t\ttmr_tpg->se_tpg_tfo->get_fabric_name(),\n\t\t\t\ttmr_nacl->initiatorname);\n\t\t}\n\t}\n\tpr_debug(\"LUN_RESET: %s starting for [%s], tas: %d\\n\",\n\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\tdev->transport->name, tas);\n\n\tcore_tmr_drain_tmr_list(dev, tmr, preempt_and_abort_list);\n\tcore_tmr_drain_state_list(dev, prout_cmd, tmr_sess, tas,\n\t\t\t\tpreempt_and_abort_list);\n\n\t/*\n\t * Clear any legacy SPC-2 reservation when called during\n\t * LOGICAL UNIT RESET\n\t */\n\tif (!preempt_and_abort_list &&\n\t     (dev->dev_reservation_flags & DRF_SPC2_RESERVATIONS)) {\n\t\tspin_lock(&dev->dev_reservation_lock);\n\t\tdev->dev_reserved_node_acl = NULL;\n\t\tdev->dev_reservation_flags &= ~DRF_SPC2_RESERVATIONS;\n\t\tspin_unlock(&dev->dev_reservation_lock);\n\t\tpr_debug(\"LUN_RESET: SCSI-2 Released reservation\\n\");\n\t}\n\n\tatomic_long_inc(&dev->num_resets);\n\n\tpr_debug(\"LUN_RESET: %s for [%s] Complete\\n\",\n\t\t\t(preempt_and_abort_list) ? \"Preempt\" : \"TMR\",\n\t\t\tdev->transport->name);\n\treturn 0;\n}\n\nvoid core_tmr_release_req(\n\tstruct se_tmr_req *tmr)\n{\n\tstruct se_device *dev = tmr->tmr_dev;\n\tunsigned long flags;\n\n\tif (!dev) {\n\t\tkfree(tmr);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&dev->se_tmr_lock, flags);\n\tlist_del_init(&tmr->tmr_list);\n\tspin_unlock_irqrestore(&dev->se_tmr_lock, flags);\n\n\tkfree(tmr);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\n#define SDHCI_ARASAN_CLK_CTRL_OFFSET\t0x2c\n\n\n#define CLK_CTRL_TIMEOUT_SHIFT\t\t16\n\n\n#define CLK_CTRL_TIMEOUT_MASK\t\t(0xf << CLK_CTRL_TIMEOUT_SHIFT)\n\n\n#define CLK_CTRL_TIMEOUT_MIN_EXP\t13\n\n\nstatic int sdhci_arasan_syscon_write(struct sdhci_host *host,\n\t\t\t\t   const struct sdhci_arasan_soc_ctl_field *fld,\n\t\t\t\t   u32 val)\n{\n\tstruct sdhci_pltfm_host *pltfm_host = sdhci_priv(host);\n\tstruct sdhci_arasan_data *sdhci_arasan = sdhci_pltfm_priv(pltfm_host);\n\tstruct regmap *soc_ctl_base = sdhci_arasan->soc_ctl_base;\n\tu32 reg = fld->reg;\n\tu16 width = fld->width;\n\ts16 shift = fld->shift;\n\tint ret;\n\n\t/*\n\t * Silently return errors for shift < 0 so caller doesn't have\n\t * to check for fields which are optional.  For fields that\n\t * are required then caller needs to do something special\n\t * anyway.\n\t */\n\tif (shift < 0)\n\t\treturn -EINVAL;\n\n\tif (sdhci_arasan->soc_ctl_map->hiword_update)\n\t\tret = regmap_write(soc_ctl_base, reg,\n\t\t\t\t   HIWORD_UPDATE(val, GENMASK(width, 0),\n\t\t\t\t\t\t shift));\n\telse\n\t\tret = regmap_update_bits(soc_ctl_base, reg,\n\t\t\t\t\t GENMASK(shift + width, shift),\n\t\t\t\t\t val << shift);\n\n\t/* Yell about (unexpected) regmap errors */\n\tif (ret)\n\t\tpr_warn(\"%s: Regmap write fail: %d\\n\",\n\t\t\t mmc_hostname(host->mmc), ret);\n\n\treturn ret;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int sdhci_arasan_syscon_write(struct sdhci_host *host,\n\t\t\t\t   const struct sdhci_arasan_soc_ctl_field *fld,\n\t\t\t\t   u32 val)\n{\n\tstruct sdhci_pltfm_host *pltfm_host = sdhci_priv(host);\n\tstruct sdhci_arasan_data *sdhci_arasan = sdhci_pltfm_priv(pltfm_host);\n\tstruct regmap *soc_ctl_base = sdhci_arasan->soc_ctl_base;\n\tu32 reg = fld->reg;\n\tu16 width = fld->width;\n\ts16 shift = fld->shift;\n\tint ret;\n\n\t/*\n\t * Silently return errors for shift < 0 so caller doesn't have\n\t * to check for fields which are optional.  For fields that\n\t * are required then caller needs to do something special\n\t * anyway.\n\t */\n\tif (shift < 0)\n\t\treturn -EINVAL;\n\n\tif (sdhci_arasan->soc_ctl_map->hiword_update)\n\t\tret = regmap_write(soc_ctl_base, reg,\n\t\t\t\t   HIWORD_UPDATE(val, GENMASK(width, 0),\n\t\t\t\t\t\t shift));\n\telse\n\t\tret = regmap_update_bits(soc_ctl_base, reg,\n\t\t\t\t\t GENMASK(shift + width, shift),\n\t\t\t\t\t val << shift);\n\n\t/* Yell about (unexpected) regmap errors */\n\tif (ret)\n\t\tpr_warn(\"%s: Regmap write fail: %d\\n\",\n\t\t\t mmc_hostname(host->mmc), ret);\n\n\treturn ret;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\n#define SDHCI_ARASAN_CLK_CTRL_OFFSET\t0x2c\n\n\n\n\n#define CLK_CTRL_TIMEOUT_SHIFT\t\t16\n\n\n#define CLK_CTRL_TIMEOUT_MASK\t\t(0xf << CLK_CTRL_TIMEOUT_SHIFT)\n\n\n#define CLK_CTRL_TIMEOUT_MIN_EXP\t13\n\n\n\n\nu32 div;\n\n\ndiv = readl(host->ioaddr + SDHCI_ARASAN_CLK_CTRL_OFFSET);\n\n\ndiv = (div & CLK_CTRL_TIMEOUT_MASK) >> CLK_CTRL_TIMEOUT_SHIFT;\n\n\nfreq = clk_get_rate(pltfm_host->clk);\n\n\nfreq /= 1 << (CLK_CTRL_TIMEOUT_MIN_EXP + div);\n'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\n/* SDHCI timeout clock is in kHz */\n\n\nfreq = DIV_ROUND_UP(clk_get_rate(pltfm_host->clk), 1000);\n\n\n/* or in MHz */\n\n\nif (host->caps & SDHCI_TIMEOUT_CLK_UNIT)\n\n\nfreq = DIV_ROUND_UP(freq, 1000);\n'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void dwc3_prepare_one_trb(struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, dma_addr_t dma,\n\t\tunsigned length, unsigned chain, unsigned node)\n{\n\tstruct dwc3_trb\t\t*trb;\n\n\tdwc3_trace(trace_dwc3_gadget, \"%s: req %p dma %08llx length %d%s\",\n\t\t\tdep->name, req, (unsigned long long) dma,\n\t\t\tlength, chain ? \" chain\" : \"\");\n\n\ttrb = &dep->trb_pool[dep->trb_enqueue];\n\n\tif (!req->trb) {\n\t\tdwc3_gadget_move_started_request(req);\n\t\treq->trb = trb;\n\t\treq->trb_dma = dwc3_trb_dma_offset(dep, trb);\n\t\treq->first_trb_index = dep->trb_enqueue;\n\t}\n\n\tdwc3_ep_inc_enq(dep);\n\n\ttrb->size = DWC3_TRB_SIZE_LENGTH(length);\n\ttrb->bpl = lower_32_bits(dma);\n\ttrb->bph = upper_32_bits(dma);\n\n\tswitch (usb_endpoint_type(dep->endpoint.desc)) {\n\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\ttrb->ctrl = DWC3_TRBCTL_CONTROL_SETUP;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_ISOC:\n\t\tif (!node)\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS_FIRST;\n\t\telse\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS;\n\n\t\t/* always enable Interrupt on Missed ISOC */\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_ISP_IMI;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_BULK:\n\tcase USB_ENDPOINT_XFER_INT:\n\t\ttrb->ctrl = DWC3_TRBCTL_NORMAL;\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * This is only possible with faulty memory because we\n\t\t * checked it already :)\n\t\t */\n\t\tBUG();\n\t}\n\n\t/* always enable Continue on Short Packet */\n\ttrb->ctrl |= DWC3_TRB_CTRL_CSP;\n\n\tif ((!req->request.no_interrupt && !chain) ||\n\t\t\t(dwc3_calc_trbs_left(dep) == 0))\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_IOC | DWC3_TRB_CTRL_ISP_IMI;\n\n\tif (chain)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_CHN;\n\n\tif (usb_endpoint_xfer_bulk(dep->endpoint.desc) && dep->stream_capable)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_SID_SOFN(req->request.stream_id);\n\n\ttrb->ctrl |= DWC3_TRB_CTRL_HWO;\n\n\tdep->queued_requests++;\n\n\ttrace_dwc3_prepare_trb(dep, trb);\n}\n\nstatic int __dwc3_cleanup_done_trbs(struct dwc3 *dwc, struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, struct dwc3_trb *trb,\n\t\tconst struct dwc3_event_depevt *event, int status,\n\t\tint chain)\n{\n\tunsigned int\t\tcount;\n\tunsigned int\t\ts_pkt = 0;\n\tunsigned int\t\ttrb_status;\n\n\tdep->queued_requests--;\n\tdwc3_ep_inc_deq(dep);\n\ttrace_dwc3_complete_trb(dep, trb);\n\n\t/*\n\t * If we're in the middle of series of chained TRBs and we\n\t * receive a short transfer along the way, DWC3 will skip\n\t * through all TRBs including the last TRB in the chain (the\n\t * where CHN bit is zero. DWC3 will also avoid clearing HWO\n\t * bit and SW has to do it manually.\n\t *\n\t * We're going to do that here to avoid problems of HW trying\n\t * to use bogus TRBs for transfers.\n\t */\n\tif (chain && (trb->ctrl & DWC3_TRB_CTRL_HWO))\n\t\ttrb->ctrl &= ~DWC3_TRB_CTRL_HWO;\n\n\tif ((trb->ctrl & DWC3_TRB_CTRL_HWO) && status != -ESHUTDOWN)\n\t\treturn 1;\n\n\tcount = trb->size & DWC3_TRB_SIZE_MASK;\n\treq->request.actual += count;\n\n\tif (dep->direction) {\n\t\tif (count) {\n\t\t\ttrb_status = DWC3_TRB_SIZE_TRBSTS(trb->size);\n\t\t\tif (trb_status == DWC3_TRBSTS_MISSED_ISOC) {\n\t\t\t\tdwc3_trace(trace_dwc3_gadget,\n\t\t\t\t\t\t\"%s: incomplete IN transfer\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\t/*\n\t\t\t\t * If missed isoc occurred and there is\n\t\t\t\t * no request queued then issue END\n\t\t\t\t * TRANSFER, so that core generates\n\t\t\t\t * next xfernotready and we will issue\n\t\t\t\t * a fresh START TRANSFER.\n\t\t\t\t * If there are still queued request\n\t\t\t\t * then wait, do not issue either END\n\t\t\t\t * or UPDATE TRANSFER, just attach next\n\t\t\t\t * request in pending_list during\n\t\t\t\t * giveback.If any future queued request\n\t\t\t\t * is successfully transferred then we\n\t\t\t\t * will issue UPDATE TRANSFER for all\n\t\t\t\t * request in the pending_list.\n\t\t\t\t */\n\t\t\t\tdep->flags |= DWC3_EP_MISSED_ISOC;\n\t\t\t} else {\n\t\t\t\tdev_err(dwc->dev, \"incomplete IN transfer %s\\n\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\tstatus = -ECONNRESET;\n\t\t\t}\n\t\t} else {\n\t\t\tdep->flags &= ~DWC3_EP_MISSED_ISOC;\n\t\t}\n\t} else {\n\t\tif (count && (event->status & DEPEVT_STATUS_SHORT))\n\t\t\ts_pkt = 1;\n\t}\n\n\tif (s_pkt && !chain)\n\t\treturn 1;\n\n\tif ((event->status & DEPEVT_STATUS_IOC) &&\n\t\t\t(trb->ctrl & DWC3_TRB_CTRL_IOC))\n\t\treturn 1;\n\n\treturn 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void dwc3_prepare_one_trb(struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, dma_addr_t dma,\n\t\tunsigned length, unsigned chain, unsigned node)\n{\n\tstruct dwc3_trb\t\t*trb;\n\n\tdwc3_trace(trace_dwc3_gadget, \"%s: req %p dma %08llx length %d%s\",\n\t\t\tdep->name, req, (unsigned long long) dma,\n\t\t\tlength, chain ? \" chain\" : \"\");\n\n\ttrb = &dep->trb_pool[dep->trb_enqueue];\n\n\tif (!req->trb) {\n\t\tdwc3_gadget_move_started_request(req);\n\t\treq->trb = trb;\n\t\treq->trb_dma = dwc3_trb_dma_offset(dep, trb);\n\t\treq->first_trb_index = dep->trb_enqueue;\n\t\tdep->queued_requests++;\n\t}\n\n\tdwc3_ep_inc_enq(dep);\n\n\ttrb->size = DWC3_TRB_SIZE_LENGTH(length);\n\ttrb->bpl = lower_32_bits(dma);\n\ttrb->bph = upper_32_bits(dma);\n\n\tswitch (usb_endpoint_type(dep->endpoint.desc)) {\n\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\ttrb->ctrl = DWC3_TRBCTL_CONTROL_SETUP;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_ISOC:\n\t\tif (!node)\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS_FIRST;\n\t\telse\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS;\n\n\t\t/* always enable Interrupt on Missed ISOC */\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_ISP_IMI;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_BULK:\n\tcase USB_ENDPOINT_XFER_INT:\n\t\ttrb->ctrl = DWC3_TRBCTL_NORMAL;\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * This is only possible with faulty memory because we\n\t\t * checked it already :)\n\t\t */\n\t\tBUG();\n\t}\n\n\t/* always enable Continue on Short Packet */\n\ttrb->ctrl |= DWC3_TRB_CTRL_CSP;\n\n\tif ((!req->request.no_interrupt && !chain) ||\n\t\t\t(dwc3_calc_trbs_left(dep) == 0))\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_IOC | DWC3_TRB_CTRL_ISP_IMI;\n\n\tif (chain)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_CHN;\n\n\tif (usb_endpoint_xfer_bulk(dep->endpoint.desc) && dep->stream_capable)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_SID_SOFN(req->request.stream_id);\n\n\ttrb->ctrl |= DWC3_TRB_CTRL_HWO;\n\n\ttrace_dwc3_prepare_trb(dep, trb);\n}\n\nstatic int __dwc3_cleanup_done_trbs(struct dwc3 *dwc, struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, struct dwc3_trb *trb,\n\t\tconst struct dwc3_event_depevt *event, int status,\n\t\tint chain)\n{\n\tunsigned int\t\tcount;\n\tunsigned int\t\ts_pkt = 0;\n\tunsigned int\t\ttrb_status;\n\n\tdwc3_ep_inc_deq(dep);\n\n\tif (req->trb == trb)\n\t\tdep->queued_requests--;\n\n\ttrace_dwc3_complete_trb(dep, trb);\n\n\t/*\n\t * If we're in the middle of series of chained TRBs and we\n\t * receive a short transfer along the way, DWC3 will skip\n\t * through all TRBs including the last TRB in the chain (the\n\t * where CHN bit is zero. DWC3 will also avoid clearing HWO\n\t * bit and SW has to do it manually.\n\t *\n\t * We're going to do that here to avoid problems of HW trying\n\t * to use bogus TRBs for transfers.\n\t */\n\tif (chain && (trb->ctrl & DWC3_TRB_CTRL_HWO))\n\t\ttrb->ctrl &= ~DWC3_TRB_CTRL_HWO;\n\n\tif ((trb->ctrl & DWC3_TRB_CTRL_HWO) && status != -ESHUTDOWN)\n\t\treturn 1;\n\n\tcount = trb->size & DWC3_TRB_SIZE_MASK;\n\treq->request.actual += count;\n\n\tif (dep->direction) {\n\t\tif (count) {\n\t\t\ttrb_status = DWC3_TRB_SIZE_TRBSTS(trb->size);\n\t\t\tif (trb_status == DWC3_TRBSTS_MISSED_ISOC) {\n\t\t\t\tdwc3_trace(trace_dwc3_gadget,\n\t\t\t\t\t\t\"%s: incomplete IN transfer\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\t/*\n\t\t\t\t * If missed isoc occurred and there is\n\t\t\t\t * no request queued then issue END\n\t\t\t\t * TRANSFER, so that core generates\n\t\t\t\t * next xfernotready and we will issue\n\t\t\t\t * a fresh START TRANSFER.\n\t\t\t\t * If there are still queued request\n\t\t\t\t * then wait, do not issue either END\n\t\t\t\t * or UPDATE TRANSFER, just attach next\n\t\t\t\t * request in pending_list during\n\t\t\t\t * giveback.If any future queued request\n\t\t\t\t * is successfully transferred then we\n\t\t\t\t * will issue UPDATE TRANSFER for all\n\t\t\t\t * request in the pending_list.\n\t\t\t\t */\n\t\t\t\tdep->flags |= DWC3_EP_MISSED_ISOC;\n\t\t\t} else {\n\t\t\t\tdev_err(dwc->dev, \"incomplete IN transfer %s\\n\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\tstatus = -ECONNRESET;\n\t\t\t}\n\t\t} else {\n\t\t\tdep->flags &= ~DWC3_EP_MISSED_ISOC;\n\t\t}\n\t} else {\n\t\tif (count && (event->status & DEPEVT_STATUS_SHORT))\n\t\t\ts_pkt = 1;\n\t}\n\n\tif (s_pkt && !chain)\n\t\treturn 1;\n\n\tif ((event->status & DEPEVT_STATUS_IOC) &&\n\t\t\t(trb->ctrl & DWC3_TRB_CTRL_IOC))\n\t\treturn 1;\n\n\treturn 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void dwc3_prepare_one_trb(struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, dma_addr_t dma,\n\t\tunsigned length, unsigned last, unsigned chain, unsigned node)\n{\n\tstruct dwc3_trb\t\t*trb;\n\n\tdwc3_trace(trace_dwc3_gadget, \"%s: req %p dma %08llx length %d%s%s\",\n\t\t\tdep->name, req, (unsigned long long) dma,\n\t\t\tlength, last ? \" last\" : \"\",\n\t\t\tchain ? \" chain\" : \"\");\n\n\n\ttrb = &dep->trb_pool[dep->trb_enqueue];\n\n\tif (!req->trb) {\n\t\tdwc3_gadget_move_started_request(req);\n\t\treq->trb = trb;\n\t\treq->trb_dma = dwc3_trb_dma_offset(dep, trb);\n\t\treq->first_trb_index = dep->trb_enqueue;\n\t}\n\n\tdwc3_ep_inc_enq(dep);\n\n\ttrb->size = DWC3_TRB_SIZE_LENGTH(length);\n\ttrb->bpl = lower_32_bits(dma);\n\ttrb->bph = upper_32_bits(dma);\n\n\tswitch (usb_endpoint_type(dep->endpoint.desc)) {\n\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\ttrb->ctrl = DWC3_TRBCTL_CONTROL_SETUP;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_ISOC:\n\t\tif (!node)\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS_FIRST;\n\t\telse\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS;\n\n\t\t/* always enable Interrupt on Missed ISOC */\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_ISP_IMI;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_BULK:\n\tcase USB_ENDPOINT_XFER_INT:\n\t\ttrb->ctrl = DWC3_TRBCTL_NORMAL;\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * This is only possible with faulty memory because we\n\t\t * checked it already :)\n\t\t */\n\t\tBUG();\n\t}\n\n\t/* always enable Continue on Short Packet */\n\ttrb->ctrl |= DWC3_TRB_CTRL_CSP;\n\n\tif (!req->request.no_interrupt && !chain)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_IOC | DWC3_TRB_CTRL_ISP_IMI;\n\n\tif (last && !usb_endpoint_xfer_isoc(dep->endpoint.desc))\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_LST;\n\n\tif (chain)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_CHN;\n\n\tif (usb_endpoint_xfer_bulk(dep->endpoint.desc) && dep->stream_capable)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_SID_SOFN(req->request.stream_id);\n\n\ttrb->ctrl |= DWC3_TRB_CTRL_HWO;\n\n\tdep->queued_requests++;\n\n\ttrace_dwc3_prepare_trb(dep, trb);\n}\n\nstatic int __dwc3_cleanup_done_trbs(struct dwc3 *dwc, struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, struct dwc3_trb *trb,\n\t\tconst struct dwc3_event_depevt *event, int status,\n\t\tint chain)\n{\n\tunsigned int\t\tcount;\n\tunsigned int\t\ts_pkt = 0;\n\tunsigned int\t\ttrb_status;\n\n\tdep->queued_requests--;\n\ttrace_dwc3_complete_trb(dep, trb);\n\n\t/*\n\t * If we're in the middle of series of chained TRBs and we\n\t * receive a short transfer along the way, DWC3 will skip\n\t * through all TRBs including the last TRB in the chain (the\n\t * where CHN bit is zero. DWC3 will also avoid clearing HWO\n\t * bit and SW has to do it manually.\n\t *\n\t * We're going to do that here to avoid problems of HW trying\n\t * to use bogus TRBs for transfers.\n\t */\n\tif (chain && (trb->ctrl & DWC3_TRB_CTRL_HWO))\n\t\ttrb->ctrl &= ~DWC3_TRB_CTRL_HWO;\n\n\tif ((trb->ctrl & DWC3_TRB_CTRL_HWO) && status != -ESHUTDOWN)\n\t\treturn 1;\n\n\tcount = trb->size & DWC3_TRB_SIZE_MASK;\n\n\tif (dep->direction) {\n\t\tif (count) {\n\t\t\ttrb_status = DWC3_TRB_SIZE_TRBSTS(trb->size);\n\t\t\tif (trb_status == DWC3_TRBSTS_MISSED_ISOC) {\n\t\t\t\tdwc3_trace(trace_dwc3_gadget,\n\t\t\t\t\t\t\"%s: incomplete IN transfer\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\t/*\n\t\t\t\t * If missed isoc occurred and there is\n\t\t\t\t * no request queued then issue END\n\t\t\t\t * TRANSFER, so that core generates\n\t\t\t\t * next xfernotready and we will issue\n\t\t\t\t * a fresh START TRANSFER.\n\t\t\t\t * If there are still queued request\n\t\t\t\t * then wait, do not issue either END\n\t\t\t\t * or UPDATE TRANSFER, just attach next\n\t\t\t\t * request in pending_list during\n\t\t\t\t * giveback.If any future queued request\n\t\t\t\t * is successfully transferred then we\n\t\t\t\t * will issue UPDATE TRANSFER for all\n\t\t\t\t * request in the pending_list.\n\t\t\t\t */\n\t\t\t\tdep->flags |= DWC3_EP_MISSED_ISOC;\n\t\t\t} else {\n\t\t\t\tdev_err(dwc->dev, \"incomplete IN transfer %s\\n\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\tstatus = -ECONNRESET;\n\t\t\t}\n\t\t} else {\n\t\t\tdep->flags &= ~DWC3_EP_MISSED_ISOC;\n\t\t}\n\t} else {\n\t\tif (count && (event->status & DEPEVT_STATUS_SHORT))\n\t\t\ts_pkt = 1;\n\t}\n\n\tif (s_pkt && !chain)\n\t\treturn 1;\n\tif ((event->status & DEPEVT_STATUS_LST) &&\n\t\t\t(trb->ctrl & (DWC3_TRB_CTRL_LST |\n\t\t\t\tDWC3_TRB_CTRL_HWO)))\n\t\treturn 1;\n\tif ((event->status & DEPEVT_STATUS_IOC) &&\n\t\t\t(trb->ctrl & DWC3_TRB_CTRL_IOC))\n\t\treturn 1;\n\treturn 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void dwc3_prepare_one_trb(struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, dma_addr_t dma,\n\t\tunsigned length, unsigned last, unsigned chain, unsigned node)\n{\n\tstruct dwc3_trb\t\t*trb;\n\n\tdwc3_trace(trace_dwc3_gadget, \"%s: req %p dma %08llx length %d%s%s\",\n\t\t\tdep->name, req, (unsigned long long) dma,\n\t\t\tlength, last ? \" last\" : \"\",\n\t\t\tchain ? \" chain\" : \"\");\n\n\n\ttrb = &dep->trb_pool[dep->trb_enqueue];\n\n\tif (!req->trb) {\n\t\tdwc3_gadget_move_started_request(req);\n\t\treq->trb = trb;\n\t\treq->trb_dma = dwc3_trb_dma_offset(dep, trb);\n\t\treq->first_trb_index = dep->trb_enqueue;\n\t\tdep->queued_requests++;\n\t}\n\n\tdwc3_ep_inc_enq(dep);\n\n\ttrb->size = DWC3_TRB_SIZE_LENGTH(length);\n\ttrb->bpl = lower_32_bits(dma);\n\ttrb->bph = upper_32_bits(dma);\n\n\tswitch (usb_endpoint_type(dep->endpoint.desc)) {\n\tcase USB_ENDPOINT_XFER_CONTROL:\n\t\ttrb->ctrl = DWC3_TRBCTL_CONTROL_SETUP;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_ISOC:\n\t\tif (!node)\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS_FIRST;\n\t\telse\n\t\t\ttrb->ctrl = DWC3_TRBCTL_ISOCHRONOUS;\n\n\t\t/* always enable Interrupt on Missed ISOC */\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_ISP_IMI;\n\t\tbreak;\n\n\tcase USB_ENDPOINT_XFER_BULK:\n\tcase USB_ENDPOINT_XFER_INT:\n\t\ttrb->ctrl = DWC3_TRBCTL_NORMAL;\n\t\tbreak;\n\tdefault:\n\t\t/*\n\t\t * This is only possible with faulty memory because we\n\t\t * checked it already :)\n\t\t */\n\t\tBUG();\n\t}\n\n\t/* always enable Continue on Short Packet */\n\ttrb->ctrl |= DWC3_TRB_CTRL_CSP;\n\n\tif (!req->request.no_interrupt && !chain)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_IOC | DWC3_TRB_CTRL_ISP_IMI;\n\n\tif (last && !usb_endpoint_xfer_isoc(dep->endpoint.desc))\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_LST;\n\n\tif (chain)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_CHN;\n\n\tif (usb_endpoint_xfer_bulk(dep->endpoint.desc) && dep->stream_capable)\n\t\ttrb->ctrl |= DWC3_TRB_CTRL_SID_SOFN(req->request.stream_id);\n\n\ttrb->ctrl |= DWC3_TRB_CTRL_HWO;\n\n\ttrace_dwc3_prepare_trb(dep, trb);\n}\n\nstatic int __dwc3_cleanup_done_trbs(struct dwc3 *dwc, struct dwc3_ep *dep,\n\t\tstruct dwc3_request *req, struct dwc3_trb *trb,\n\t\tconst struct dwc3_event_depevt *event, int status,\n\t\tint chain)\n{\n\tunsigned int\t\tcount;\n\tunsigned int\t\ts_pkt = 0;\n\tunsigned int\t\ttrb_status;\n\n\tif (req->trb == trb)\n\t\tdep->queued_requests--;\n\n\ttrace_dwc3_complete_trb(dep, trb);\n\n\t/*\n\t * If we're in the middle of series of chained TRBs and we\n\t * receive a short transfer along the way, DWC3 will skip\n\t * through all TRBs including the last TRB in the chain (the\n\t * where CHN bit is zero. DWC3 will also avoid clearing HWO\n\t * bit and SW has to do it manually.\n\t *\n\t * We're going to do that here to avoid problems of HW trying\n\t * to use bogus TRBs for transfers.\n\t */\n\tif (chain && (trb->ctrl & DWC3_TRB_CTRL_HWO))\n\t\ttrb->ctrl &= ~DWC3_TRB_CTRL_HWO;\n\n\tif ((trb->ctrl & DWC3_TRB_CTRL_HWO) && status != -ESHUTDOWN)\n\t\treturn 1;\n\n\tcount = trb->size & DWC3_TRB_SIZE_MASK;\n\n\tif (dep->direction) {\n\t\tif (count) {\n\t\t\ttrb_status = DWC3_TRB_SIZE_TRBSTS(trb->size);\n\t\t\tif (trb_status == DWC3_TRBSTS_MISSED_ISOC) {\n\t\t\t\tdwc3_trace(trace_dwc3_gadget,\n\t\t\t\t\t\t\"%s: incomplete IN transfer\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\t/*\n\t\t\t\t * If missed isoc occurred and there is\n\t\t\t\t * no request queued then issue END\n\t\t\t\t * TRANSFER, so that core generates\n\t\t\t\t * next xfernotready and we will issue\n\t\t\t\t * a fresh START TRANSFER.\n\t\t\t\t * If there are still queued request\n\t\t\t\t * then wait, do not issue either END\n\t\t\t\t * or UPDATE TRANSFER, just attach next\n\t\t\t\t * request in pending_list during\n\t\t\t\t * giveback.If any future queued request\n\t\t\t\t * is successfully transferred then we\n\t\t\t\t * will issue UPDATE TRANSFER for all\n\t\t\t\t * request in the pending_list.\n\t\t\t\t */\n\t\t\t\tdep->flags |= DWC3_EP_MISSED_ISOC;\n\t\t\t} else {\n\t\t\t\tdev_err(dwc->dev, \"incomplete IN transfer %s\\n\",\n\t\t\t\t\t\tdep->name);\n\t\t\t\tstatus = -ECONNRESET;\n\t\t\t}\n\t\t} else {\n\t\t\tdep->flags &= ~DWC3_EP_MISSED_ISOC;\n\t\t}\n\t} else {\n\t\tif (count && (event->status & DEPEVT_STATUS_SHORT))\n\t\t\ts_pkt = 1;\n\t}\n\n\tif (s_pkt && !chain)\n\t\treturn 1;\n\tif ((event->status & DEPEVT_STATUS_LST) &&\n\t\t\t(trb->ctrl & (DWC3_TRB_CTRL_LST |\n\t\t\t\tDWC3_TRB_CTRL_HWO)))\n\t\treturn 1;\n\tif ((event->status & DEPEVT_STATUS_IOC) &&\n\t\t\t(trb->ctrl & DWC3_TRB_CTRL_IOC))\n\t\treturn 1;\n\treturn 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int wm8994_set_dai_sysclk(struct snd_soc_dai *dai,\n\t\tint clk_id, unsigned int freq, int dir)\n{\n\tstruct snd_soc_component *component = dai->component;\n\tstruct wm8994_priv *wm8994 = snd_soc_component_get_drvdata(component);\n\tint i;\n\n\tswitch (dai->id) {\n\tcase 1:\n\tcase 2:\n\t\tbreak;\n\n\tdefault:\n\t\t/* AIF3 shares clocking with AIF1/2 */\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (clk_id) {\n\tcase WM8994_SYSCLK_MCLK1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK1;\n\t\twm8994->mclk[0] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK1 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_MCLK2:\n\t\t/* TODO: Set GPIO AF */\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK2;\n\t\twm8994->mclk[1] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK2 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL1;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL1\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL2:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL2;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL2\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_OPCLK:\n\t\t/* Special case - a division (times 10) is given and\n\t\t * no effect on main clocking.\n\t\t */\n\t\tif (freq) {\n\t\t\tfor (i = 0; i < ARRAY_SIZE(opclk_divs); i++)\n\t\t\t\tif (opclk_divs[i] == freq)\n\t\t\t\t\tbreak;\n\t\t\tif (i == ARRAY_SIZE(opclk_divs))\n\t\t\t\treturn -EINVAL;\n\t\t\tsnd_soc_component_update_bits(component, WM8994_CLOCKING_2,\n\t\t\t\t\t    WM8994_OPCLK_DIV_MASK, i);\n\t\t\tsnd_soc_component_update_bits(component, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, WM8994_OPCLK_ENA);\n\t\t} else {\n\t\t\tsnd_soc_component_update_bits(component, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, 0);\n\t\t}\n\t\t/* fall through */\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tconfigure_clock(component);\n\n\t/*\n\t * If SYSCLK will be less than 50kHz adjust AIFnCLK dividers\n\t * for detection.\n\t */\n\tif (max(wm8994->aifclk[0], wm8994->aifclk[1]) < 50000) {\n\t\tdev_dbg(component->dev, \"Configuring AIFs for 128fs\\n\");\n\n\t\twm8994->aifdiv[0] = snd_soc_component_read32(component, WM8994_AIF1_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\t\twm8994->aifdiv[1] = snd_soc_component_read32(component, WM8994_AIF2_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK, 0x1);\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK, 0x1);\n\t} else if (wm8994->aifdiv[0]) {\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[0]);\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[1]);\n\n\t\twm8994->aifdiv[0] = 0;\n\t\twm8994->aifdiv[1] = 0;\n\t}\n\n\treturn 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int wm8994_set_dai_sysclk(struct snd_soc_dai *dai,\n\t\tint clk_id, unsigned int freq, int dir)\n{\n\tstruct snd_soc_component *component = dai->component;\n\tstruct wm8994_priv *wm8994 = snd_soc_component_get_drvdata(component);\n\tint i;\n\n\tswitch (dai->id) {\n\tcase 1:\n\tcase 2:\n\t\tbreak;\n\n\tdefault:\n\t\t/* AIF3 shares clocking with AIF1/2 */\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (clk_id) {\n\tcase WM8994_SYSCLK_MCLK1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK1;\n\t\twm8994->mclk[0] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK1 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_MCLK2:\n\t\t/* TODO: Set GPIO AF */\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK2;\n\t\twm8994->mclk[1] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK2 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL1;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL1\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL2:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL2;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL2\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_OPCLK:\n\t\t/* Special case - a division (times 10) is given and\n\t\t * no effect on main clocking.\n\t\t */\n\t\tif (freq) {\n\t\t\tfor (i = 0; i < ARRAY_SIZE(opclk_divs); i++)\n\t\t\t\tif (opclk_divs[i] == freq)\n\t\t\t\t\tbreak;\n\t\t\tif (i == ARRAY_SIZE(opclk_divs))\n\t\t\t\treturn -EINVAL;\n\t\t\tsnd_soc_component_update_bits(component, WM8994_CLOCKING_2,\n\t\t\t\t\t    WM8994_OPCLK_DIV_MASK, i);\n\t\t\tsnd_soc_component_update_bits(component, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, WM8994_OPCLK_ENA);\n\t\t} else {\n\t\t\tsnd_soc_component_update_bits(component, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, 0);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tconfigure_clock(component);\n\n\t/*\n\t * If SYSCLK will be less than 50kHz adjust AIFnCLK dividers\n\t * for detection.\n\t */\n\tif (max(wm8994->aifclk[0], wm8994->aifclk[1]) < 50000) {\n\t\tdev_dbg(component->dev, \"Configuring AIFs for 128fs\\n\");\n\n\t\twm8994->aifdiv[0] = snd_soc_component_read32(component, WM8994_AIF1_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\t\twm8994->aifdiv[1] = snd_soc_component_read32(component, WM8994_AIF2_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK, 0x1);\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK, 0x1);\n\t} else if (wm8994->aifdiv[0]) {\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[0]);\n\t\tsnd_soc_component_update_bits(component, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[1]);\n\n\t\twm8994->aifdiv[0] = 0;\n\t\twm8994->aifdiv[1] = 0;\n\t}\n\n\treturn 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int wm8994_set_dai_sysclk(struct snd_soc_dai *dai,\n\t\tint clk_id, unsigned int freq, int dir)\n{\n\tstruct snd_soc_codec *codec = dai->codec;\n\tstruct wm8994_priv *wm8994 = snd_soc_codec_get_drvdata(codec);\n\tint i;\n\n\tswitch (dai->id) {\n\tcase 1:\n\tcase 2:\n\t\tbreak;\n\n\tdefault:\n\t\t/* AIF3 shares clocking with AIF1/2 */\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (clk_id) {\n\tcase WM8994_SYSCLK_MCLK1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK1;\n\t\twm8994->mclk[0] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK1 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_MCLK2:\n\t\t/* TODO: Set GPIO AF */\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK2;\n\t\twm8994->mclk[1] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK2 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL1;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL1\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL2:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL2;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL2\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_OPCLK:\n\t\t/* Special case - a division (times 10) is given and\n\t\t * no effect on main clocking.\n\t\t */\n\t\tif (freq) {\n\t\t\tfor (i = 0; i < ARRAY_SIZE(opclk_divs); i++)\n\t\t\t\tif (opclk_divs[i] == freq)\n\t\t\t\t\tbreak;\n\t\t\tif (i == ARRAY_SIZE(opclk_divs))\n\t\t\t\treturn -EINVAL;\n\t\t\tsnd_soc_update_bits(codec, WM8994_CLOCKING_2,\n\t\t\t\t\t    WM8994_OPCLK_DIV_MASK, i);\n\t\t\tsnd_soc_update_bits(codec, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, WM8994_OPCLK_ENA);\n\t\t} else {\n\t\t\tsnd_soc_update_bits(codec, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, 0);\n\t\t}\n\t\t/* fall through */\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tconfigure_clock(codec);\n\n\t/*\n\t * If SYSCLK will be less than 50kHz adjust AIFnCLK dividers\n\t * for detection.\n\t */\n\tif (max(wm8994->aifclk[0], wm8994->aifclk[1]) < 50000) {\n\t\tdev_dbg(codec->dev, \"Configuring AIFs for 128fs\\n\");\n\n\t\twm8994->aifdiv[0] = snd_soc_read(codec, WM8994_AIF1_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\t\twm8994->aifdiv[1] = snd_soc_read(codec, WM8994_AIF2_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\n\t\tsnd_soc_update_bits(codec, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK, 0x1);\n\t\tsnd_soc_update_bits(codec, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK, 0x1);\n\t} else if (wm8994->aifdiv[0]) {\n\t\tsnd_soc_update_bits(codec, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[0]);\n\t\tsnd_soc_update_bits(codec, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[1]);\n\n\t\twm8994->aifdiv[0] = 0;\n\t\twm8994->aifdiv[1] = 0;\n\t}\n\n\treturn 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int wm8994_set_dai_sysclk(struct snd_soc_dai *dai,\n\t\tint clk_id, unsigned int freq, int dir)\n{\n\tstruct snd_soc_codec *codec = dai->codec;\n\tstruct wm8994_priv *wm8994 = snd_soc_codec_get_drvdata(codec);\n\tint i;\n\n\tswitch (dai->id) {\n\tcase 1:\n\tcase 2:\n\t\tbreak;\n\n\tdefault:\n\t\t/* AIF3 shares clocking with AIF1/2 */\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (clk_id) {\n\tcase WM8994_SYSCLK_MCLK1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK1;\n\t\twm8994->mclk[0] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK1 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_MCLK2:\n\t\t/* TODO: Set GPIO AF */\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_MCLK2;\n\t\twm8994->mclk[1] = freq;\n\t\tdev_dbg(dai->dev, \"AIF%d using MCLK2 at %uHz\\n\",\n\t\t\tdai->id, freq);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL1:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL1;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL1\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_FLL2:\n\t\twm8994->sysclk[dai->id - 1] = WM8994_SYSCLK_FLL2;\n\t\tdev_dbg(dai->dev, \"AIF%d using FLL2\\n\", dai->id);\n\t\tbreak;\n\n\tcase WM8994_SYSCLK_OPCLK:\n\t\t/* Special case - a division (times 10) is given and\n\t\t * no effect on main clocking.\n\t\t */\n\t\tif (freq) {\n\t\t\tfor (i = 0; i < ARRAY_SIZE(opclk_divs); i++)\n\t\t\t\tif (opclk_divs[i] == freq)\n\t\t\t\t\tbreak;\n\t\t\tif (i == ARRAY_SIZE(opclk_divs))\n\t\t\t\treturn -EINVAL;\n\t\t\tsnd_soc_update_bits(codec, WM8994_CLOCKING_2,\n\t\t\t\t\t    WM8994_OPCLK_DIV_MASK, i);\n\t\t\tsnd_soc_update_bits(codec, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, WM8994_OPCLK_ENA);\n\t\t} else {\n\t\t\tsnd_soc_update_bits(codec, WM8994_POWER_MANAGEMENT_2,\n\t\t\t\t\t    WM8994_OPCLK_ENA, 0);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tconfigure_clock(codec);\n\n\t/*\n\t * If SYSCLK will be less than 50kHz adjust AIFnCLK dividers\n\t * for detection.\n\t */\n\tif (max(wm8994->aifclk[0], wm8994->aifclk[1]) < 50000) {\n\t\tdev_dbg(codec->dev, \"Configuring AIFs for 128fs\\n\");\n\n\t\twm8994->aifdiv[0] = snd_soc_read(codec, WM8994_AIF1_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\t\twm8994->aifdiv[1] = snd_soc_read(codec, WM8994_AIF2_RATE)\n\t\t\t& WM8994_AIF1CLK_RATE_MASK;\n\n\t\tsnd_soc_update_bits(codec, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK, 0x1);\n\t\tsnd_soc_update_bits(codec, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK, 0x1);\n\t} else if (wm8994->aifdiv[0]) {\n\t\tsnd_soc_update_bits(codec, WM8994_AIF1_RATE,\n\t\t\t\t    WM8994_AIF1CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[0]);\n\t\tsnd_soc_update_bits(codec, WM8994_AIF2_RATE,\n\t\t\t\t    WM8994_AIF2CLK_RATE_MASK,\n\t\t\t\t    wm8994->aifdiv[1]);\n\n\t\twm8994->aifdiv[0] = 0;\n\t\twm8994->aifdiv[1] = 0;\n\t}\n\n\treturn 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nu32 tf = timer->flags;\n'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nu32 tf;\n\n\n\n\n/*\n\n\n* We need to use READ_ONCE() here, otherwise the compiler\n\n\n* might re-read @tf between the check for TIMER_MIGRATING\n\n\n* and spin_lock().\n\n\n*/\n\n\ntf = READ_ONCE(timer->flags);\n'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nu32 tf = timer->flags;\n'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nu32 tf;\n\n\n\n\n/*\n\n\n* We need to use READ_ONCE() here, otherwise the compiler\n\n\n* might re-read @tf between the check for TIMER_MIGRATING\n\n\n* and spin_lock().\n\n\n*/\n\n\ntf = READ_ONCE(timer->flags);\n'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int vti_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\n\tmemcpy(dev->dev_addr, &iph->saddr, 4);\n\tmemcpy(dev->broadcast, &iph->daddr, 4);\n\n\tdev->mtu\t\t= ETH_DATA_LEN;\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_LLTX;\n\tnetif_keep_dst(dev);\n\n\treturn ip_tunnel_init(dev);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int vti_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\n\tmemcpy(dev->dev_addr, &iph->saddr, 4);\n\tmemcpy(dev->broadcast, &iph->daddr, 4);\n\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_LLTX;\n\tnetif_keep_dst(dev);\n\n\treturn ip_tunnel_init(dev);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int vti_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\n\tmemcpy(dev->dev_addr, &iph->saddr, 4);\n\tmemcpy(dev->broadcast, &iph->daddr, 4);\n\n\tdev->mtu\t\t= ETH_DATA_LEN;\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_LLTX;\n\tdev->priv_flags\t\t&= ~IFF_XMIT_DST_RELEASE;\n\n\treturn ip_tunnel_init(dev);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int vti_tunnel_init(struct net_device *dev)\n{\n\tstruct ip_tunnel *tunnel = netdev_priv(dev);\n\tstruct iphdr *iph = &tunnel->parms.iph;\n\n\tmemcpy(dev->dev_addr, &iph->saddr, 4);\n\tmemcpy(dev->broadcast, &iph->daddr, 4);\n\n\tdev->flags\t\t= IFF_NOARP;\n\tdev->iflink\t\t= 0;\n\tdev->addr_len\t\t= 4;\n\tdev->features\t\t|= NETIF_F_LLTX;\n\tdev->priv_flags\t\t&= ~IFF_XMIT_DST_RELEASE;\n\n\treturn ip_tunnel_init(dev);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int acm_probe(struct usb_interface *intf,\n\t\t     const struct usb_device_id *id)\n{\n\tstruct usb_cdc_union_desc *union_header = NULL;\n\tstruct usb_cdc_country_functional_desc *cfd = NULL;\n\tunsigned char *buffer = intf->altsetting->extra;\n\tint buflen = intf->altsetting->extralen;\n\tstruct usb_interface *control_interface;\n\tstruct usb_interface *data_interface;\n\tstruct usb_endpoint_descriptor *epctrl = NULL;\n\tstruct usb_endpoint_descriptor *epread = NULL;\n\tstruct usb_endpoint_descriptor *epwrite = NULL;\n\tstruct usb_device *usb_dev = interface_to_usbdev(intf);\n\tstruct acm *acm;\n\tint minor;\n\tint ctrlsize, readsize;\n\tu8 *buf;\n\tu8 ac_management_function = 0;\n\tu8 call_management_function = 0;\n\tint call_interface_num = -1;\n\tint data_interface_num = -1;\n\tunsigned long quirks;\n\tint num_rx_buf;\n\tint i;\n\tunsigned int elength = 0;\n\tint combined_interfaces = 0;\n\tstruct device *tty_dev;\n\tint rv = -ENOMEM;\n\n\t/* normal quirks */\n\tquirks = (unsigned long)id->driver_info;\n\n\tif (quirks == IGNORE_DEVICE)\n\t\treturn -ENODEV;\n\n\tnum_rx_buf = (quirks == SINGLE_RX_URB) ? 1 : ACM_NR;\n\n\t/* handle quirks deadly to normal probing*/\n\tif (quirks == NO_UNION_NORMAL) {\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, 1);\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\tgoto skip_normal_probe;\n\t}\n\n\t/* normal probing*/\n\tif (!buffer) {\n\t\tdev_err(&intf->dev, \"Weird descriptor references\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!buflen) {\n\t\tif (intf->cur_altsetting->endpoint &&\n\t\t\t\tintf->cur_altsetting->endpoint->extralen &&\n\t\t\t\tintf->cur_altsetting->endpoint->extra) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Seeking extra descriptors on endpoint\\n\");\n\t\t\tbuflen = intf->cur_altsetting->endpoint->extralen;\n\t\t\tbuffer = intf->cur_altsetting->endpoint->extra;\n\t\t} else {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Zero length descriptor references\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\twhile (buflen > 0) {\n\t\tif (buffer[1] != USB_DT_CS_INTERFACE) {\n\t\t\tdev_err(&intf->dev, \"skipping garbage\\n\");\n\t\t\tgoto next_desc;\n\t\t}\n\t\telength = buffer[0];\n\n\t\tswitch (buffer[2]) {\n\t\tcase USB_CDC_UNION_TYPE: /* we've found it */\n\t\t\tif (elength < sizeof(struct usb_cdc_union_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tif (union_header) {\n\t\t\t\tdev_err(&intf->dev, \"More than one \"\n\t\t\t\t\t\"union descriptor, skipping ...\\n\");\n\t\t\t\tgoto next_desc;\n\t\t\t}\n\t\t\tunion_header = (struct usb_cdc_union_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_COUNTRY_TYPE: /* export through sysfs*/\n\t\t\tif (elength < sizeof(struct usb_cdc_country_functional_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tcfd = (struct usb_cdc_country_functional_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_HEADER_TYPE: /* maybe check version */\n\t\t\tbreak; /* for now we ignore it */\n\t\tcase USB_CDC_ACM_TYPE:\n\t\t\tif (elength < 4)\n\t\t\t\tgoto next_desc;\n\t\t\tac_management_function = buffer[3];\n\t\t\tbreak;\n\t\tcase USB_CDC_CALL_MANAGEMENT_TYPE:\n\t\t\tif (elength < 5)\n\t\t\t\tgoto next_desc;\n\t\t\tcall_management_function = buffer[3];\n\t\t\tcall_interface_num = buffer[4];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * there are LOTS more CDC descriptors that\n\t\t\t * could legitimately be found here.\n\t\t\t */\n\t\t\tdev_dbg(&intf->dev, \"Ignoring descriptor: \"\n\t\t\t\t\t\"type %02x, length %ud\\n\",\n\t\t\t\t\tbuffer[2], elength);\n\t\t\tbreak;\n\t\t}\nnext_desc:\n\t\tbuflen -= elength;\n\t\tbuffer += elength;\n\t}\n\n\tif (!union_header) {\n\t\tif (call_interface_num > 0) {\n\t\t\tdev_dbg(&intf->dev, \"No union descriptor, using call management descriptor\\n\");\n\t\t\t/* quirks for Droids MuIn LCD */\n\t\t\tif (quirks & NO_DATA_INTERFACE)\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t\telse\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = call_interface_num));\n\t\t\tcontrol_interface = intf;\n\t\t} else {\n\t\t\tif (intf->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\t\tdev_dbg(&intf->dev,\"No union descriptor, giving up\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t} else {\n\t\t\t\tdev_warn(&intf->dev,\"No union descriptor, testing for castrated device\\n\");\n\t\t\t\tcombined_interfaces = 1;\n\t\t\t\tcontrol_interface = data_interface = intf;\n\t\t\t\tgoto look_for_collapsed_interface;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, union_header->bMasterInterface0);\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = union_header->bSlaveInterface0));\n\t}\n\n\tif (!control_interface || !data_interface) {\n\t\tdev_dbg(&intf->dev, \"no interfaces\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (data_interface_num != call_interface_num)\n\t\tdev_dbg(&intf->dev, \"Separate call control interface. That is not fully supported.\\n\");\n\n\tif (control_interface == data_interface) {\n\t\t/* some broken devices designed for windows work this way */\n\t\tdev_warn(&intf->dev,\"Control and data interfaces are not separated!\\n\");\n\t\tcombined_interfaces = 1;\n\t\t/* a popular other OS doesn't use it */\n\t\tquirks |= NO_CAP_LINE;\n\t\tif (data_interface->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\tdev_err(&intf->dev, \"This needs exactly 3 endpoints\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\nlook_for_collapsed_interface:\n\t\tfor (i = 0; i < 3; i++) {\n\t\t\tstruct usb_endpoint_descriptor *ep;\n\t\t\tep = &data_interface->cur_altsetting->endpoint[i].desc;\n\n\t\t\tif (usb_endpoint_is_int_in(ep))\n\t\t\t\tepctrl = ep;\n\t\t\telse if (usb_endpoint_is_bulk_out(ep))\n\t\t\t\tepwrite = ep;\n\t\t\telse if (usb_endpoint_is_bulk_in(ep))\n\t\t\t\tepread = ep;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!epctrl || !epread || !epwrite)\n\t\t\treturn -ENODEV;\n\t\telse\n\t\t\tgoto made_compressed_probe;\n\t}\n\nskip_normal_probe:\n\n\t/*workaround for switched interfaces */\n\tif (data_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t!= CDC_DATA_INTERFACE_TYPE) {\n\t\tif (control_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t== CDC_DATA_INTERFACE_TYPE) {\n\t\t\tstruct usb_interface *t;\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Your device has switched interfaces.\\n\");\n\t\t\tt = control_interface;\n\t\t\tcontrol_interface = data_interface;\n\t\t\tdata_interface = t;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Accept probe requests only for the control interface */\n\tif (!combined_interfaces && intf != control_interface)\n\t\treturn -ENODEV;\n\n\tif (!combined_interfaces && usb_interface_claimed(data_interface)) {\n\t\t/* valid in this context */\n\t\tdev_dbg(&intf->dev, \"The data interface isn't available\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\n\tif (data_interface->cur_altsetting->desc.bNumEndpoints < 2 ||\n\t    control_interface->cur_altsetting->desc.bNumEndpoints == 0)\n\t\treturn -EINVAL;\n\n\tepctrl = &control_interface->cur_altsetting->endpoint[0].desc;\n\tepread = &data_interface->cur_altsetting->endpoint[0].desc;\n\tepwrite = &data_interface->cur_altsetting->endpoint[1].desc;\n\n\n\t/* workaround for switched endpoints */\n\tif (!usb_endpoint_dir_in(epread)) {\n\t\t/* descriptors are swapped */\n\t\tstruct usb_endpoint_descriptor *t;\n\t\tdev_dbg(&intf->dev,\n\t\t\t\"The data interface has switched endpoints\\n\");\n\t\tt = epread;\n\t\tepread = epwrite;\n\t\tepwrite = t;\n\t}\nmade_compressed_probe:\n\tdev_dbg(&intf->dev, \"interfaces are valid\\n\");\n\n\tacm = kzalloc(sizeof(struct acm), GFP_KERNEL);\n\tif (acm == NULL)\n\t\tgoto alloc_fail;\n\n\tminor = acm_alloc_minor(acm);\n\tif (minor == ACM_TTY_MINORS) {\n\t\tdev_err(&intf->dev, \"no more free acm devices\\n\");\n\t\tkfree(acm);\n\t\treturn -ENODEV;\n\t}\n\n\tctrlsize = usb_endpoint_maxp(epctrl);\n\treadsize = usb_endpoint_maxp(epread) *\n\t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n\tacm->combined_interfaces = combined_interfaces;\n\tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n\tacm->control = control_interface;\n\tacm->data = data_interface;\n\tacm->minor = minor;\n\tacm->dev = usb_dev;\n\tacm->ctrl_caps = ac_management_function;\n\tif (quirks & NO_CAP_LINE)\n\t\tacm->ctrl_caps &= ~USB_CDC_CAP_LINE;\n\tacm->ctrlsize = ctrlsize;\n\tacm->readsize = readsize;\n\tacm->rx_buflimit = num_rx_buf;\n\tINIT_WORK(&acm->work, acm_softint);\n\tinit_waitqueue_head(&acm->wioctl);\n\tspin_lock_init(&acm->write_lock);\n\tspin_lock_init(&acm->read_lock);\n\tmutex_init(&acm->mutex);\n\tacm->rx_endpoint = usb_rcvbulkpipe(usb_dev, epread->bEndpointAddress);\n\tacm->is_int_ep = usb_endpoint_xfer_int(epread);\n\tif (acm->is_int_ep)\n\t\tacm->bInterval = epread->bInterval;\n\ttty_port_init(&acm->port);\n\tacm->port.ops = &acm_port_ops;\n\tinit_usb_anchor(&acm->delayed);\n\tacm->quirks = quirks;\n\n\tbuf = usb_alloc_coherent(usb_dev, ctrlsize, GFP_KERNEL, &acm->ctrl_dma);\n\tif (!buf)\n\t\tgoto alloc_fail2;\n\tacm->ctrl_buffer = buf;\n\n\tif (acm_write_buffers_alloc(acm) < 0)\n\t\tgoto alloc_fail4;\n\n\tacm->ctrlurb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!acm->ctrlurb)\n\t\tgoto alloc_fail5;\n\n\tfor (i = 0; i < num_rx_buf; i++) {\n\t\tstruct acm_rb *rb = &(acm->read_buffers[i]);\n\t\tstruct urb *urb;\n\n\t\trb->base = usb_alloc_coherent(acm->dev, readsize, GFP_KERNEL,\n\t\t\t\t\t\t\t\t&rb->dma);\n\t\tif (!rb->base)\n\t\t\tgoto alloc_fail6;\n\t\trb->index = i;\n\t\trb->instance = acm;\n\n\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!urb)\n\t\t\tgoto alloc_fail6;\n\n\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\turb->transfer_dma = rb->dma;\n\t\tif (acm->is_int_ep) {\n\t\t\tusb_fill_int_urb(urb, acm->dev,\n\t\t\t\t\t acm->rx_endpoint,\n\t\t\t\t\t rb->base,\n\t\t\t\t\t acm->readsize,\n\t\t\t\t\t acm_read_bulk_callback, rb,\n\t\t\t\t\t acm->bInterval);\n\t\t} else {\n\t\t\tusb_fill_bulk_urb(urb, acm->dev,\n\t\t\t\t\t  acm->rx_endpoint,\n\t\t\t\t\t  rb->base,\n\t\t\t\t\t  acm->readsize,\n\t\t\t\t\t  acm_read_bulk_callback, rb);\n\t\t}\n\n\t\tacm->read_urbs[i] = urb;\n\t\t__set_bit(i, &acm->read_urbs_free);\n\t}\n\tfor (i = 0; i < ACM_NW; i++) {\n\t\tstruct acm_wb *snd = &(acm->wb[i]);\n\n\t\tsnd->urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (snd->urb == NULL)\n\t\t\tgoto alloc_fail7;\n\n\t\tif (usb_endpoint_xfer_int(epwrite))\n\t\t\tusb_fill_int_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndintpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd, epwrite->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndbulkpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd);\n\t\tsnd->urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\tsnd->instance = acm;\n\t}\n\n\tusb_set_intfdata(intf, acm);\n\n\ti = device_create_file(&intf->dev, &dev_attr_bmCapabilities);\n\tif (i < 0)\n\t\tgoto alloc_fail7;\n\n\tif (cfd) { /* export the country data */\n\t\tacm->country_codes = kmalloc(cfd->bLength - 4, GFP_KERNEL);\n\t\tif (!acm->country_codes)\n\t\t\tgoto skip_countries;\n\t\tacm->country_code_size = cfd->bLength - 4;\n\t\tmemcpy(acm->country_codes, (u8 *)&cfd->wCountyCode0,\n\t\t\t\t\t\t\tcfd->bLength - 4);\n\t\tacm->country_rel_date = cfd->iCountryCodeRelDate;\n\n\t\ti = device_create_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\tif (i < 0) {\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\n\t\ti = device_create_file(&intf->dev,\n\t\t\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tif (i < 0) {\n\t\t\tdevice_remove_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\t}\n\nskip_countries:\n\tusb_fill_int_urb(acm->ctrlurb, usb_dev,\n\t\t\t usb_rcvintpipe(usb_dev, epctrl->bEndpointAddress),\n\t\t\t acm->ctrl_buffer, ctrlsize, acm_ctrl_irq, acm,\n\t\t\t /* works around buggy devices */\n\t\t\t epctrl->bInterval ? epctrl->bInterval : 16);\n\tacm->ctrlurb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\tacm->ctrlurb->transfer_dma = acm->ctrl_dma;\n\n\tdev_info(&intf->dev, \"ttyACM%d: USB ACM device\\n\", minor);\n\n\tacm->line.dwDTERate = cpu_to_le32(9600);\n\tacm->line.bDataBits = 8;\n\tacm_set_line(acm, &acm->line);\n\n\tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n\tusb_set_intfdata(data_interface, acm);\n\n\tusb_get_intf(control_interface);\n\ttty_dev = tty_port_register_device(&acm->port, acm_tty_driver, minor,\n\t\t\t&control_interface->dev);\n\tif (IS_ERR(tty_dev)) {\n\t\trv = PTR_ERR(tty_dev);\n\t\tgoto alloc_fail8;\n\t}\n\n\treturn 0;\nalloc_fail8:\n\tif (acm->country_codes) {\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_wCountryCodes);\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tkfree(acm->country_codes);\n\t}\n\tdevice_remove_file(&acm->control->dev, &dev_attr_bmCapabilities);\nalloc_fail7:\n\tusb_set_intfdata(intf, NULL);\n\tfor (i = 0; i < ACM_NW; i++)\n\t\tusb_free_urb(acm->wb[i].urb);\nalloc_fail6:\n\tfor (i = 0; i < num_rx_buf; i++)\n\t\tusb_free_urb(acm->read_urbs[i]);\n\tacm_read_buffers_free(acm);\n\tusb_free_urb(acm->ctrlurb);\nalloc_fail5:\n\tacm_write_buffers_free(acm);\nalloc_fail4:\n\tusb_free_coherent(usb_dev, ctrlsize, acm->ctrl_buffer, acm->ctrl_dma);\nalloc_fail2:\n\tacm_release_minor(acm);\n\tkfree(acm);\nalloc_fail:\n\treturn rv;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int acm_probe(struct usb_interface *intf,\n\t\t     const struct usb_device_id *id)\n{\n\tstruct usb_cdc_union_desc *union_header = NULL;\n\tstruct usb_cdc_country_functional_desc *cfd = NULL;\n\tunsigned char *buffer = intf->altsetting->extra;\n\tint buflen = intf->altsetting->extralen;\n\tstruct usb_interface *control_interface;\n\tstruct usb_interface *data_interface;\n\tstruct usb_endpoint_descriptor *epctrl = NULL;\n\tstruct usb_endpoint_descriptor *epread = NULL;\n\tstruct usb_endpoint_descriptor *epwrite = NULL;\n\tstruct usb_device *usb_dev = interface_to_usbdev(intf);\n\tstruct acm *acm;\n\tint minor;\n\tint ctrlsize, readsize;\n\tu8 *buf;\n\tu8 ac_management_function = 0;\n\tu8 call_management_function = 0;\n\tint call_interface_num = -1;\n\tint data_interface_num = -1;\n\tunsigned long quirks;\n\tint num_rx_buf;\n\tint i;\n\tunsigned int elength = 0;\n\tint combined_interfaces = 0;\n\tstruct device *tty_dev;\n\tint rv = -ENOMEM;\n\n\t/* normal quirks */\n\tquirks = (unsigned long)id->driver_info;\n\n\tif (quirks == IGNORE_DEVICE)\n\t\treturn -ENODEV;\n\n\tnum_rx_buf = (quirks == SINGLE_RX_URB) ? 1 : ACM_NR;\n\n\t/* handle quirks deadly to normal probing*/\n\tif (quirks == NO_UNION_NORMAL) {\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, 1);\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\tgoto skip_normal_probe;\n\t}\n\n\t/* normal probing*/\n\tif (!buffer) {\n\t\tdev_err(&intf->dev, \"Weird descriptor references\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!buflen) {\n\t\tif (intf->cur_altsetting->endpoint &&\n\t\t\t\tintf->cur_altsetting->endpoint->extralen &&\n\t\t\t\tintf->cur_altsetting->endpoint->extra) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Seeking extra descriptors on endpoint\\n\");\n\t\t\tbuflen = intf->cur_altsetting->endpoint->extralen;\n\t\t\tbuffer = intf->cur_altsetting->endpoint->extra;\n\t\t} else {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Zero length descriptor references\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\twhile (buflen > 0) {\n\t\telength = buffer[0];\n\t\tif (!elength) {\n\t\t\tdev_err(&intf->dev, \"skipping garbage byte\\n\");\n\t\t\telength = 1;\n\t\t\tgoto next_desc;\n\t\t}\n\t\tif (buffer[1] != USB_DT_CS_INTERFACE) {\n\t\t\tdev_err(&intf->dev, \"skipping garbage\\n\");\n\t\t\tgoto next_desc;\n\t\t}\n\n\t\tswitch (buffer[2]) {\n\t\tcase USB_CDC_UNION_TYPE: /* we've found it */\n\t\t\tif (elength < sizeof(struct usb_cdc_union_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tif (union_header) {\n\t\t\t\tdev_err(&intf->dev, \"More than one \"\n\t\t\t\t\t\"union descriptor, skipping ...\\n\");\n\t\t\t\tgoto next_desc;\n\t\t\t}\n\t\t\tunion_header = (struct usb_cdc_union_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_COUNTRY_TYPE: /* export through sysfs*/\n\t\t\tif (elength < sizeof(struct usb_cdc_country_functional_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tcfd = (struct usb_cdc_country_functional_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_HEADER_TYPE: /* maybe check version */\n\t\t\tbreak; /* for now we ignore it */\n\t\tcase USB_CDC_ACM_TYPE:\n\t\t\tif (elength < 4)\n\t\t\t\tgoto next_desc;\n\t\t\tac_management_function = buffer[3];\n\t\t\tbreak;\n\t\tcase USB_CDC_CALL_MANAGEMENT_TYPE:\n\t\t\tif (elength < 5)\n\t\t\t\tgoto next_desc;\n\t\t\tcall_management_function = buffer[3];\n\t\t\tcall_interface_num = buffer[4];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * there are LOTS more CDC descriptors that\n\t\t\t * could legitimately be found here.\n\t\t\t */\n\t\t\tdev_dbg(&intf->dev, \"Ignoring descriptor: \"\n\t\t\t\t\t\"type %02x, length %ud\\n\",\n\t\t\t\t\tbuffer[2], elength);\n\t\t\tbreak;\n\t\t}\nnext_desc:\n\t\tbuflen -= elength;\n\t\tbuffer += elength;\n\t}\n\n\tif (!union_header) {\n\t\tif (call_interface_num > 0) {\n\t\t\tdev_dbg(&intf->dev, \"No union descriptor, using call management descriptor\\n\");\n\t\t\t/* quirks for Droids MuIn LCD */\n\t\t\tif (quirks & NO_DATA_INTERFACE)\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t\telse\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = call_interface_num));\n\t\t\tcontrol_interface = intf;\n\t\t} else {\n\t\t\tif (intf->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\t\tdev_dbg(&intf->dev,\"No union descriptor, giving up\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t} else {\n\t\t\t\tdev_warn(&intf->dev,\"No union descriptor, testing for castrated device\\n\");\n\t\t\t\tcombined_interfaces = 1;\n\t\t\t\tcontrol_interface = data_interface = intf;\n\t\t\t\tgoto look_for_collapsed_interface;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, union_header->bMasterInterface0);\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = union_header->bSlaveInterface0));\n\t}\n\n\tif (!control_interface || !data_interface) {\n\t\tdev_dbg(&intf->dev, \"no interfaces\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (data_interface_num != call_interface_num)\n\t\tdev_dbg(&intf->dev, \"Separate call control interface. That is not fully supported.\\n\");\n\n\tif (control_interface == data_interface) {\n\t\t/* some broken devices designed for windows work this way */\n\t\tdev_warn(&intf->dev,\"Control and data interfaces are not separated!\\n\");\n\t\tcombined_interfaces = 1;\n\t\t/* a popular other OS doesn't use it */\n\t\tquirks |= NO_CAP_LINE;\n\t\tif (data_interface->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\tdev_err(&intf->dev, \"This needs exactly 3 endpoints\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\nlook_for_collapsed_interface:\n\t\tfor (i = 0; i < 3; i++) {\n\t\t\tstruct usb_endpoint_descriptor *ep;\n\t\t\tep = &data_interface->cur_altsetting->endpoint[i].desc;\n\n\t\t\tif (usb_endpoint_is_int_in(ep))\n\t\t\t\tepctrl = ep;\n\t\t\telse if (usb_endpoint_is_bulk_out(ep))\n\t\t\t\tepwrite = ep;\n\t\t\telse if (usb_endpoint_is_bulk_in(ep))\n\t\t\t\tepread = ep;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!epctrl || !epread || !epwrite)\n\t\t\treturn -ENODEV;\n\t\telse\n\t\t\tgoto made_compressed_probe;\n\t}\n\nskip_normal_probe:\n\n\t/*workaround for switched interfaces */\n\tif (data_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t!= CDC_DATA_INTERFACE_TYPE) {\n\t\tif (control_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t== CDC_DATA_INTERFACE_TYPE) {\n\t\t\tstruct usb_interface *t;\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Your device has switched interfaces.\\n\");\n\t\t\tt = control_interface;\n\t\t\tcontrol_interface = data_interface;\n\t\t\tdata_interface = t;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Accept probe requests only for the control interface */\n\tif (!combined_interfaces && intf != control_interface)\n\t\treturn -ENODEV;\n\n\tif (!combined_interfaces && usb_interface_claimed(data_interface)) {\n\t\t/* valid in this context */\n\t\tdev_dbg(&intf->dev, \"The data interface isn't available\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\n\tif (data_interface->cur_altsetting->desc.bNumEndpoints < 2 ||\n\t    control_interface->cur_altsetting->desc.bNumEndpoints == 0)\n\t\treturn -EINVAL;\n\n\tepctrl = &control_interface->cur_altsetting->endpoint[0].desc;\n\tepread = &data_interface->cur_altsetting->endpoint[0].desc;\n\tepwrite = &data_interface->cur_altsetting->endpoint[1].desc;\n\n\n\t/* workaround for switched endpoints */\n\tif (!usb_endpoint_dir_in(epread)) {\n\t\t/* descriptors are swapped */\n\t\tstruct usb_endpoint_descriptor *t;\n\t\tdev_dbg(&intf->dev,\n\t\t\t\"The data interface has switched endpoints\\n\");\n\t\tt = epread;\n\t\tepread = epwrite;\n\t\tepwrite = t;\n\t}\nmade_compressed_probe:\n\tdev_dbg(&intf->dev, \"interfaces are valid\\n\");\n\n\tacm = kzalloc(sizeof(struct acm), GFP_KERNEL);\n\tif (acm == NULL)\n\t\tgoto alloc_fail;\n\n\tminor = acm_alloc_minor(acm);\n\tif (minor == ACM_TTY_MINORS) {\n\t\tdev_err(&intf->dev, \"no more free acm devices\\n\");\n\t\tkfree(acm);\n\t\treturn -ENODEV;\n\t}\n\n\tctrlsize = usb_endpoint_maxp(epctrl);\n\treadsize = usb_endpoint_maxp(epread) *\n\t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n\tacm->combined_interfaces = combined_interfaces;\n\tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n\tacm->control = control_interface;\n\tacm->data = data_interface;\n\tacm->minor = minor;\n\tacm->dev = usb_dev;\n\tacm->ctrl_caps = ac_management_function;\n\tif (quirks & NO_CAP_LINE)\n\t\tacm->ctrl_caps &= ~USB_CDC_CAP_LINE;\n\tacm->ctrlsize = ctrlsize;\n\tacm->readsize = readsize;\n\tacm->rx_buflimit = num_rx_buf;\n\tINIT_WORK(&acm->work, acm_softint);\n\tinit_waitqueue_head(&acm->wioctl);\n\tspin_lock_init(&acm->write_lock);\n\tspin_lock_init(&acm->read_lock);\n\tmutex_init(&acm->mutex);\n\tacm->rx_endpoint = usb_rcvbulkpipe(usb_dev, epread->bEndpointAddress);\n\tacm->is_int_ep = usb_endpoint_xfer_int(epread);\n\tif (acm->is_int_ep)\n\t\tacm->bInterval = epread->bInterval;\n\ttty_port_init(&acm->port);\n\tacm->port.ops = &acm_port_ops;\n\tinit_usb_anchor(&acm->delayed);\n\tacm->quirks = quirks;\n\n\tbuf = usb_alloc_coherent(usb_dev, ctrlsize, GFP_KERNEL, &acm->ctrl_dma);\n\tif (!buf)\n\t\tgoto alloc_fail2;\n\tacm->ctrl_buffer = buf;\n\n\tif (acm_write_buffers_alloc(acm) < 0)\n\t\tgoto alloc_fail4;\n\n\tacm->ctrlurb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!acm->ctrlurb)\n\t\tgoto alloc_fail5;\n\n\tfor (i = 0; i < num_rx_buf; i++) {\n\t\tstruct acm_rb *rb = &(acm->read_buffers[i]);\n\t\tstruct urb *urb;\n\n\t\trb->base = usb_alloc_coherent(acm->dev, readsize, GFP_KERNEL,\n\t\t\t\t\t\t\t\t&rb->dma);\n\t\tif (!rb->base)\n\t\t\tgoto alloc_fail6;\n\t\trb->index = i;\n\t\trb->instance = acm;\n\n\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!urb)\n\t\t\tgoto alloc_fail6;\n\n\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\turb->transfer_dma = rb->dma;\n\t\tif (acm->is_int_ep) {\n\t\t\tusb_fill_int_urb(urb, acm->dev,\n\t\t\t\t\t acm->rx_endpoint,\n\t\t\t\t\t rb->base,\n\t\t\t\t\t acm->readsize,\n\t\t\t\t\t acm_read_bulk_callback, rb,\n\t\t\t\t\t acm->bInterval);\n\t\t} else {\n\t\t\tusb_fill_bulk_urb(urb, acm->dev,\n\t\t\t\t\t  acm->rx_endpoint,\n\t\t\t\t\t  rb->base,\n\t\t\t\t\t  acm->readsize,\n\t\t\t\t\t  acm_read_bulk_callback, rb);\n\t\t}\n\n\t\tacm->read_urbs[i] = urb;\n\t\t__set_bit(i, &acm->read_urbs_free);\n\t}\n\tfor (i = 0; i < ACM_NW; i++) {\n\t\tstruct acm_wb *snd = &(acm->wb[i]);\n\n\t\tsnd->urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (snd->urb == NULL)\n\t\t\tgoto alloc_fail7;\n\n\t\tif (usb_endpoint_xfer_int(epwrite))\n\t\t\tusb_fill_int_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndintpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd, epwrite->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndbulkpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd);\n\t\tsnd->urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\tsnd->instance = acm;\n\t}\n\n\tusb_set_intfdata(intf, acm);\n\n\ti = device_create_file(&intf->dev, &dev_attr_bmCapabilities);\n\tif (i < 0)\n\t\tgoto alloc_fail7;\n\n\tif (cfd) { /* export the country data */\n\t\tacm->country_codes = kmalloc(cfd->bLength - 4, GFP_KERNEL);\n\t\tif (!acm->country_codes)\n\t\t\tgoto skip_countries;\n\t\tacm->country_code_size = cfd->bLength - 4;\n\t\tmemcpy(acm->country_codes, (u8 *)&cfd->wCountyCode0,\n\t\t\t\t\t\t\tcfd->bLength - 4);\n\t\tacm->country_rel_date = cfd->iCountryCodeRelDate;\n\n\t\ti = device_create_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\tif (i < 0) {\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\n\t\ti = device_create_file(&intf->dev,\n\t\t\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tif (i < 0) {\n\t\t\tdevice_remove_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\t}\n\nskip_countries:\n\tusb_fill_int_urb(acm->ctrlurb, usb_dev,\n\t\t\t usb_rcvintpipe(usb_dev, epctrl->bEndpointAddress),\n\t\t\t acm->ctrl_buffer, ctrlsize, acm_ctrl_irq, acm,\n\t\t\t /* works around buggy devices */\n\t\t\t epctrl->bInterval ? epctrl->bInterval : 16);\n\tacm->ctrlurb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\tacm->ctrlurb->transfer_dma = acm->ctrl_dma;\n\n\tdev_info(&intf->dev, \"ttyACM%d: USB ACM device\\n\", minor);\n\n\tacm->line.dwDTERate = cpu_to_le32(9600);\n\tacm->line.bDataBits = 8;\n\tacm_set_line(acm, &acm->line);\n\n\tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n\tusb_set_intfdata(data_interface, acm);\n\n\tusb_get_intf(control_interface);\n\ttty_dev = tty_port_register_device(&acm->port, acm_tty_driver, minor,\n\t\t\t&control_interface->dev);\n\tif (IS_ERR(tty_dev)) {\n\t\trv = PTR_ERR(tty_dev);\n\t\tgoto alloc_fail8;\n\t}\n\n\treturn 0;\nalloc_fail8:\n\tif (acm->country_codes) {\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_wCountryCodes);\n\t\tdevice_remove_file(&acm->control->dev,\n\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tkfree(acm->country_codes);\n\t}\n\tdevice_remove_file(&acm->control->dev, &dev_attr_bmCapabilities);\nalloc_fail7:\n\tusb_set_intfdata(intf, NULL);\n\tfor (i = 0; i < ACM_NW; i++)\n\t\tusb_free_urb(acm->wb[i].urb);\nalloc_fail6:\n\tfor (i = 0; i < num_rx_buf; i++)\n\t\tusb_free_urb(acm->read_urbs[i]);\n\tacm_read_buffers_free(acm);\n\tusb_free_urb(acm->ctrlurb);\nalloc_fail5:\n\tacm_write_buffers_free(acm);\nalloc_fail4:\n\tusb_free_coherent(usb_dev, ctrlsize, acm->ctrl_buffer, acm->ctrl_dma);\nalloc_fail2:\n\tacm_release_minor(acm);\n\tkfree(acm);\nalloc_fail:\n\treturn rv;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int acm_probe(struct usb_interface *intf,\n\t\t     const struct usb_device_id *id)\n{\n\tstruct usb_cdc_union_desc *union_header = NULL;\n\tstruct usb_cdc_country_functional_desc *cfd = NULL;\n\tunsigned char *buffer = intf->altsetting->extra;\n\tint buflen = intf->altsetting->extralen;\n\tstruct usb_interface *control_interface;\n\tstruct usb_interface *data_interface;\n\tstruct usb_endpoint_descriptor *epctrl = NULL;\n\tstruct usb_endpoint_descriptor *epread = NULL;\n\tstruct usb_endpoint_descriptor *epwrite = NULL;\n\tstruct usb_device *usb_dev = interface_to_usbdev(intf);\n\tstruct acm *acm;\n\tint minor;\n\tint ctrlsize, readsize;\n\tu8 *buf;\n\tu8 ac_management_function = 0;\n\tu8 call_management_function = 0;\n\tint call_interface_num = -1;\n\tint data_interface_num = -1;\n\tunsigned long quirks;\n\tint num_rx_buf;\n\tint i;\n\tunsigned int elength = 0;\n\tint combined_interfaces = 0;\n\n\t/* normal quirks */\n\tquirks = (unsigned long)id->driver_info;\n\tnum_rx_buf = (quirks == SINGLE_RX_URB) ? 1 : ACM_NR;\n\n\t/* handle quirks deadly to normal probing*/\n\tif (quirks == NO_UNION_NORMAL) {\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, 1);\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\tgoto skip_normal_probe;\n\t}\n\n\t/* normal probing*/\n\tif (!buffer) {\n\t\tdev_err(&intf->dev, \"Weird descriptor references\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!buflen) {\n\t\tif (intf->cur_altsetting->endpoint &&\n\t\t\t\tintf->cur_altsetting->endpoint->extralen &&\n\t\t\t\tintf->cur_altsetting->endpoint->extra) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Seeking extra descriptors on endpoint\\n\");\n\t\t\tbuflen = intf->cur_altsetting->endpoint->extralen;\n\t\t\tbuffer = intf->cur_altsetting->endpoint->extra;\n\t\t} else {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Zero length descriptor references\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\twhile (buflen > 0) {\n\t\tif (buffer[1] != USB_DT_CS_INTERFACE) {\n\t\t\tdev_err(&intf->dev, \"skipping garbage\\n\");\n\t\t\tgoto next_desc;\n\t\t}\n\t\telength = buffer[0];\n\n\t\tswitch (buffer[2]) {\n\t\tcase USB_CDC_UNION_TYPE: /* we've found it */\n\t\t\tif (elength < sizeof(struct usb_cdc_union_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tif (union_header) {\n\t\t\t\tdev_err(&intf->dev, \"More than one \"\n\t\t\t\t\t\"union descriptor, skipping ...\\n\");\n\t\t\t\tgoto next_desc;\n\t\t\t}\n\t\t\tunion_header = (struct usb_cdc_union_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_COUNTRY_TYPE: /* export through sysfs*/\n\t\t\tif (elength < sizeof(struct usb_cdc_country_functional_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tcfd = (struct usb_cdc_country_functional_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_HEADER_TYPE: /* maybe check version */\n\t\t\tbreak; /* for now we ignore it */\n\t\tcase USB_CDC_ACM_TYPE:\n\t\t\tif (elength < 4)\n\t\t\t\tgoto next_desc;\n\t\t\tac_management_function = buffer[3];\n\t\t\tbreak;\n\t\tcase USB_CDC_CALL_MANAGEMENT_TYPE:\n\t\t\tif (elength < 5)\n\t\t\t\tgoto next_desc;\n\t\t\tcall_management_function = buffer[3];\n\t\t\tcall_interface_num = buffer[4];\n\t\t\tif ( (quirks & NOT_A_MODEM) == 0 && (call_management_function & 3) != 3)\n\t\t\t\tdev_err(&intf->dev, \"This device cannot do calls on its own. It is not a modem.\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * there are LOTS more CDC descriptors that\n\t\t\t * could legitimately be found here.\n\t\t\t */\n\t\t\tdev_dbg(&intf->dev, \"Ignoring descriptor: \"\n\t\t\t\t\t\"type %02x, length %ud\\n\",\n\t\t\t\t\tbuffer[2], elength);\n\t\t\tbreak;\n\t\t}\nnext_desc:\n\t\tbuflen -= elength;\n\t\tbuffer += elength;\n\t}\n\n\tif (!union_header) {\n\t\tif (call_interface_num > 0) {\n\t\t\tdev_dbg(&intf->dev, \"No union descriptor, using call management descriptor\\n\");\n\t\t\t/* quirks for Droids MuIn LCD */\n\t\t\tif (quirks & NO_DATA_INTERFACE)\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t\telse\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = call_interface_num));\n\t\t\tcontrol_interface = intf;\n\t\t} else {\n\t\t\tif (intf->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\t\tdev_dbg(&intf->dev,\"No union descriptor, giving up\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t} else {\n\t\t\t\tdev_warn(&intf->dev,\"No union descriptor, testing for castrated device\\n\");\n\t\t\t\tcombined_interfaces = 1;\n\t\t\t\tcontrol_interface = data_interface = intf;\n\t\t\t\tgoto look_for_collapsed_interface;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, union_header->bMasterInterface0);\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = union_header->bSlaveInterface0));\n\t}\n\n\tif (!control_interface || !data_interface) {\n\t\tdev_dbg(&intf->dev, \"no interfaces\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (data_interface_num != call_interface_num)\n\t\tdev_dbg(&intf->dev, \"Separate call control interface. That is not fully supported.\\n\");\n\n\tif (control_interface == data_interface) {\n\t\t/* some broken devices designed for windows work this way */\n\t\tdev_warn(&intf->dev,\"Control and data interfaces are not separated!\\n\");\n\t\tcombined_interfaces = 1;\n\t\t/* a popular other OS doesn't use it */\n\t\tquirks |= NO_CAP_LINE;\n\t\tif (data_interface->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\tdev_err(&intf->dev, \"This needs exactly 3 endpoints\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\nlook_for_collapsed_interface:\n\t\tfor (i = 0; i < 3; i++) {\n\t\t\tstruct usb_endpoint_descriptor *ep;\n\t\t\tep = &data_interface->cur_altsetting->endpoint[i].desc;\n\n\t\t\tif (usb_endpoint_is_int_in(ep))\n\t\t\t\tepctrl = ep;\n\t\t\telse if (usb_endpoint_is_bulk_out(ep))\n\t\t\t\tepwrite = ep;\n\t\t\telse if (usb_endpoint_is_bulk_in(ep))\n\t\t\t\tepread = ep;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!epctrl || !epread || !epwrite)\n\t\t\treturn -ENODEV;\n\t\telse\n\t\t\tgoto made_compressed_probe;\n\t}\n\nskip_normal_probe:\n\n\t/*workaround for switched interfaces */\n\tif (data_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t!= CDC_DATA_INTERFACE_TYPE) {\n\t\tif (control_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t== CDC_DATA_INTERFACE_TYPE) {\n\t\t\tstruct usb_interface *t;\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Your device has switched interfaces.\\n\");\n\t\t\tt = control_interface;\n\t\t\tcontrol_interface = data_interface;\n\t\t\tdata_interface = t;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Accept probe requests only for the control interface */\n\tif (!combined_interfaces && intf != control_interface)\n\t\treturn -ENODEV;\n\n\tif (!combined_interfaces && usb_interface_claimed(data_interface)) {\n\t\t/* valid in this context */\n\t\tdev_dbg(&intf->dev, \"The data interface isn't available\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\n\tif (data_interface->cur_altsetting->desc.bNumEndpoints < 2 ||\n\t    control_interface->cur_altsetting->desc.bNumEndpoints == 0)\n\t\treturn -EINVAL;\n\n\tepctrl = &control_interface->cur_altsetting->endpoint[0].desc;\n\tepread = &data_interface->cur_altsetting->endpoint[0].desc;\n\tepwrite = &data_interface->cur_altsetting->endpoint[1].desc;\n\n\n\t/* workaround for switched endpoints */\n\tif (!usb_endpoint_dir_in(epread)) {\n\t\t/* descriptors are swapped */\n\t\tstruct usb_endpoint_descriptor *t;\n\t\tdev_dbg(&intf->dev,\n\t\t\t\"The data interface has switched endpoints\\n\");\n\t\tt = epread;\n\t\tepread = epwrite;\n\t\tepwrite = t;\n\t}\nmade_compressed_probe:\n\tdev_dbg(&intf->dev, \"interfaces are valid\\n\");\n\tfor (minor = 0; minor < ACM_TTY_MINORS && acm_table[minor]; minor++);\n\n\tif (minor == ACM_TTY_MINORS) {\n\t\tdev_err(&intf->dev, \"no more free acm devices\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tacm = kzalloc(sizeof(struct acm), GFP_KERNEL);\n\tif (acm == NULL) {\n\t\tdev_err(&intf->dev, \"out of memory (acm kzalloc)\\n\");\n\t\tgoto alloc_fail;\n\t}\n\n\tctrlsize = usb_endpoint_maxp(epctrl);\n\treadsize = usb_endpoint_maxp(epread) *\n\t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n\tacm->combined_interfaces = combined_interfaces;\n\tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n\tacm->control = control_interface;\n\tacm->data = data_interface;\n\tacm->minor = minor;\n\tacm->dev = usb_dev;\n\tacm->ctrl_caps = ac_management_function;\n\tif (quirks & NO_CAP_LINE)\n\t\tacm->ctrl_caps &= ~USB_CDC_CAP_LINE;\n\tacm->ctrlsize = ctrlsize;\n\tacm->readsize = readsize;\n\tacm->rx_buflimit = num_rx_buf;\n\tINIT_WORK(&acm->work, acm_softint);\n\tspin_lock_init(&acm->write_lock);\n\tspin_lock_init(&acm->read_lock);\n\tmutex_init(&acm->mutex);\n\tacm->rx_endpoint = usb_rcvbulkpipe(usb_dev, epread->bEndpointAddress);\n\tacm->is_int_ep = usb_endpoint_xfer_int(epread);\n\tif (acm->is_int_ep)\n\t\tacm->bInterval = epread->bInterval;\n\ttty_port_init(&acm->port);\n\tacm->port.ops = &acm_port_ops;\n\tinit_usb_anchor(&acm->delayed);\n\n\tbuf = usb_alloc_coherent(usb_dev, ctrlsize, GFP_KERNEL, &acm->ctrl_dma);\n\tif (!buf) {\n\t\tdev_err(&intf->dev, \"out of memory (ctrl buffer alloc)\\n\");\n\t\tgoto alloc_fail2;\n\t}\n\tacm->ctrl_buffer = buf;\n\n\tif (acm_write_buffers_alloc(acm) < 0) {\n\t\tdev_err(&intf->dev, \"out of memory (write buffer alloc)\\n\");\n\t\tgoto alloc_fail4;\n\t}\n\n\tacm->ctrlurb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!acm->ctrlurb) {\n\t\tdev_err(&intf->dev, \"out of memory (ctrlurb kmalloc)\\n\");\n\t\tgoto alloc_fail5;\n\t}\n\tfor (i = 0; i < num_rx_buf; i++) {\n\t\tstruct acm_rb *rb = &(acm->read_buffers[i]);\n\t\tstruct urb *urb;\n\n\t\trb->base = usb_alloc_coherent(acm->dev, readsize, GFP_KERNEL,\n\t\t\t\t\t\t\t\t&rb->dma);\n\t\tif (!rb->base) {\n\t\t\tdev_err(&intf->dev, \"out of memory \"\n\t\t\t\t\t\"(read bufs usb_alloc_coherent)\\n\");\n\t\t\tgoto alloc_fail6;\n\t\t}\n\t\trb->index = i;\n\t\trb->instance = acm;\n\n\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!urb) {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"out of memory (read urbs usb_alloc_urb)\\n\");\n\t\t\tgoto alloc_fail6;\n\t\t}\n\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\turb->transfer_dma = rb->dma;\n\t\tif (acm->is_int_ep) {\n\t\t\tusb_fill_int_urb(urb, acm->dev,\n\t\t\t\t\t acm->rx_endpoint,\n\t\t\t\t\t rb->base,\n\t\t\t\t\t acm->readsize,\n\t\t\t\t\t acm_read_bulk_callback, rb,\n\t\t\t\t\t acm->bInterval);\n\t\t} else {\n\t\t\tusb_fill_bulk_urb(urb, acm->dev,\n\t\t\t\t\t  acm->rx_endpoint,\n\t\t\t\t\t  rb->base,\n\t\t\t\t\t  acm->readsize,\n\t\t\t\t\t  acm_read_bulk_callback, rb);\n\t\t}\n\n\t\tacm->read_urbs[i] = urb;\n\t\t__set_bit(i, &acm->read_urbs_free);\n\t}\n\tfor (i = 0; i < ACM_NW; i++) {\n\t\tstruct acm_wb *snd = &(acm->wb[i]);\n\n\t\tsnd->urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (snd->urb == NULL) {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"out of memory (write urbs usb_alloc_urb)\\n\");\n\t\t\tgoto alloc_fail7;\n\t\t}\n\n\t\tif (usb_endpoint_xfer_int(epwrite))\n\t\t\tusb_fill_int_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndintpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd, epwrite->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndbulkpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd);\n\t\tsnd->urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\tsnd->instance = acm;\n\t}\n\n\tusb_set_intfdata(intf, acm);\n\n\ti = device_create_file(&intf->dev, &dev_attr_bmCapabilities);\n\tif (i < 0)\n\t\tgoto alloc_fail7;\n\n\tif (cfd) { /* export the country data */\n\t\tacm->country_codes = kmalloc(cfd->bLength - 4, GFP_KERNEL);\n\t\tif (!acm->country_codes)\n\t\t\tgoto skip_countries;\n\t\tacm->country_code_size = cfd->bLength - 4;\n\t\tmemcpy(acm->country_codes, (u8 *)&cfd->wCountyCode0,\n\t\t\t\t\t\t\tcfd->bLength - 4);\n\t\tacm->country_rel_date = cfd->iCountryCodeRelDate;\n\n\t\ti = device_create_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\tif (i < 0) {\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\n\t\ti = device_create_file(&intf->dev,\n\t\t\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tif (i < 0) {\n\t\t\tdevice_remove_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\t}\n\nskip_countries:\n\tusb_fill_int_urb(acm->ctrlurb, usb_dev,\n\t\t\t usb_rcvintpipe(usb_dev, epctrl->bEndpointAddress),\n\t\t\t acm->ctrl_buffer, ctrlsize, acm_ctrl_irq, acm,\n\t\t\t /* works around buggy devices */\n\t\t\t epctrl->bInterval ? epctrl->bInterval : 0xff);\n\tacm->ctrlurb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\tacm->ctrlurb->transfer_dma = acm->ctrl_dma;\n\n\tdev_info(&intf->dev, \"ttyACM%d: USB ACM device\\n\", minor);\n\n\tacm_set_control(acm, acm->ctrlout);\n\n\tacm->line.dwDTERate = cpu_to_le32(9600);\n\tacm->line.bDataBits = 8;\n\tacm_set_line(acm, &acm->line);\n\n\tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n\tusb_set_intfdata(data_interface, acm);\n\n\tusb_get_intf(control_interface);\n\ttty_register_device(acm_tty_driver, minor, &control_interface->dev);\n\n\tacm_table[minor] = acm;\n\n\treturn 0;\nalloc_fail7:\n\tfor (i = 0; i < ACM_NW; i++)\n\t\tusb_free_urb(acm->wb[i].urb);\nalloc_fail6:\n\tfor (i = 0; i < num_rx_buf; i++)\n\t\tusb_free_urb(acm->read_urbs[i]);\n\tacm_read_buffers_free(acm);\n\tusb_free_urb(acm->ctrlurb);\nalloc_fail5:\n\tacm_write_buffers_free(acm);\nalloc_fail4:\n\tusb_free_coherent(usb_dev, ctrlsize, acm->ctrl_buffer, acm->ctrl_dma);\nalloc_fail2:\n\tkfree(acm);\nalloc_fail:\n\treturn -ENOMEM;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int acm_probe(struct usb_interface *intf,\n\t\t     const struct usb_device_id *id)\n{\n\tstruct usb_cdc_union_desc *union_header = NULL;\n\tstruct usb_cdc_country_functional_desc *cfd = NULL;\n\tunsigned char *buffer = intf->altsetting->extra;\n\tint buflen = intf->altsetting->extralen;\n\tstruct usb_interface *control_interface;\n\tstruct usb_interface *data_interface;\n\tstruct usb_endpoint_descriptor *epctrl = NULL;\n\tstruct usb_endpoint_descriptor *epread = NULL;\n\tstruct usb_endpoint_descriptor *epwrite = NULL;\n\tstruct usb_device *usb_dev = interface_to_usbdev(intf);\n\tstruct acm *acm;\n\tint minor;\n\tint ctrlsize, readsize;\n\tu8 *buf;\n\tu8 ac_management_function = 0;\n\tu8 call_management_function = 0;\n\tint call_interface_num = -1;\n\tint data_interface_num = -1;\n\tunsigned long quirks;\n\tint num_rx_buf;\n\tint i;\n\tunsigned int elength = 0;\n\tint combined_interfaces = 0;\n\n\t/* normal quirks */\n\tquirks = (unsigned long)id->driver_info;\n\tnum_rx_buf = (quirks == SINGLE_RX_URB) ? 1 : ACM_NR;\n\n\t/* handle quirks deadly to normal probing*/\n\tif (quirks == NO_UNION_NORMAL) {\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, 1);\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\tgoto skip_normal_probe;\n\t}\n\n\t/* normal probing*/\n\tif (!buffer) {\n\t\tdev_err(&intf->dev, \"Weird descriptor references\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!buflen) {\n\t\tif (intf->cur_altsetting->endpoint &&\n\t\t\t\tintf->cur_altsetting->endpoint->extralen &&\n\t\t\t\tintf->cur_altsetting->endpoint->extra) {\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Seeking extra descriptors on endpoint\\n\");\n\t\t\tbuflen = intf->cur_altsetting->endpoint->extralen;\n\t\t\tbuffer = intf->cur_altsetting->endpoint->extra;\n\t\t} else {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"Zero length descriptor references\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\twhile (buflen > 0) {\n\t\telength = buffer[0];\n\t\tif (!elength) {\n\t\t\tdev_err(&intf->dev, \"skipping garbage byte\\n\");\n\t\t\telength = 1;\n\t\t\tgoto next_desc;\n\t\t}\n\t\tif (buffer[1] != USB_DT_CS_INTERFACE) {\n\t\t\tdev_err(&intf->dev, \"skipping garbage\\n\");\n\t\t\tgoto next_desc;\n\t\t}\n\n\t\tswitch (buffer[2]) {\n\t\tcase USB_CDC_UNION_TYPE: /* we've found it */\n\t\t\tif (elength < sizeof(struct usb_cdc_union_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tif (union_header) {\n\t\t\t\tdev_err(&intf->dev, \"More than one \"\n\t\t\t\t\t\"union descriptor, skipping ...\\n\");\n\t\t\t\tgoto next_desc;\n\t\t\t}\n\t\t\tunion_header = (struct usb_cdc_union_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_COUNTRY_TYPE: /* export through sysfs*/\n\t\t\tif (elength < sizeof(struct usb_cdc_country_functional_desc))\n\t\t\t\tgoto next_desc;\n\t\t\tcfd = (struct usb_cdc_country_functional_desc *)buffer;\n\t\t\tbreak;\n\t\tcase USB_CDC_HEADER_TYPE: /* maybe check version */\n\t\t\tbreak; /* for now we ignore it */\n\t\tcase USB_CDC_ACM_TYPE:\n\t\t\tif (elength < 4)\n\t\t\t\tgoto next_desc;\n\t\t\tac_management_function = buffer[3];\n\t\t\tbreak;\n\t\tcase USB_CDC_CALL_MANAGEMENT_TYPE:\n\t\t\tif (elength < 5)\n\t\t\t\tgoto next_desc;\n\t\t\tcall_management_function = buffer[3];\n\t\t\tcall_interface_num = buffer[4];\n\t\t\tif ( (quirks & NOT_A_MODEM) == 0 && (call_management_function & 3) != 3)\n\t\t\t\tdev_err(&intf->dev, \"This device cannot do calls on its own. It is not a modem.\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t/*\n\t\t\t * there are LOTS more CDC descriptors that\n\t\t\t * could legitimately be found here.\n\t\t\t */\n\t\t\tdev_dbg(&intf->dev, \"Ignoring descriptor: \"\n\t\t\t\t\t\"type %02x, length %ud\\n\",\n\t\t\t\t\tbuffer[2], elength);\n\t\t\tbreak;\n\t\t}\nnext_desc:\n\t\tbuflen -= elength;\n\t\tbuffer += elength;\n\t}\n\n\tif (!union_header) {\n\t\tif (call_interface_num > 0) {\n\t\t\tdev_dbg(&intf->dev, \"No union descriptor, using call management descriptor\\n\");\n\t\t\t/* quirks for Droids MuIn LCD */\n\t\t\tif (quirks & NO_DATA_INTERFACE)\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, 0);\n\t\t\telse\n\t\t\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = call_interface_num));\n\t\t\tcontrol_interface = intf;\n\t\t} else {\n\t\t\tif (intf->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\t\tdev_dbg(&intf->dev,\"No union descriptor, giving up\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t} else {\n\t\t\t\tdev_warn(&intf->dev,\"No union descriptor, testing for castrated device\\n\");\n\t\t\t\tcombined_interfaces = 1;\n\t\t\t\tcontrol_interface = data_interface = intf;\n\t\t\t\tgoto look_for_collapsed_interface;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tcontrol_interface = usb_ifnum_to_if(usb_dev, union_header->bMasterInterface0);\n\t\tdata_interface = usb_ifnum_to_if(usb_dev, (data_interface_num = union_header->bSlaveInterface0));\n\t}\n\n\tif (!control_interface || !data_interface) {\n\t\tdev_dbg(&intf->dev, \"no interfaces\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (data_interface_num != call_interface_num)\n\t\tdev_dbg(&intf->dev, \"Separate call control interface. That is not fully supported.\\n\");\n\n\tif (control_interface == data_interface) {\n\t\t/* some broken devices designed for windows work this way */\n\t\tdev_warn(&intf->dev,\"Control and data interfaces are not separated!\\n\");\n\t\tcombined_interfaces = 1;\n\t\t/* a popular other OS doesn't use it */\n\t\tquirks |= NO_CAP_LINE;\n\t\tif (data_interface->cur_altsetting->desc.bNumEndpoints != 3) {\n\t\t\tdev_err(&intf->dev, \"This needs exactly 3 endpoints\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\nlook_for_collapsed_interface:\n\t\tfor (i = 0; i < 3; i++) {\n\t\t\tstruct usb_endpoint_descriptor *ep;\n\t\t\tep = &data_interface->cur_altsetting->endpoint[i].desc;\n\n\t\t\tif (usb_endpoint_is_int_in(ep))\n\t\t\t\tepctrl = ep;\n\t\t\telse if (usb_endpoint_is_bulk_out(ep))\n\t\t\t\tepwrite = ep;\n\t\t\telse if (usb_endpoint_is_bulk_in(ep))\n\t\t\t\tepread = ep;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!epctrl || !epread || !epwrite)\n\t\t\treturn -ENODEV;\n\t\telse\n\t\t\tgoto made_compressed_probe;\n\t}\n\nskip_normal_probe:\n\n\t/*workaround for switched interfaces */\n\tif (data_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t!= CDC_DATA_INTERFACE_TYPE) {\n\t\tif (control_interface->cur_altsetting->desc.bInterfaceClass\n\t\t\t\t\t\t== CDC_DATA_INTERFACE_TYPE) {\n\t\t\tstruct usb_interface *t;\n\t\t\tdev_dbg(&intf->dev,\n\t\t\t\t\"Your device has switched interfaces.\\n\");\n\t\t\tt = control_interface;\n\t\t\tcontrol_interface = data_interface;\n\t\t\tdata_interface = t;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Accept probe requests only for the control interface */\n\tif (!combined_interfaces && intf != control_interface)\n\t\treturn -ENODEV;\n\n\tif (!combined_interfaces && usb_interface_claimed(data_interface)) {\n\t\t/* valid in this context */\n\t\tdev_dbg(&intf->dev, \"The data interface isn't available\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\n\tif (data_interface->cur_altsetting->desc.bNumEndpoints < 2 ||\n\t    control_interface->cur_altsetting->desc.bNumEndpoints == 0)\n\t\treturn -EINVAL;\n\n\tepctrl = &control_interface->cur_altsetting->endpoint[0].desc;\n\tepread = &data_interface->cur_altsetting->endpoint[0].desc;\n\tepwrite = &data_interface->cur_altsetting->endpoint[1].desc;\n\n\n\t/* workaround for switched endpoints */\n\tif (!usb_endpoint_dir_in(epread)) {\n\t\t/* descriptors are swapped */\n\t\tstruct usb_endpoint_descriptor *t;\n\t\tdev_dbg(&intf->dev,\n\t\t\t\"The data interface has switched endpoints\\n\");\n\t\tt = epread;\n\t\tepread = epwrite;\n\t\tepwrite = t;\n\t}\nmade_compressed_probe:\n\tdev_dbg(&intf->dev, \"interfaces are valid\\n\");\n\tfor (minor = 0; minor < ACM_TTY_MINORS && acm_table[minor]; minor++);\n\n\tif (minor == ACM_TTY_MINORS) {\n\t\tdev_err(&intf->dev, \"no more free acm devices\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tacm = kzalloc(sizeof(struct acm), GFP_KERNEL);\n\tif (acm == NULL) {\n\t\tdev_err(&intf->dev, \"out of memory (acm kzalloc)\\n\");\n\t\tgoto alloc_fail;\n\t}\n\n\tctrlsize = usb_endpoint_maxp(epctrl);\n\treadsize = usb_endpoint_maxp(epread) *\n\t\t\t\t(quirks == SINGLE_RX_URB ? 1 : 2);\n\tacm->combined_interfaces = combined_interfaces;\n\tacm->writesize = usb_endpoint_maxp(epwrite) * 20;\n\tacm->control = control_interface;\n\tacm->data = data_interface;\n\tacm->minor = minor;\n\tacm->dev = usb_dev;\n\tacm->ctrl_caps = ac_management_function;\n\tif (quirks & NO_CAP_LINE)\n\t\tacm->ctrl_caps &= ~USB_CDC_CAP_LINE;\n\tacm->ctrlsize = ctrlsize;\n\tacm->readsize = readsize;\n\tacm->rx_buflimit = num_rx_buf;\n\tINIT_WORK(&acm->work, acm_softint);\n\tspin_lock_init(&acm->write_lock);\n\tspin_lock_init(&acm->read_lock);\n\tmutex_init(&acm->mutex);\n\tacm->rx_endpoint = usb_rcvbulkpipe(usb_dev, epread->bEndpointAddress);\n\tacm->is_int_ep = usb_endpoint_xfer_int(epread);\n\tif (acm->is_int_ep)\n\t\tacm->bInterval = epread->bInterval;\n\ttty_port_init(&acm->port);\n\tacm->port.ops = &acm_port_ops;\n\tinit_usb_anchor(&acm->delayed);\n\n\tbuf = usb_alloc_coherent(usb_dev, ctrlsize, GFP_KERNEL, &acm->ctrl_dma);\n\tif (!buf) {\n\t\tdev_err(&intf->dev, \"out of memory (ctrl buffer alloc)\\n\");\n\t\tgoto alloc_fail2;\n\t}\n\tacm->ctrl_buffer = buf;\n\n\tif (acm_write_buffers_alloc(acm) < 0) {\n\t\tdev_err(&intf->dev, \"out of memory (write buffer alloc)\\n\");\n\t\tgoto alloc_fail4;\n\t}\n\n\tacm->ctrlurb = usb_alloc_urb(0, GFP_KERNEL);\n\tif (!acm->ctrlurb) {\n\t\tdev_err(&intf->dev, \"out of memory (ctrlurb kmalloc)\\n\");\n\t\tgoto alloc_fail5;\n\t}\n\tfor (i = 0; i < num_rx_buf; i++) {\n\t\tstruct acm_rb *rb = &(acm->read_buffers[i]);\n\t\tstruct urb *urb;\n\n\t\trb->base = usb_alloc_coherent(acm->dev, readsize, GFP_KERNEL,\n\t\t\t\t\t\t\t\t&rb->dma);\n\t\tif (!rb->base) {\n\t\t\tdev_err(&intf->dev, \"out of memory \"\n\t\t\t\t\t\"(read bufs usb_alloc_coherent)\\n\");\n\t\t\tgoto alloc_fail6;\n\t\t}\n\t\trb->index = i;\n\t\trb->instance = acm;\n\n\t\turb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!urb) {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"out of memory (read urbs usb_alloc_urb)\\n\");\n\t\t\tgoto alloc_fail6;\n\t\t}\n\t\turb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\turb->transfer_dma = rb->dma;\n\t\tif (acm->is_int_ep) {\n\t\t\tusb_fill_int_urb(urb, acm->dev,\n\t\t\t\t\t acm->rx_endpoint,\n\t\t\t\t\t rb->base,\n\t\t\t\t\t acm->readsize,\n\t\t\t\t\t acm_read_bulk_callback, rb,\n\t\t\t\t\t acm->bInterval);\n\t\t} else {\n\t\t\tusb_fill_bulk_urb(urb, acm->dev,\n\t\t\t\t\t  acm->rx_endpoint,\n\t\t\t\t\t  rb->base,\n\t\t\t\t\t  acm->readsize,\n\t\t\t\t\t  acm_read_bulk_callback, rb);\n\t\t}\n\n\t\tacm->read_urbs[i] = urb;\n\t\t__set_bit(i, &acm->read_urbs_free);\n\t}\n\tfor (i = 0; i < ACM_NW; i++) {\n\t\tstruct acm_wb *snd = &(acm->wb[i]);\n\n\t\tsnd->urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (snd->urb == NULL) {\n\t\t\tdev_err(&intf->dev,\n\t\t\t\t\"out of memory (write urbs usb_alloc_urb)\\n\");\n\t\t\tgoto alloc_fail7;\n\t\t}\n\n\t\tif (usb_endpoint_xfer_int(epwrite))\n\t\t\tusb_fill_int_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndintpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd, epwrite->bInterval);\n\t\telse\n\t\t\tusb_fill_bulk_urb(snd->urb, usb_dev,\n\t\t\t\tusb_sndbulkpipe(usb_dev, epwrite->bEndpointAddress),\n\t\t\t\tNULL, acm->writesize, acm_write_bulk, snd);\n\t\tsnd->urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\t\tsnd->instance = acm;\n\t}\n\n\tusb_set_intfdata(intf, acm);\n\n\ti = device_create_file(&intf->dev, &dev_attr_bmCapabilities);\n\tif (i < 0)\n\t\tgoto alloc_fail7;\n\n\tif (cfd) { /* export the country data */\n\t\tacm->country_codes = kmalloc(cfd->bLength - 4, GFP_KERNEL);\n\t\tif (!acm->country_codes)\n\t\t\tgoto skip_countries;\n\t\tacm->country_code_size = cfd->bLength - 4;\n\t\tmemcpy(acm->country_codes, (u8 *)&cfd->wCountyCode0,\n\t\t\t\t\t\t\tcfd->bLength - 4);\n\t\tacm->country_rel_date = cfd->iCountryCodeRelDate;\n\n\t\ti = device_create_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\tif (i < 0) {\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\n\t\ti = device_create_file(&intf->dev,\n\t\t\t\t\t\t&dev_attr_iCountryCodeRelDate);\n\t\tif (i < 0) {\n\t\t\tdevice_remove_file(&intf->dev, &dev_attr_wCountryCodes);\n\t\t\tkfree(acm->country_codes);\n\t\t\tacm->country_codes = NULL;\n\t\t\tacm->country_code_size = 0;\n\t\t\tgoto skip_countries;\n\t\t}\n\t}\n\nskip_countries:\n\tusb_fill_int_urb(acm->ctrlurb, usb_dev,\n\t\t\t usb_rcvintpipe(usb_dev, epctrl->bEndpointAddress),\n\t\t\t acm->ctrl_buffer, ctrlsize, acm_ctrl_irq, acm,\n\t\t\t /* works around buggy devices */\n\t\t\t epctrl->bInterval ? epctrl->bInterval : 0xff);\n\tacm->ctrlurb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\n\tacm->ctrlurb->transfer_dma = acm->ctrl_dma;\n\n\tdev_info(&intf->dev, \"ttyACM%d: USB ACM device\\n\", minor);\n\n\tacm_set_control(acm, acm->ctrlout);\n\n\tacm->line.dwDTERate = cpu_to_le32(9600);\n\tacm->line.bDataBits = 8;\n\tacm_set_line(acm, &acm->line);\n\n\tusb_driver_claim_interface(&acm_driver, data_interface, acm);\n\tusb_set_intfdata(data_interface, acm);\n\n\tusb_get_intf(control_interface);\n\ttty_register_device(acm_tty_driver, minor, &control_interface->dev);\n\n\tacm_table[minor] = acm;\n\n\treturn 0;\nalloc_fail7:\n\tfor (i = 0; i < ACM_NW; i++)\n\t\tusb_free_urb(acm->wb[i].urb);\nalloc_fail6:\n\tfor (i = 0; i < num_rx_buf; i++)\n\t\tusb_free_urb(acm->read_urbs[i]);\n\tacm_read_buffers_free(acm);\n\tusb_free_urb(acm->ctrlurb);\nalloc_fail5:\n\tacm_write_buffers_free(acm);\nalloc_fail4:\n\tusb_free_coherent(usb_dev, ctrlsize, acm->ctrl_buffer, acm->ctrl_dma);\nalloc_fail2:\n\tkfree(acm);\nalloc_fail:\n\treturn -ENOMEM;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tmap = dm_get_live_table(md, &srcu_idx);\n\n\tspin_lock(&_minor_lock);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\tif (dm_request_based(md) && md->kworker_task)\n\t\tflush_kthread_worker(&md->kworker);\n\n\t/*\n\t * Take suspend_lock so that presuspend and postsuspend methods\n\t * do not race with internal suspend.\n\t */\n\tmutex_lock(&md->suspend_lock);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\tmutex_unlock(&md->suspend_lock);\n\n\t/* dm_put_live_table must be before msleep, otherwise deadlock is possible */\n\tdm_put_live_table(md, srcu_idx);\n\n\t/*\n\t * Rare, but there may be I/O requests still going to complete,\n\t * for example.  Wait for all references to disappear.\n\t * No one should increment the reference count of the mapped_device,\n\t * after the mapped_device state becomes DMF_FREEING.\n\t */\n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tmsleep(1);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_sysfs_exit(md);\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tspin_lock(&_minor_lock);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\tif (dm_request_based(md) && md->kworker_task)\n\t\tflush_kthread_worker(&md->kworker);\n\n\t/*\n\t * Take suspend_lock so that presuspend and postsuspend methods\n\t * do not race with internal suspend.\n\t */\n\tmutex_lock(&md->suspend_lock);\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\t/* dm_put_live_table must be before msleep, otherwise deadlock is possible */\n\tdm_put_live_table(md, srcu_idx);\n\tmutex_unlock(&md->suspend_lock);\n\n\t/*\n\t * Rare, but there may be I/O requests still going to complete,\n\t * for example.  Wait for all references to disappear.\n\t * No one should increment the reference count of the mapped_device,\n\t * after the mapped_device state becomes DMF_FREEING.\n\t */\n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tmsleep(1);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_sysfs_exit(md);\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tspin_lock(&_minor_lock);\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\t/*\n\t * Take suspend_lock so that presuspend and postsuspend methods\n\t * do not race with internal suspend.\n\t */\n\tmutex_lock(&md->suspend_lock);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\tmutex_unlock(&md->suspend_lock);\n\n\t/* dm_put_live_table must be before msleep, otherwise deadlock is possible */\n\tdm_put_live_table(md, srcu_idx);\n\n\t/*\n\t * Rare, but there may be I/O requests still going to complete,\n\t * for example.  Wait for all references to disappear.\n\t * No one should increment the reference count of the mapped_device,\n\t * after the mapped_device state becomes DMF_FREEING.\n\t */\n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tmsleep(1);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_sysfs_exit(md);\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tspin_lock(&_minor_lock);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\t/*\n\t * Take suspend_lock so that presuspend and postsuspend methods\n\t * do not race with internal suspend.\n\t */\n\tmutex_lock(&md->suspend_lock);\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\t/* dm_put_live_table must be before msleep, otherwise deadlock is possible */\n\tdm_put_live_table(md, srcu_idx);\n\tmutex_unlock(&md->suspend_lock);\n\n\t/*\n\t * Rare, but there may be I/O requests still going to complete,\n\t * for example.  Wait for all references to disappear.\n\t * No one should increment the reference count of the mapped_device,\n\t * after the mapped_device state becomes DMF_FREEING.\n\t */\n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tmsleep(1);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_sysfs_exit(md);\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nacpi_status\nacpi_ds_eval_region_operands(struct acpi_walk_state *walk_state,\n\t\t\t     union acpi_parse_object *op)\n{\n\tacpi_status status;\n\tunion acpi_operand_object *obj_desc;\n\tunion acpi_operand_object *operand_desc;\n\tstruct acpi_namespace_node *node;\n\tunion acpi_parse_object *next_op;\n\n\tACPI_FUNCTION_TRACE_PTR(ds_eval_region_operands, op);\n\n\t/*\n\t * This is where we evaluate the address and length fields of the\n\t * op_region declaration\n\t */\n\tnode = op->common.node;\n\n\t/* next_op points to the op that holds the space_ID */\n\n\tnext_op = op->common.value.arg;\n\n\t/* next_op points to address op */\n\n\tnext_op = next_op->common.next;\n\n\t/* Evaluate/create the address and length operands */\n\n\tstatus = acpi_ds_create_operands(walk_state, next_op);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\t/* Resolve the length and address operands to numbers */\n\n\tstatus =\n\t    acpi_ex_resolve_operands(op->common.aml_opcode, ACPI_WALK_OPERANDS,\n\t\t\t\t     walk_state);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\tobj_desc = acpi_ns_get_attached_object(node);\n\tif (!obj_desc) {\n\t\treturn_ACPI_STATUS(AE_NOT_EXIST);\n\t}\n\n\t/*\n\t * Get the length operand and save it\n\t * (at Top of stack)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 1];\n\n\tobj_desc->region.length = (u32) operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\t/*\n\t * Get the address and save it\n\t * (at top of stack - 1)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 2];\n\n\tobj_desc->region.address = (acpi_physical_address)\n\t    operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\tACPI_DEBUG_PRINT((ACPI_DB_EXEC, \"RgnObj %p Addr %8.8X%8.8X Len %X\\n\",\n\t\t\t  obj_desc,\n\t\t\t  ACPI_FORMAT_UINT64(obj_desc->region.address),\n\t\t\t  obj_desc->region.length));\n\n\t/* Now the address and length are valid for this opregion */\n\n\tobj_desc->region.flags |= AOPOBJ_DATA_VALID;\n\treturn_ACPI_STATUS(status);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nacpi_status\nacpi_ds_eval_region_operands(struct acpi_walk_state *walk_state,\n\t\t\t     union acpi_parse_object *op)\n{\n\tacpi_status status;\n\tunion acpi_operand_object *obj_desc;\n\tunion acpi_operand_object *operand_desc;\n\tstruct acpi_namespace_node *node;\n\tunion acpi_parse_object *next_op;\n\n\tACPI_FUNCTION_TRACE_PTR(ds_eval_region_operands, op);\n\n\t/*\n\t * This is where we evaluate the address and length fields of the\n\t * op_region declaration\n\t */\n\tnode = op->common.node;\n\n\t/* next_op points to the op that holds the space_ID */\n\n\tnext_op = op->common.value.arg;\n\n\t/* next_op points to address op */\n\n\tnext_op = next_op->common.next;\n\n\t/* Evaluate/create the address and length operands */\n\n\tstatus = acpi_ds_create_operands(walk_state, next_op);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\t/* Resolve the length and address operands to numbers */\n\n\tstatus =\n\t    acpi_ex_resolve_operands(op->common.aml_opcode, ACPI_WALK_OPERANDS,\n\t\t\t\t     walk_state);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\tobj_desc = acpi_ns_get_attached_object(node);\n\tif (!obj_desc) {\n\t\treturn_ACPI_STATUS(AE_NOT_EXIST);\n\t}\n\n\t/*\n\t * Get the length operand and save it\n\t * (at Top of stack)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 1];\n\n\tobj_desc->region.length = (u32) operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\t/*\n\t * Get the address and save it\n\t * (at top of stack - 1)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 2];\n\n\tobj_desc->region.address = (acpi_physical_address)\n\t    operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\tACPI_DEBUG_PRINT((ACPI_DB_EXEC, \"RgnObj %p Addr %8.8X%8.8X Len %X\\n\",\n\t\t\t  obj_desc,\n\t\t\t  ACPI_FORMAT_UINT64(obj_desc->region.address),\n\t\t\t  obj_desc->region.length));\n\n\tstatus = acpi_ut_add_address_range(obj_desc->region.space_id,\n\t\t\t\t\t   obj_desc->region.address,\n\t\t\t\t\t   obj_desc->region.length, node);\n\n\t/* Now the address and length are valid for this opregion */\n\n\tobj_desc->region.flags |= AOPOBJ_DATA_VALID;\n\treturn_ACPI_STATUS(status);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nacpi_status\nacpi_ds_eval_region_operands(struct acpi_walk_state *walk_state,\n\t\t\t     union acpi_parse_object *op)\n{\n\tacpi_status status;\n\tunion acpi_operand_object *obj_desc;\n\tunion acpi_operand_object *operand_desc;\n\tstruct acpi_namespace_node *node;\n\tunion acpi_parse_object *next_op;\n\n\tACPI_FUNCTION_TRACE_PTR(ds_eval_region_operands, op);\n\n\t/*\n\t * This is where we evaluate the address and length fields of the\n\t * op_region declaration\n\t */\n\tnode = op->common.node;\n\n\t/* next_op points to the op that holds the space_ID */\n\n\tnext_op = op->common.value.arg;\n\n\t/* next_op points to address op */\n\n\tnext_op = next_op->common.next;\n\n\t/* Evaluate/create the address and length operands */\n\n\tstatus = acpi_ds_create_operands(walk_state, next_op);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\t/* Resolve the length and address operands to numbers */\n\n\tstatus = acpi_ex_resolve_operands(op->common.aml_opcode,\n\t\t\t\t\t  ACPI_WALK_OPERANDS, walk_state);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\tobj_desc = acpi_ns_get_attached_object(node);\n\tif (!obj_desc) {\n\t\treturn_ACPI_STATUS(AE_NOT_EXIST);\n\t}\n\n\t/*\n\t * Get the length operand and save it\n\t * (at Top of stack)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 1];\n\n\tobj_desc->region.length = (u32) operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\t/*\n\t * Get the address and save it\n\t * (at top of stack - 1)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 2];\n\n\tobj_desc->region.address = (acpi_physical_address)\n\t    operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\tACPI_DEBUG_PRINT((ACPI_DB_EXEC, \"RgnObj %p Addr %8.8X%8.8X Len %X\\n\",\n\t\t\t  obj_desc,\n\t\t\t  ACPI_FORMAT_UINT64(obj_desc->region.address),\n\t\t\t  obj_desc->region.length));\n\n\t/* Now the address and length are valid for this opregion */\n\n\tobj_desc->region.flags |= AOPOBJ_DATA_VALID;\n\n\treturn_ACPI_STATUS(status);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nacpi_status\nacpi_ds_eval_region_operands(struct acpi_walk_state *walk_state,\n\t\t\t     union acpi_parse_object *op)\n{\n\tacpi_status status;\n\tunion acpi_operand_object *obj_desc;\n\tunion acpi_operand_object *operand_desc;\n\tstruct acpi_namespace_node *node;\n\tunion acpi_parse_object *next_op;\n\n\tACPI_FUNCTION_TRACE_PTR(ds_eval_region_operands, op);\n\n\t/*\n\t * This is where we evaluate the address and length fields of the\n\t * op_region declaration\n\t */\n\tnode = op->common.node;\n\n\t/* next_op points to the op that holds the space_ID */\n\n\tnext_op = op->common.value.arg;\n\n\t/* next_op points to address op */\n\n\tnext_op = next_op->common.next;\n\n\t/* Evaluate/create the address and length operands */\n\n\tstatus = acpi_ds_create_operands(walk_state, next_op);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\t/* Resolve the length and address operands to numbers */\n\n\tstatus = acpi_ex_resolve_operands(op->common.aml_opcode,\n\t\t\t\t\t  ACPI_WALK_OPERANDS, walk_state);\n\tif (ACPI_FAILURE(status)) {\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\tobj_desc = acpi_ns_get_attached_object(node);\n\tif (!obj_desc) {\n\t\treturn_ACPI_STATUS(AE_NOT_EXIST);\n\t}\n\n\t/*\n\t * Get the length operand and save it\n\t * (at Top of stack)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 1];\n\n\tobj_desc->region.length = (u32) operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\t/*\n\t * Get the address and save it\n\t * (at top of stack - 1)\n\t */\n\toperand_desc = walk_state->operands[walk_state->num_operands - 2];\n\n\tobj_desc->region.address = (acpi_physical_address)\n\t    operand_desc->integer.value;\n\tacpi_ut_remove_reference(operand_desc);\n\n\tACPI_DEBUG_PRINT((ACPI_DB_EXEC, \"RgnObj %p Addr %8.8X%8.8X Len %X\\n\",\n\t\t\t  obj_desc,\n\t\t\t  ACPI_FORMAT_UINT64(obj_desc->region.address),\n\t\t\t  obj_desc->region.length));\n\n\tstatus = acpi_ut_add_address_range(obj_desc->region.space_id,\n\t\t\t\t\t   obj_desc->region.address,\n\t\t\t\t\t   obj_desc->region.length, node);\n\n\t/* Now the address and length are valid for this opregion */\n\n\tobj_desc->region.flags |= AOPOBJ_DATA_VALID;\n\n\treturn_ACPI_STATUS(status);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\n_truncate(struct ubifs_info *c, const struct inode *inode,\n\t\t       loff_t old_size, loff_t new_size)\n{\n\tunion ubifs_key key, to_key;\n\tstruct ubifs_ino_node *ino;\n\tstruct ubifs_trun_node *trun;\n\tstruct ubifs_data_node *uninitialized_var(dn);\n\tint err, dlen, len, lnum, offs, bit, sz, sync = IS_SYNC(inode);\n\tstruct ubifs_inode *ui = ubifs_inode(inode);\n\tino_t inum = inode->i_ino;\n\tunsigned int blk;\n\n\tdbg_jnl(\"ino %lu, size %lld -> %lld\",\n\t\t(unsigned long)inum, old_size, new_size);\n\tubifs_assert(!ui->data_len);\n\tubifs_assert(S_ISREG(inode->i_mode));\n\tubifs_assert(mutex_is_locked(&ui->ui_mutex));\n\n\tsz = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ +\n\t     UBIFS_MAX_DATA_NODE_SZ * WORST_COMPR_FACTOR;\n\tino = kmalloc(sz, GFP_NOFS);\n\tif (!ino)\n\t\treturn -ENOMEM;\n\n\ttrun = (void *)ino + UBIFS_INO_NODE_SZ;\n\ttrun->ch.node_type = UBIFS_TRUN_NODE;\n\ttrun->inum = cpu_to_le32(inum);\n\ttrun->old_size = cpu_to_le64(old_size);\n\ttrun->new_size = cpu_to_le64(new_size);\n\tzero_trun_node_unused(trun);\n\n\tdlen = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tif (dlen) {\n\t\t/* Get last data block so it can be truncated */\n\t\tdn = (void *)trun + UBIFS_TRUN_NODE_SZ;\n\t\tblk = new_size >> UBIFS_BLOCK_SHIFT;\n\t\tdata_key_init(c, &key, inum, blk);\n\t\tdbg_jnlk(&key, \"last block key \");\n\t\terr = ubifs_tnc_lookup(c, &key, dn);\n\t\tif (err == -ENOENT)\n\t\t\tdlen = 0; /* Not found (so it is a hole) */\n\t\telse if (err)\n\t\t\tgoto out_free;\n\t\telse {\n\t\t\tif (le32_to_cpu(dn->size) <= dlen)\n\t\t\t\tdlen = 0; /* Nothing to do */\n\t\t\telse {\n\t\t\t\terr = truncate_data_node(c, inode, blk, dn, &dlen);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Must make reservation before allocating sequence numbers */\n\tlen = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ;\n\tif (dlen)\n\t\tlen += dlen;\n\terr = make_reservation(c, BASEHD, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tpack_inode(c, ino, inode, 0);\n\tubifs_prep_grp_node(c, trun, UBIFS_TRUN_NODE_SZ, dlen ? 0 : 1);\n\tif (dlen)\n\t\tubifs_prep_grp_node(c, dn, dlen, 1);\n\n\terr = write_head(c, BASEHD, ino, len, &lnum, &offs, sync);\n\tif (err)\n\t\tgoto out_release;\n\tif (!sync)\n\t\tubifs_wbuf_add_ino_nolock(&c->jheads[BASEHD].wbuf, inum);\n\trelease_head(c, BASEHD);\n\n\tif (dlen) {\n\t\tsz = offs + UBIFS_INO_NODE_SZ + UBIFS_TRUN_NODE_SZ;\n\t\terr = ubifs_tnc_add(c, &key, lnum, sz, dlen);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tino_key_init(c, &key, inum);\n\terr = ubifs_tnc_add(c, &key, lnum, offs, UBIFS_INO_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\terr = ubifs_add_dirt(c, lnum, UBIFS_TRUN_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\tbit = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (new_size >> UBIFS_BLOCK_SHIFT) + (bit ? 1 : 0);\n\tdata_key_init(c, &key, inum, blk);\n\n\tbit = old_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (old_size >> UBIFS_BLOCK_SHIFT) - (bit ? 0 : 1);\n\tdata_key_init(c, &to_key, inum, blk);\n\n\terr = ubifs_tnc_remove_range(c, &key, &to_key);\n\tif (err)\n\t\tgoto out_ro;\n\n\tfinish_reservation(c);\n\tspin_lock(&ui->ui_lock);\n\tui->synced_i_size = ui->ui_size;\n\tspin_unlock(&ui->ui_lock);\n\tmark_inode_clean(c, ui);\n\tkfree(ino);\n\treturn 0;\n\nout_release:\n\trelease_head(c, BASEHD);\nout_ro:\n\tubifs_ro_mode(c, err);\n\tfinish_reservation(c);\nout_free:\n\tkfree(ino);\n\treturn err;\n}\n\n\n/**\n * ubi'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\n_truncate(struct ubifs_info *c, const struct inode *inode,\n\t\t       loff_t old_size, loff_t new_size)\n{\n\tunion ubifs_key key, to_key;\n\tstruct ubifs_ino_node *ino;\n\tstruct ubifs_trun_node *trun;\n\tstruct ubifs_data_node *uninitialized_var(dn);\n\tint err, dlen, len, lnum, offs, bit, sz, sync = IS_SYNC(inode);\n\tstruct ubifs_inode *ui = ubifs_inode(inode);\n\tino_t inum = inode->i_ino;\n\tunsigned int blk;\n\n\tdbg_jnl(\"ino %lu, size %lld -> %lld\",\n\t\t(unsigned long)inum, old_size, new_size);\n\tubifs_assert(!ui->data_len);\n\tubifs_assert(S_ISREG(inode->i_mode));\n\tubifs_assert(mutex_is_locked(&ui->ui_mutex));\n\n\tsz = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ +\n\t     UBIFS_MAX_DATA_NODE_SZ * WORST_COMPR_FACTOR;\n\tino = kmalloc(sz, GFP_NOFS);\n\tif (!ino)\n\t\treturn -ENOMEM;\n\n\ttrun = (void *)ino + UBIFS_INO_NODE_SZ;\n\ttrun->ch.node_type = UBIFS_TRUN_NODE;\n\ttrun->inum = cpu_to_le32(inum);\n\ttrun->old_size = cpu_to_le64(old_size);\n\ttrun->new_size = cpu_to_le64(new_size);\n\tzero_trun_node_unused(trun);\n\n\tdlen = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tif (dlen) {\n\t\t/* Get last data block so it can be truncated */\n\t\tdn = (void *)trun + UBIFS_TRUN_NODE_SZ;\n\t\tblk = new_size >> UBIFS_BLOCK_SHIFT;\n\t\tdata_key_init(c, &key, inum, blk);\n\t\tdbg_jnlk(&key, \"last block key \");\n\t\terr = ubifs_tnc_lookup(c, &key, dn);\n\t\tif (err == -ENOENT)\n\t\t\tdlen = 0; /* Not found (so it is a hole) */\n\t\telse if (err)\n\t\t\tgoto out_free;\n\t\telse {\n\t\t\tint dn_len = le32_to_cpu(dn->size);\n\n\t\t\tif (dn_len <= 0 || dn_len > UBIFS_BLOCK_SIZE) {\n\t\t\t\tubifs_err(c, \"bad data node (block %u, inode %lu)\",\n\t\t\t\t\t  blk, inode->i_ino);\n\t\t\t\tubifs_dump_node(c, dn);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tif (dn_len <= dlen)\n\t\t\t\tdlen = 0; /* Nothing to do */\n\t\t\telse {\n\t\t\t\terr = truncate_data_node(c, inode, blk, dn, &dlen);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Must make reservation before allocating sequence numbers */\n\tlen = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ;\n\tif (dlen)\n\t\tlen += dlen;\n\terr = make_reservation(c, BASEHD, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tpack_inode(c, ino, inode, 0);\n\tubifs_prep_grp_node(c, trun, UBIFS_TRUN_NODE_SZ, dlen ? 0 : 1);\n\tif (dlen)\n\t\tubifs_prep_grp_node(c, dn, dlen, 1);\n\n\terr = write_head(c, BASEHD, ino, len, &lnum, &offs, sync);\n\tif (err)\n\t\tgoto out_release;\n\tif (!sync)\n\t\tubifs_wbuf_add_ino_nolock(&c->jheads[BASEHD].wbuf, inum);\n\trelease_head(c, BASEHD);\n\n\tif (dlen) {\n\t\tsz = offs + UBIFS_INO_NODE_SZ + UBIFS_TRUN_NODE_SZ;\n\t\terr = ubifs_tnc_add(c, &key, lnum, sz, dlen);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tino_key_init(c, &key, inum);\n\terr = ubifs_tnc_add(c, &key, lnum, offs, UBIFS_INO_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\terr = ubifs_add_dirt(c, lnum, UBIFS_TRUN_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\tbit = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (new_size >> UBIFS_BLOCK_SHIFT) + (bit ? 1 : 0);\n\tdata_key_init(c, &key, inum, blk);\n\n\tbit = old_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (old_size >> UBIFS_BLOCK_SHIFT) - (bit ? 0 : 1);\n\tdata_key_init(c, &to_key, inum, blk);\n\n\terr = ubifs_tnc_remove_range(c, &key, &to_key);\n\tif (err)\n\t\tgoto out_ro;\n\n\tfinish_reservation(c);\n\tspin_lock(&ui->ui_lock);\n\tui->synced_i_size = ui->ui_size;\n\tspin_unlock(&ui->ui_lock);\n\tmark_inode_clean(c, ui);\n\tkfree(ino);\n\treturn 0;\n\nout_release:\n\trelease_head(c, BASEHD);\nout_ro:\n\tubifs_ro_mode(c, err);\n\tfinish_reservation(c);\nout_free:\n\tkfree(ino);\n\treturn err;\n}\n\n\n/**\n * ubi'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\n_truncate(struct ubifs_info *c, const struct inode *inode,\n\t\t       loff_t old_size, loff_t new_size)\n{\n\tunion ubifs_key key, to_key;\n\tstruct ubifs_ino_node *ino;\n\tstruct ubifs_trun_node *trun;\n\tstruct ubifs_data_node *uninitialized_var(dn);\n\tint err, dlen, len, lnum, offs, bit, sz, sync = IS_SYNC(inode);\n\tstruct ubifs_inode *ui = ubifs_inode(inode);\n\tino_t inum = inode->i_ino;\n\tunsigned int blk;\n\n\tdbg_jnl(\"ino %lu, size %lld -> %lld\",\n\t\t(unsigned long)inum, old_size, new_size);\n\tubifs_assert(!ui->data_len);\n\tubifs_assert(S_ISREG(inode->i_mode));\n\tubifs_assert(mutex_is_locked(&ui->ui_mutex));\n\n\tsz = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ +\n\t     UBIFS_MAX_DATA_NODE_SZ * WORST_COMPR_FACTOR;\n\tino = kmalloc(sz, GFP_NOFS);\n\tif (!ino)\n\t\treturn -ENOMEM;\n\n\ttrun = (void *)ino + UBIFS_INO_NODE_SZ;\n\ttrun->ch.node_type = UBIFS_TRUN_NODE;\n\ttrun->inum = cpu_to_le32(inum);\n\ttrun->old_size = cpu_to_le64(old_size);\n\ttrun->new_size = cpu_to_le64(new_size);\n\tzero_trun_node_unused(trun);\n\n\tdlen = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tif (dlen) {\n\t\t/* Get last data block so it can be truncated */\n\t\tdn = (void *)trun + UBIFS_TRUN_NODE_SZ;\n\t\tblk = new_size >> UBIFS_BLOCK_SHIFT;\n\t\tdata_key_init(c, &key, inum, blk);\n\t\tdbg_jnlk(&key, \"last block key \");\n\t\terr = ubifs_tnc_lookup(c, &key, dn);\n\t\tif (err == -ENOENT)\n\t\t\tdlen = 0; /* Not found (so it is a hole) */\n\t\telse if (err)\n\t\t\tgoto out_free;\n\t\telse {\n\t\t\tif (le32_to_cpu(dn->size) <= dlen)\n\t\t\t\tdlen = 0; /* Nothing to do */\n\t\t\telse {\n\t\t\t\tint compr_type = le16_to_cpu(dn->compr_type);\n\n\t\t\t\tif (compr_type != UBIFS_COMPR_NONE) {\n\t\t\t\t\terr = recomp_data_node(dn, &dlen);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tgoto out_free;\n\t\t\t\t} else {\n\t\t\t\t\tdn->size = cpu_to_le32(dlen);\n\t\t\t\t\tdlen += UBIFS_DATA_NODE_SZ;\n\t\t\t\t}\n\t\t\t\tzero_data_node_unused(dn);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Must make reservation before allocating sequence numbers */\n\tlen = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ;\n\tif (dlen)\n\t\tlen += dlen;\n\terr = make_reservation(c, BASEHD, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tpack_inode(c, ino, inode, 0);\n\tubifs_prep_grp_node(c, trun, UBIFS_TRUN_NODE_SZ, dlen ? 0 : 1);\n\tif (dlen)\n\t\tubifs_prep_grp_node(c, dn, dlen, 1);\n\n\terr = write_head(c, BASEHD, ino, len, &lnum, &offs, sync);\n\tif (err)\n\t\tgoto out_release;\n\tif (!sync)\n\t\tubifs_wbuf_add_ino_nolock(&c->jheads[BASEHD].wbuf, inum);\n\trelease_head(c, BASEHD);\n\n\tif (dlen) {\n\t\tsz = offs + UBIFS_INO_NODE_SZ + UBIFS_TRUN_NODE_SZ;\n\t\terr = ubifs_tnc_add(c, &key, lnum, sz, dlen);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tino_key_init(c, &key, inum);\n\terr = ubifs_tnc_add(c, &key, lnum, offs, UBIFS_INO_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\terr = ubifs_add_dirt(c, lnum, UBIFS_TRUN_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\tbit = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (new_size >> UBIFS_BLOCK_SHIFT) + (bit ? 1 : 0);\n\tdata_key_init(c, &key, inum, blk);\n\n\tbit = old_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (old_size >> UBIFS_BLOCK_SHIFT) - (bit ? 0 : 1);\n\tdata_key_init(c, &to_key, inum, blk);\n\n\terr = ubifs_tnc_remove_range(c, &key, &to_key);\n\tif (err)\n\t\tgoto out_ro;\n\n\tfinish_reservation(c);\n\tspin_lock(&ui->ui_lock);\n\tui->synced_i_size = ui->ui_size;\n\tspin_unlock(&ui->ui_lock);\n\tmark_inode_clean(c, ui);\n\tkfree(ino);\n\treturn 0;\n\nout_release:\n\trelease_head(c, BASEHD);\nout_ro:\n\tubifs_ro_mode(c, err);\n\tfinish_reservation(c);\nout_free:\n\tkfree(ino);\n\treturn err;\n}\n\n\n/**\n * ubi'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\n_truncate(struct ubifs_info *c, const struct inode *inode,\n\t\t       loff_t old_size, loff_t new_size)\n{\n\tunion ubifs_key key, to_key;\n\tstruct ubifs_ino_node *ino;\n\tstruct ubifs_trun_node *trun;\n\tstruct ubifs_data_node *uninitialized_var(dn);\n\tint err, dlen, len, lnum, offs, bit, sz, sync = IS_SYNC(inode);\n\tstruct ubifs_inode *ui = ubifs_inode(inode);\n\tino_t inum = inode->i_ino;\n\tunsigned int blk;\n\n\tdbg_jnl(\"ino %lu, size %lld -> %lld\",\n\t\t(unsigned long)inum, old_size, new_size);\n\tubifs_assert(!ui->data_len);\n\tubifs_assert(S_ISREG(inode->i_mode));\n\tubifs_assert(mutex_is_locked(&ui->ui_mutex));\n\n\tsz = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ +\n\t     UBIFS_MAX_DATA_NODE_SZ * WORST_COMPR_FACTOR;\n\tino = kmalloc(sz, GFP_NOFS);\n\tif (!ino)\n\t\treturn -ENOMEM;\n\n\ttrun = (void *)ino + UBIFS_INO_NODE_SZ;\n\ttrun->ch.node_type = UBIFS_TRUN_NODE;\n\ttrun->inum = cpu_to_le32(inum);\n\ttrun->old_size = cpu_to_le64(old_size);\n\ttrun->new_size = cpu_to_le64(new_size);\n\tzero_trun_node_unused(trun);\n\n\tdlen = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tif (dlen) {\n\t\t/* Get last data block so it can be truncated */\n\t\tdn = (void *)trun + UBIFS_TRUN_NODE_SZ;\n\t\tblk = new_size >> UBIFS_BLOCK_SHIFT;\n\t\tdata_key_init(c, &key, inum, blk);\n\t\tdbg_jnlk(&key, \"last block key \");\n\t\terr = ubifs_tnc_lookup(c, &key, dn);\n\t\tif (err == -ENOENT)\n\t\t\tdlen = 0; /* Not found (so it is a hole) */\n\t\telse if (err)\n\t\t\tgoto out_free;\n\t\telse {\n\t\t\tint dn_len = le32_to_cpu(dn->size);\n\n\t\t\tif (dn_len <= 0 || dn_len > UBIFS_BLOCK_SIZE) {\n\t\t\t\tubifs_err(\"bad data node (block %u, inode %lu)\",\n\t\t\t\t\t  blk, inode->i_ino);\n\t\t\t\tubifs_dump_node(c, dn);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tif (dn_len <= dlen)\n\t\t\t\tdlen = 0; /* Nothing to do */\n\t\t\telse {\n\t\t\t\tint compr_type = le16_to_cpu(dn->compr_type);\n\n\t\t\t\tif (compr_type != UBIFS_COMPR_NONE) {\n\t\t\t\t\terr = recomp_data_node(dn, &dlen);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tgoto out_free;\n\t\t\t\t} else {\n\t\t\t\t\tdn->size = cpu_to_le32(dlen);\n\t\t\t\t\tdlen += UBIFS_DATA_NODE_SZ;\n\t\t\t\t}\n\t\t\t\tzero_data_node_unused(dn);\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Must make reservation before allocating sequence numbers */\n\tlen = UBIFS_TRUN_NODE_SZ + UBIFS_INO_NODE_SZ;\n\tif (dlen)\n\t\tlen += dlen;\n\terr = make_reservation(c, BASEHD, len);\n\tif (err)\n\t\tgoto out_free;\n\n\tpack_inode(c, ino, inode, 0);\n\tubifs_prep_grp_node(c, trun, UBIFS_TRUN_NODE_SZ, dlen ? 0 : 1);\n\tif (dlen)\n\t\tubifs_prep_grp_node(c, dn, dlen, 1);\n\n\terr = write_head(c, BASEHD, ino, len, &lnum, &offs, sync);\n\tif (err)\n\t\tgoto out_release;\n\tif (!sync)\n\t\tubifs_wbuf_add_ino_nolock(&c->jheads[BASEHD].wbuf, inum);\n\trelease_head(c, BASEHD);\n\n\tif (dlen) {\n\t\tsz = offs + UBIFS_INO_NODE_SZ + UBIFS_TRUN_NODE_SZ;\n\t\terr = ubifs_tnc_add(c, &key, lnum, sz, dlen);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tino_key_init(c, &key, inum);\n\terr = ubifs_tnc_add(c, &key, lnum, offs, UBIFS_INO_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\terr = ubifs_add_dirt(c, lnum, UBIFS_TRUN_NODE_SZ);\n\tif (err)\n\t\tgoto out_ro;\n\n\tbit = new_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (new_size >> UBIFS_BLOCK_SHIFT) + (bit ? 1 : 0);\n\tdata_key_init(c, &key, inum, blk);\n\n\tbit = old_size & (UBIFS_BLOCK_SIZE - 1);\n\tblk = (old_size >> UBIFS_BLOCK_SHIFT) - (bit ? 0 : 1);\n\tdata_key_init(c, &to_key, inum, blk);\n\n\terr = ubifs_tnc_remove_range(c, &key, &to_key);\n\tif (err)\n\t\tgoto out_ro;\n\n\tfinish_reservation(c);\n\tspin_lock(&ui->ui_lock);\n\tui->synced_i_size = ui->ui_size;\n\tspin_unlock(&ui->ui_lock);\n\tmark_inode_clean(c, ui);\n\tkfree(ino);\n\treturn 0;\n\nout_release:\n\trelease_head(c, BASEHD);\nout_ro:\n\tubifs_ro_mode(c, err);\n\tfinish_reservation(c);\nout_free:\n\tkfree(ino);\n\treturn err;\n}\n\n\n/**\n * ubi'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int bsg_prepare_job(struct device *dev, struct request *req)\n{\n\tstruct request *rsp = req->next_rq;\n\tstruct scsi_request *rq = scsi_req(req);\n\tstruct bsg_job *job = blk_mq_rq_to_pdu(req);\n\tint ret;\n\n\tjob->request = rq->cmd;\n\tjob->request_len = rq->cmd_len;\n\n\tif (req->bio) {\n\t\tret = bsg_map_buffer(&job->request_payload, req);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_job;\n\t}\n\tif (rsp && rsp->bio) {\n\t\tret = bsg_map_buffer(&job->reply_payload, rsp);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_rqst_payload;\n\t}\n\tjob->dev = dev;\n\t/* take a reference for the request */\n\tget_device(job->dev);\n\tkref_init(&job->kref);\n\treturn 0;\n\nfailjob_rls_rqst_payload:\n\tkfree(job->request_payload.sg_list);\nfailjob_rls_job:\n\tkfree(job);\n\treturn -ENOMEM;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int bsg_prepare_job(struct device *dev, struct request *req)\n{\n\tstruct request *rsp = req->next_rq;\n\tstruct scsi_request *rq = scsi_req(req);\n\tstruct bsg_job *job = blk_mq_rq_to_pdu(req);\n\tint ret;\n\n\tjob->request = rq->cmd;\n\tjob->request_len = rq->cmd_len;\n\n\tif (req->bio) {\n\t\tret = bsg_map_buffer(&job->request_payload, req);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_job;\n\t}\n\tif (rsp && rsp->bio) {\n\t\tret = bsg_map_buffer(&job->reply_payload, rsp);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_rqst_payload;\n\t}\n\tjob->dev = dev;\n\t/* take a reference for the request */\n\tget_device(job->dev);\n\tkref_init(&job->kref);\n\treturn 0;\n\nfailjob_rls_rqst_payload:\n\tkfree(job->request_payload.sg_list);\nfailjob_rls_job:\n\treturn -ENOMEM;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int bsg_create_job(struct device *dev, struct request *req)\n{\n\tstruct request *rsp = req->next_rq;\n\tstruct request_queue *q = req->q;\n\tstruct bsg_job *job;\n\tint ret;\n\n\tBUG_ON(req->special);\n\n\tjob = kzalloc(sizeof(struct bsg_job) + q->bsg_job_size, GFP_KERNEL);\n\tif (!job)\n\t\treturn -ENOMEM;\n\n\treq->special = job;\n\tjob->req = req;\n\tif (q->bsg_job_size)\n\t\tjob->dd_data = (void *)&job[1];\n\tjob->request = req->cmd;\n\tjob->request_len = req->cmd_len;\n\tjob->reply = req->sense;\n\tjob->reply_len = SCSI_SENSE_BUFFERSIZE;\t/* Size of sense buffer\n\t\t\t\t\t\t * allocated */\n\tif (req->bio) {\n\t\tret = bsg_map_buffer(&job->request_payload, req);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_job;\n\t}\n\tif (rsp && rsp->bio) {\n\t\tret = bsg_map_buffer(&job->reply_payload, rsp);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_rqst_payload;\n\t}\n\tjob->dev = dev;\n\t/* take a reference for the request */\n\tget_device(job->dev);\n\treturn 0;\n\nfailjob_rls_rqst_payload:\n\tkfree(job->request_payload.sg_list);\nfailjob_rls_job:\n\tkfree(job);\n\treturn -ENOMEM;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int bsg_create_job(struct device *dev, struct request *req)\n{\n\tstruct request *rsp = req->next_rq;\n\tstruct request_queue *q = req->q;\n\tstruct bsg_job *job;\n\tint ret;\n\n\tBUG_ON(req->special);\n\n\tjob = kzalloc(sizeof(struct bsg_job) + q->bsg_job_size, GFP_KERNEL);\n\tif (!job)\n\t\treturn -ENOMEM;\n\n\treq->special = job;\n\tjob->req = req;\n\tif (q->bsg_job_size)\n\t\tjob->dd_data = (void *)&job[1];\n\tjob->request = req->cmd;\n\tjob->request_len = req->cmd_len;\n\tjob->reply = req->sense;\n\tjob->reply_len = SCSI_SENSE_BUFFERSIZE;\t/* Size of sense buffer\n\t\t\t\t\t\t * allocated */\n\tif (req->bio) {\n\t\tret = bsg_map_buffer(&job->request_payload, req);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_job;\n\t}\n\tif (rsp && rsp->bio) {\n\t\tret = bsg_map_buffer(&job->reply_payload, rsp);\n\t\tif (ret)\n\t\t\tgoto failjob_rls_rqst_payload;\n\t}\n\tjob->dev = dev;\n\t/* take a reference for the request */\n\tget_device(job->dev);\n\treturn 0;\n\nfailjob_rls_rqst_payload:\n\tkfree(job->request_payload.sg_list);\nfailjob_rls_job:\n\treturn -ENOMEM;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int tango_nand_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct clk *clk;\n\tstruct resource *res;\n\tstruct tango_nfc *nfc;\n\tstruct device_node *np;\n\n\tnfc = devm_kzalloc(&pdev->dev, sizeof(*nfc), GFP_KERNEL);\n\tif (!nfc)\n\t\treturn -ENOMEM;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tnfc->reg_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->reg_base))\n\t\treturn PTR_ERR(nfc->reg_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tnfc->mem_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->mem_base))\n\t\treturn PTR_ERR(nfc->mem_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\n\tnfc->pbus_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->pbus_base))\n\t\treturn PTR_ERR(nfc->pbus_base);\n\n\twritel_relaxed(MODE_RAW, nfc->pbus_base + PBUS_PAD_MODE);\n\n\tclk = clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(clk))\n\t\treturn PTR_ERR(clk);\n\n\tnfc->chan = dma_request_chan(&pdev->dev, \"rxtx\");\n\tif (IS_ERR(nfc->chan))\n\t\treturn PTR_ERR(nfc->chan);\n\n\tplatform_set_drvdata(pdev, nfc);\n\tnand_hw_control_init(&nfc->hw);\n\tnfc->freq_kHz = clk_get_rate(clk) / 1000;\n\n\tfor_each_child_of_node(pdev->dev.of_node, np) {\n\t\terr = chip_init(&pdev->dev, np);\n\t\tif (err) {\n\t\t\ttango_nand_remove(pdev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int tango_nand_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct clk *clk;\n\tstruct resource *res;\n\tstruct tango_nfc *nfc;\n\tstruct device_node *np;\n\n\tnfc = devm_kzalloc(&pdev->dev, sizeof(*nfc), GFP_KERNEL);\n\tif (!nfc)\n\t\treturn -ENOMEM;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tnfc->reg_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->reg_base))\n\t\treturn PTR_ERR(nfc->reg_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tnfc->mem_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->mem_base))\n\t\treturn PTR_ERR(nfc->mem_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\n\tnfc->pbus_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->pbus_base))\n\t\treturn PTR_ERR(nfc->pbus_base);\n\n\twritel_relaxed(MODE_RAW, nfc->pbus_base + PBUS_PAD_MODE);\n\n\tclk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(clk))\n\t\treturn PTR_ERR(clk);\n\n\tnfc->chan = dma_request_chan(&pdev->dev, \"rxtx\");\n\tif (IS_ERR(nfc->chan))\n\t\treturn PTR_ERR(nfc->chan);\n\n\tplatform_set_drvdata(pdev, nfc);\n\tnand_hw_control_init(&nfc->hw);\n\tnfc->freq_kHz = clk_get_rate(clk) / 1000;\n\n\tfor_each_child_of_node(pdev->dev.of_node, np) {\n\t\terr = chip_init(&pdev->dev, np);\n\t\tif (err) {\n\t\t\ttango_nand_remove(pdev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int tango_nand_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct clk *clk;\n\tstruct resource *res;\n\tstruct tango_nfc *nfc;\n\tstruct device_node *np;\n\n\tnfc = devm_kzalloc(&pdev->dev, sizeof(*nfc), GFP_KERNEL);\n\tif (!nfc)\n\t\treturn -ENOMEM;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tnfc->reg_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->reg_base))\n\t\treturn PTR_ERR(nfc->reg_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tnfc->mem_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->mem_base))\n\t\treturn PTR_ERR(nfc->mem_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\n\tnfc->pbus_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->pbus_base))\n\t\treturn PTR_ERR(nfc->pbus_base);\n\n\twritel_relaxed(MODE_RAW, nfc->pbus_base + PBUS_PAD_MODE);\n\n\tclk = clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(clk))\n\t\treturn PTR_ERR(clk);\n\n\tnfc->chan = dma_request_chan(&pdev->dev, \"rxtx\");\n\tif (IS_ERR(nfc->chan))\n\t\treturn PTR_ERR(nfc->chan);\n\n\tplatform_set_drvdata(pdev, nfc);\n\tnand_hw_control_init(&nfc->hw);\n\tnfc->freq_kHz = clk_get_rate(clk) / 1000;\n\n\tfor_each_child_of_node(pdev->dev.of_node, np) {\n\t\terr = chip_init(&pdev->dev, np);\n\t\tif (err) {\n\t\t\ttango_nand_remove(pdev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int tango_nand_probe(struct platform_device *pdev)\n{\n\tint err;\n\tstruct clk *clk;\n\tstruct resource *res;\n\tstruct tango_nfc *nfc;\n\tstruct device_node *np;\n\n\tnfc = devm_kzalloc(&pdev->dev, sizeof(*nfc), GFP_KERNEL);\n\tif (!nfc)\n\t\treturn -ENOMEM;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tnfc->reg_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->reg_base))\n\t\treturn PTR_ERR(nfc->reg_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tnfc->mem_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->mem_base))\n\t\treturn PTR_ERR(nfc->mem_base);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\n\tnfc->pbus_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(nfc->pbus_base))\n\t\treturn PTR_ERR(nfc->pbus_base);\n\n\twritel_relaxed(MODE_RAW, nfc->pbus_base + PBUS_PAD_MODE);\n\n\tclk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(clk))\n\t\treturn PTR_ERR(clk);\n\n\tnfc->chan = dma_request_chan(&pdev->dev, \"rxtx\");\n\tif (IS_ERR(nfc->chan))\n\t\treturn PTR_ERR(nfc->chan);\n\n\tplatform_set_drvdata(pdev, nfc);\n\tnand_hw_control_init(&nfc->hw);\n\tnfc->freq_kHz = clk_get_rate(clk) / 1000;\n\n\tfor_each_child_of_node(pdev->dev.of_node, np) {\n\t\terr = chip_init(&pdev->dev, np);\n\t\tif (err) {\n\t\t\ttango_nand_remove(pdev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void dummy_timer(unsigned long _dum_hcd)\n{\n\tstruct dummy_hcd\t*dum_hcd = (struct dummy_hcd *) _dum_hcd;\n\tstruct dummy\t\t*dum = dum_hcd->dum;\n\tstruct urbp\t\t*urbp, *tmp;\n\tunsigned long\t\tflags;\n\tint\t\t\tlimit, total;\n\tint\t\t\ti;\n\n\t/* simplistic model for one frame's bandwidth */\n\tswitch (dum->gadget.speed) {\n\tcase USB_SPEED_LOW:\n\t\ttotal = 8/*bytes*/ * 12/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_FULL:\n\t\ttotal = 64/*bytes*/ * 19/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_HIGH:\n\t\ttotal = 512/*bytes*/ * 13/*packets*/ * 8/*uframes*/;\n\t\tbreak;\n\tcase USB_SPEED_SUPER:\n\t\t/* Bus speed is 500000 bytes/ms, so use a little less */\n\t\ttotal = 490000;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dummy_dev(dum_hcd), \"bogus device speed\\n\");\n\t\treturn;\n\t}\n\n\t/* FIXME if HZ != 1000 this will probably misbehave ... */\n\n\t/* look at each urb queued by the host side driver */\n\tspin_lock_irqsave(&dum->lock, flags);\n\n\tif (!dum_hcd->udev) {\n\t\tdev_err(dummy_dev(dum_hcd),\n\t\t\t\t\"timer fired with no URBs pending?\\n\");\n\t\tspin_unlock_irqrestore(&dum->lock, flags);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < DUMMY_ENDPOINTS; i++) {\n\t\tif (!ep_info[i].name)\n\t\t\tbreak;\n\t\tdum->ep[i].already_seen = 0;\n\t}\n\nrestart:\n\tlist_for_each_entry_safe(urbp, tmp, &dum_hcd->urbp_list, urbp_list) {\n\t\tstruct urb\t\t*urb;\n\t\tstruct dummy_request\t*req;\n\t\tu8\t\t\taddress;\n\t\tstruct dummy_ep\t\t*ep = NULL;\n\t\tint\t\t\ttype;\n\t\tint\t\t\tstatus = -EINPROGRESS;\n\n\t\turb = urbp->urb;\n\t\tif (urb->unlinked)\n\t\t\tgoto return_urb;\n\t\telse if (dum_hcd->rh_state != DUMMY_RH_RUNNING)\n\t\t\tcontinue;\n\t\ttype = usb_pipetype(urb->pipe);\n\n\t\t/* used up this frame's non-periodic bandwidth?\n\t\t * FIXME there's infinite bandwidth for control and\n\t\t * periodic transfers ... unrealistic.\n\t\t */\n\t\tif (total <= 0 && type == PIPE_BULK)\n\t\t\tcontinue;\n\n\t\t/* find the gadget's ep for this request (if configured) */\n\t\taddress = usb_pipeendpoint (urb->pipe);\n\t\tif (usb_pipein(urb->pipe))\n\t\t\taddress |= USB_DIR_IN;\n\t\tep = find_endpoint(dum, address);\n\t\tif (!ep) {\n\t\t\t/* set_configuration() disagreement */\n\t\t\tdev_dbg(dummy_dev(dum_hcd),\n\t\t\t\t\"no ep configured for urb %p\\n\",\n\t\t\t\turb);\n\t\t\tstatus = -EPROTO;\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\tif (ep->already_seen)\n\t\t\tcontinue;\n\t\tep->already_seen = 1;\n\t\tif (ep == &dum->ep[0] && urb->error_count) {\n\t\t\tep->setup_stage = 1;\t/* a new urb */\n\t\t\turb->error_count = 0;\n\t\t}\n\t\tif (ep->halted && !ep->setup_stage) {\n\t\t\t/* NOTE: must not be iso! */\n\t\t\tdev_dbg(dummy_dev(dum_hcd), \"ep %s halted, urb %p\\n\",\n\t\t\t\t\tep->ep.name, urb);\n\t\t\tstatus = -EPIPE;\n\t\t\tgoto return_urb;\n\t\t}\n\t\t/* FIXME make sure both ends agree on maxpacket */\n\n\t\t/* handle control requests */\n\t\tif (ep == &dum->ep[0] && ep->setup_stage) {\n\t\t\tstruct usb_ctrlrequest\t\tsetup;\n\t\t\tint\t\t\t\tvalue = 1;\n\n\t\t\tsetup = *(struct usb_ctrlrequest *) urb->setup_packet;\n\t\t\t/* paranoia, in case of stale queued data */\n\t\t\tlist_for_each_entry(req, &ep->queue, queue) {\n\t\t\t\tlist_del_init(&req->queue);\n\t\t\t\treq->req.status = -EOVERFLOW;\n\t\t\t\tdev_dbg(udc_dev(dum), \"stale req = %p\\n\",\n\t\t\t\t\t\treq);\n\n\t\t\t\tspin_unlock(&dum->lock);\n\t\t\t\tusb_gadget_giveback_request(&ep->ep, &req->req);\n\t\t\t\tspin_lock(&dum->lock);\n\t\t\t\tep->already_seen = 0;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\t/* gadget driver never sees set_address or operations\n\t\t\t * on standard feature flags.  some hardware doesn't\n\t\t\t * even expose them.\n\t\t\t */\n\t\t\tep->last_io = jiffies;\n\t\t\tep->setup_stage = 0;\n\t\t\tep->halted = 0;\n\n\t\t\tvalue = handle_control_request(dum_hcd, urb, &setup,\n\t\t\t\t\t\t       &status);\n\n\t\t\t/* gadget driver handles all other requests.  block\n\t\t\t * until setup() returns; no reentrancy issues etc.\n\t\t\t */\n\t\t\tif (value > 0) {\n\t\t\t\tspin_unlock(&dum->lock);\n\t\t\t\tvalue = dum->driver->setup(&dum->gadget,\n\t\t\t\t\t\t&setup);\n\t\t\t\tspin_lock(&dum->lock);\n\n\t\t\t\tif (value >= 0) {\n\t\t\t\t\t/* no delays (max 64KB data stage) */\n\t\t\t\t\tlimit = 64*1024;\n\t\t\t\t\tgoto treat_control_like_bulk;\n\t\t\t\t}\n\t\t\t\t/* error, see below */\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\tif (value != -EOPNOTSUPP)\n\t\t\t\t\tdev_dbg(udc_dev(dum),\n\t\t\t\t\t\t\"setup --> %d\\n\",\n\t\t\t\t\t\tvalue);\n\t\t\t\tstatus = -EPIPE;\n\t\t\t\turb->actual_length = 0;\n\t\t\t}\n\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\t/* non-control requests */\n\t\tlimit = total;\n\t\tswitch (usb_pipetype(urb->pipe)) {\n\t\tcase PIPE_ISOCHRONOUS:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * use urb->iso_frame_desc[i].\n\t\t\t * complete whether or not ep has requests queued.\n\t\t\t * report random errors, to debug drivers.\n\t\t\t */\n\t\t\tlimit = max(limit, periodic_bytes(dum, ep));\n\t\t\tstatus = -ENOSYS;\n\t\t\tbreak;\n\n\t\tcase PIPE_INTERRUPT:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * this almost certainly polls too fast.\n\t\t\t */\n\t\t\tlimit = max(limit, periodic_bytes(dum, ep));\n\t\t\t/* FALLTHROUGH */\n\n\t\tdefault:\ntreat_control_like_bulk:\n\t\t\tep->last_io = jiffies;\n\t\t\ttotal -= transfer(dum_hcd, urb, ep, limit, &status);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* incomplete transfer? */\n\t\tif (status == -EINPROGRESS)\n\t\t\tcontinue;\n\nreturn_urb:\n\t\tlist_del(&urbp->urbp_list);\n\t\tkfree(urbp);\n\t\tif (ep)\n\t\t\tep->already_seen = ep->setup_stage = 0;\n\n\t\tusb_hcd_unlink_urb_from_ep(dummy_hcd_to_hcd(dum_hcd), urb);\n\t\tspin_unlock(&dum->lock);\n\t\tusb_hcd_giveback_urb(dummy_hcd_to_hcd(dum_hcd), urb, status);\n\t\tspin_lock(&dum->lock);\n\n\t\tgoto restart;\n\t}\n\n\tif (list_empty(&dum_hcd->urbp_list)) {\n\t\tusb_put_dev(dum_hcd->udev);\n\t\tdum_hcd->udev = NULL;\n\t} else if (dum_hcd->rh_state == DUMMY_RH_RUNNING) {\n\t\t/* want a 1 msec delay here */\n\t\tmod_timer(&dum_hcd->timer, jiffies + msecs_to_jiffies(1));\n\t}\n\n\tspin_unlock_irqrestore(&dum->lock, flags);\n}\n\nstatic int dummy_urb_enqueue(\n\tstruct usb_hcd\t\t\t*hcd,\n\tstruct urb\t\t\t*urb,\n\tgfp_t\t\t\t\tmem_flags\n) {\n\tstruct dummy_hcd *dum_hcd;\n\tstruct urbp\t*urbp;\n\tunsigned long\tflags;\n\tint\t\trc;\n\n\turbp = kmalloc(sizeof *urbp, mem_flags);\n\tif (!urbp)\n\t\treturn -ENOMEM;\n\turbp->urb = urb;\n\turbp->miter_started = 0;\n\n\tdum_hcd = hcd_to_dummy_hcd(hcd);\n\tspin_lock_irqsave(&dum_hcd->dum->lock, flags);\n\n\trc = dummy_validate_stream(dum_hcd, urb);\n\tif (rc) {\n\t\tkfree(urbp);\n\t\tgoto done;\n\t}\n\n\trc = usb_hcd_link_urb_to_ep(hcd, urb);\n\tif (rc) {\n\t\tkfree(urbp);\n\t\tgoto done;\n\t}\n\n\tif (!dum_hcd->udev) {\n\t\tdum_hcd->udev = urb->dev;\n\t\tusb_get_dev(dum_hcd->udev);\n\t} else if (unlikely(dum_hcd->udev != urb->dev))\n\t\tdev_err(dummy_dev(dum_hcd), \"usb_device address has changed!\\n\");\n\n\tlist_add_tail(&urbp->urbp_list, &dum_hcd->urbp_list);\n\turb->hcpriv = urbp;\n\tif (usb_pipetype(urb->pipe) == PIPE_CONTROL)\n\t\turb->error_count = 1;\t\t/* mark as a new urb */\n\n\t/* kick the scheduler, it'll do the rest */\n\tif (!timer_pending(&dum_hcd->timer))\n\t\tmod_timer(&dum_hcd->timer, jiffies + 1);\n\n done:\n\tspin_unlock_irqrestore(&dum_hcd->dum->lock, flags);\n\treturn rc;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstruct urbp\t\t\t*next_frame_urbp;\n\n\n\n\nstatic void dummy_timer(unsigned long _dum_hcd)\n{\n\tstruct dummy_hcd\t*dum_hcd = (struct dummy_hcd *) _dum_hcd;\n\tstruct dummy\t\t*dum = dum_hcd->dum;\n\tstruct urbp\t\t*urbp, *tmp;\n\tunsigned long\t\tflags;\n\tint\t\t\tlimit, total;\n\tint\t\t\ti;\n\n\t/* simplistic model for one frame's bandwidth */\n\tswitch (dum->gadget.speed) {\n\tcase USB_SPEED_LOW:\n\t\ttotal = 8/*bytes*/ * 12/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_FULL:\n\t\ttotal = 64/*bytes*/ * 19/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_HIGH:\n\t\ttotal = 512/*bytes*/ * 13/*packets*/ * 8/*uframes*/;\n\t\tbreak;\n\tcase USB_SPEED_SUPER:\n\t\t/* Bus speed is 500000 bytes/ms, so use a little less */\n\t\ttotal = 490000;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dummy_dev(dum_hcd), \"bogus device speed\\n\");\n\t\treturn;\n\t}\n\n\t/* FIXME if HZ != 1000 this will probably misbehave ... */\n\n\t/* look at each urb queued by the host side driver */\n\tspin_lock_irqsave(&dum->lock, flags);\n\n\tif (!dum_hcd->udev) {\n\t\tdev_err(dummy_dev(dum_hcd),\n\t\t\t\t\"timer fired with no URBs pending?\\n\");\n\t\tspin_unlock_irqrestore(&dum->lock, flags);\n\t\treturn;\n\t}\n\tdum_hcd->next_frame_urbp = NULL;\n\n\tfor (i = 0; i < DUMMY_ENDPOINTS; i++) {\n\t\tif (!ep_info[i].name)\n\t\t\tbreak;\n\t\tdum->ep[i].already_seen = 0;\n\t}\n\nrestart:\n\tlist_for_each_entry_safe(urbp, tmp, &dum_hcd->urbp_list, urbp_list) {\n\t\tstruct urb\t\t*urb;\n\t\tstruct dummy_request\t*req;\n\t\tu8\t\t\taddress;\n\t\tstruct dummy_ep\t\t*ep = NULL;\n\t\tint\t\t\ttype;\n\t\tint\t\t\tstatus = -EINPROGRESS;\n\n\t\t/* stop when we reach URBs queued after the timer interrupt */\n\t\tif (urbp == dum_hcd->next_frame_urbp)\n\t\t\tbreak;\n\n\t\turb = urbp->urb;\n\t\tif (urb->unlinked)\n\t\t\tgoto return_urb;\n\t\telse if (dum_hcd->rh_state != DUMMY_RH_RUNNING)\n\t\t\tcontinue;\n\t\ttype = usb_pipetype(urb->pipe);\n\n\t\t/* used up this frame's non-periodic bandwidth?\n\t\t * FIXME there's infinite bandwidth for control and\n\t\t * periodic transfers ... unrealistic.\n\t\t */\n\t\tif (total <= 0 && type == PIPE_BULK)\n\t\t\tcontinue;\n\n\t\t/* find the gadget's ep for this request (if configured) */\n\t\taddress = usb_pipeendpoint (urb->pipe);\n\t\tif (usb_pipein(urb->pipe))\n\t\t\taddress |= USB_DIR_IN;\n\t\tep = find_endpoint(dum, address);\n\t\tif (!ep) {\n\t\t\t/* set_configuration() disagreement */\n\t\t\tdev_dbg(dummy_dev(dum_hcd),\n\t\t\t\t\"no ep configured for urb %p\\n\",\n\t\t\t\turb);\n\t\t\tstatus = -EPROTO;\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\tif (ep->already_seen)\n\t\t\tcontinue;\n\t\tep->already_seen = 1;\n\t\tif (ep == &dum->ep[0] && urb->error_count) {\n\t\t\tep->setup_stage = 1;\t/* a new urb */\n\t\t\turb->error_count = 0;\n\t\t}\n\t\tif (ep->halted && !ep->setup_stage) {\n\t\t\t/* NOTE: must not be iso! */\n\t\t\tdev_dbg(dummy_dev(dum_hcd), \"ep %s halted, urb %p\\n\",\n\t\t\t\t\tep->ep.name, urb);\n\t\t\tstatus = -EPIPE;\n\t\t\tgoto return_urb;\n\t\t}\n\t\t/* FIXME make sure both ends agree on maxpacket */\n\n\t\t/* handle control requests */\n\t\tif (ep == &dum->ep[0] && ep->setup_stage) {\n\t\t\tstruct usb_ctrlrequest\t\tsetup;\n\t\t\tint\t\t\t\tvalue = 1;\n\n\t\t\tsetup = *(struct usb_ctrlrequest *) urb->setup_packet;\n\t\t\t/* paranoia, in case of stale queued data */\n\t\t\tlist_for_each_entry(req, &ep->queue, queue) {\n\t\t\t\tlist_del_init(&req->queue);\n\t\t\t\treq->req.status = -EOVERFLOW;\n\t\t\t\tdev_dbg(udc_dev(dum), \"stale req = %p\\n\",\n\t\t\t\t\t\treq);\n\n\t\t\t\tspin_unlock(&dum->lock);\n\t\t\t\tusb_gadget_giveback_request(&ep->ep, &req->req);\n\t\t\t\tspin_lock(&dum->lock);\n\t\t\t\tep->already_seen = 0;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\t/* gadget driver never sees set_address or operations\n\t\t\t * on standard feature flags.  some hardware doesn't\n\t\t\t * even expose them.\n\t\t\t */\n\t\t\tep->last_io = jiffies;\n\t\t\tep->setup_stage = 0;\n\t\t\tep->halted = 0;\n\n\t\t\tvalue = handle_control_request(dum_hcd, urb, &setup,\n\t\t\t\t\t\t       &status);\n\n\t\t\t/* gadget driver handles all other requests.  block\n\t\t\t * until setup() returns; no reentrancy issues etc.\n\t\t\t */\n\t\t\tif (value > 0) {\n\t\t\t\tspin_unlock(&dum->lock);\n\t\t\t\tvalue = dum->driver->setup(&dum->gadget,\n\t\t\t\t\t\t&setup);\n\t\t\t\tspin_lock(&dum->lock);\n\n\t\t\t\tif (value >= 0) {\n\t\t\t\t\t/* no delays (max 64KB data stage) */\n\t\t\t\t\tlimit = 64*1024;\n\t\t\t\t\tgoto treat_control_like_bulk;\n\t\t\t\t}\n\t\t\t\t/* error, see below */\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\tif (value != -EOPNOTSUPP)\n\t\t\t\t\tdev_dbg(udc_dev(dum),\n\t\t\t\t\t\t\"setup --> %d\\n\",\n\t\t\t\t\t\tvalue);\n\t\t\t\tstatus = -EPIPE;\n\t\t\t\turb->actual_length = 0;\n\t\t\t}\n\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\t/* non-control requests */\n\t\tlimit = total;\n\t\tswitch (usb_pipetype(urb->pipe)) {\n\t\tcase PIPE_ISOCHRONOUS:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * use urb->iso_frame_desc[i].\n\t\t\t * complete whether or not ep has requests queued.\n\t\t\t * report random errors, to debug drivers.\n\t\t\t */\n\t\t\tlimit = max(limit, periodic_bytes(dum, ep));\n\t\t\tstatus = -ENOSYS;\n\t\t\tbreak;\n\n\t\tcase PIPE_INTERRUPT:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * this almost certainly polls too fast.\n\t\t\t */\n\t\t\tlimit = max(limit, periodic_bytes(dum, ep));\n\t\t\t/* FALLTHROUGH */\n\n\t\tdefault:\ntreat_control_like_bulk:\n\t\t\tep->last_io = jiffies;\n\t\t\ttotal -= transfer(dum_hcd, urb, ep, limit, &status);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* incomplete transfer? */\n\t\tif (status == -EINPROGRESS)\n\t\t\tcontinue;\n\nreturn_urb:\n\t\tlist_del(&urbp->urbp_list);\n\t\tkfree(urbp);\n\t\tif (ep)\n\t\t\tep->already_seen = ep->setup_stage = 0;\n\n\t\tusb_hcd_unlink_urb_from_ep(dummy_hcd_to_hcd(dum_hcd), urb);\n\t\tspin_unlock(&dum->lock);\n\t\tusb_hcd_giveback_urb(dummy_hcd_to_hcd(dum_hcd), urb, status);\n\t\tspin_lock(&dum->lock);\n\n\t\tgoto restart;\n\t}\n\n\tif (list_empty(&dum_hcd->urbp_list)) {\n\t\tusb_put_dev(dum_hcd->udev);\n\t\tdum_hcd->udev = NULL;\n\t} else if (dum_hcd->rh_state == DUMMY_RH_RUNNING) {\n\t\t/* want a 1 msec delay here */\n\t\tmod_timer(&dum_hcd->timer, jiffies + msecs_to_jiffies(1));\n\t}\n\n\tspin_unlock_irqrestore(&dum->lock, flags);\n}\n\nstatic int dummy_urb_enqueue(\n\tstruct usb_hcd\t\t\t*hcd,\n\tstruct urb\t\t\t*urb,\n\tgfp_t\t\t\t\tmem_flags\n) {\n\tstruct dummy_hcd *dum_hcd;\n\tstruct urbp\t*urbp;\n\tunsigned long\tflags;\n\tint\t\trc;\n\n\turbp = kmalloc(sizeof *urbp, mem_flags);\n\tif (!urbp)\n\t\treturn -ENOMEM;\n\turbp->urb = urb;\n\turbp->miter_started = 0;\n\n\tdum_hcd = hcd_to_dummy_hcd(hcd);\n\tspin_lock_irqsave(&dum_hcd->dum->lock, flags);\n\n\trc = dummy_validate_stream(dum_hcd, urb);\n\tif (rc) {\n\t\tkfree(urbp);\n\t\tgoto done;\n\t}\n\n\trc = usb_hcd_link_urb_to_ep(hcd, urb);\n\tif (rc) {\n\t\tkfree(urbp);\n\t\tgoto done;\n\t}\n\n\tif (!dum_hcd->udev) {\n\t\tdum_hcd->udev = urb->dev;\n\t\tusb_get_dev(dum_hcd->udev);\n\t} else if (unlikely(dum_hcd->udev != urb->dev))\n\t\tdev_err(dummy_dev(dum_hcd), \"usb_device address has changed!\\n\");\n\n\tlist_add_tail(&urbp->urbp_list, &dum_hcd->urbp_list);\n\turb->hcpriv = urbp;\n\tif (!dum_hcd->next_frame_urbp)\n\t\tdum_hcd->next_frame_urbp = urbp;\n\tif (usb_pipetype(urb->pipe) == PIPE_CONTROL)\n\t\turb->error_count = 1;\t\t/* mark as a new urb */\n\n\t/* kick the scheduler, it'll do the rest */\n\tif (!timer_pending(&dum_hcd->timer))\n\t\tmod_timer(&dum_hcd->timer, jiffies + 1);\n\n done:\n\tspin_unlock_irqrestore(&dum_hcd->dum->lock, flags);\n\treturn rc;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void dummy_timer(unsigned long _dum_hcd)\n{\n\tstruct dummy_hcd\t*dum_hcd = (struct dummy_hcd *) _dum_hcd;\n\tstruct dummy\t\t*dum = dum_hcd->dum;\n\tstruct urbp\t\t*urbp, *tmp;\n\tunsigned long\t\tflags;\n\tint\t\t\tlimit, total;\n\tint\t\t\ti;\n\n\t/* simplistic model for one frame's bandwidth */\n\tswitch (dum->gadget.speed) {\n\tcase USB_SPEED_LOW:\n\t\ttotal = 8/*bytes*/ * 12/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_FULL:\n\t\ttotal = 64/*bytes*/ * 19/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_HIGH:\n\t\ttotal = 512/*bytes*/ * 13/*packets*/ * 8/*uframes*/;\n\t\tbreak;\n\tcase USB_SPEED_SUPER:\n\t\t/* Bus speed is 500000 bytes/ms, so use a little less */\n\t\ttotal = 490000;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dummy_dev(dum_hcd), \"bogus device speed\\n\");\n\t\treturn;\n\t}\n\n\t/* FIXME if HZ != 1000 this will probably misbehave ... */\n\n\t/* look at each urb queued by the host side driver */\n\tspin_lock_irqsave (&dum->lock, flags);\n\n\tif (!dum_hcd->udev) {\n\t\tdev_err(dummy_dev(dum_hcd),\n\t\t\t\t\"timer fired with no URBs pending?\\n\");\n\t\tspin_unlock_irqrestore (&dum->lock, flags);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < DUMMY_ENDPOINTS; i++) {\n\t\tif (!ep_name [i])\n\t\t\tbreak;\n\t\tdum->ep [i].already_seen = 0;\n\t}\n\nrestart:\n\tlist_for_each_entry_safe(urbp, tmp, &dum_hcd->urbp_list, urbp_list) {\n\t\tstruct urb\t\t*urb;\n\t\tstruct dummy_request\t*req;\n\t\tu8\t\t\taddress;\n\t\tstruct dummy_ep\t\t*ep = NULL;\n\t\tint\t\t\ttype;\n\t\tint\t\t\tstatus = -EINPROGRESS;\n\n\t\turb = urbp->urb;\n\t\tif (urb->unlinked)\n\t\t\tgoto return_urb;\n\t\telse if (dum_hcd->rh_state != DUMMY_RH_RUNNING)\n\t\t\tcontinue;\n\t\ttype = usb_pipetype (urb->pipe);\n\n\t\t/* used up this frame's non-periodic bandwidth?\n\t\t * FIXME there's infinite bandwidth for control and\n\t\t * periodic transfers ... unrealistic.\n\t\t */\n\t\tif (total <= 0 && type == PIPE_BULK)\n\t\t\tcontinue;\n\n\t\t/* find the gadget's ep for this request (if configured) */\n\t\taddress = usb_pipeendpoint (urb->pipe);\n\t\tif (usb_pipein (urb->pipe))\n\t\t\taddress |= USB_DIR_IN;\n\t\tep = find_endpoint(dum, address);\n\t\tif (!ep) {\n\t\t\t/* set_configuration() disagreement */\n\t\t\tdev_dbg(dummy_dev(dum_hcd),\n\t\t\t\t\"no ep configured for urb %p\\n\",\n\t\t\t\turb);\n\t\t\tstatus = -EPROTO;\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\tif (ep->already_seen)\n\t\t\tcontinue;\n\t\tep->already_seen = 1;\n\t\tif (ep == &dum->ep [0] && urb->error_count) {\n\t\t\tep->setup_stage = 1;\t/* a new urb */\n\t\t\turb->error_count = 0;\n\t\t}\n\t\tif (ep->halted && !ep->setup_stage) {\n\t\t\t/* NOTE: must not be iso! */\n\t\t\tdev_dbg(dummy_dev(dum_hcd), \"ep %s halted, urb %p\\n\",\n\t\t\t\t\tep->ep.name, urb);\n\t\t\tstatus = -EPIPE;\n\t\t\tgoto return_urb;\n\t\t}\n\t\t/* FIXME make sure both ends agree on maxpacket */\n\n\t\t/* handle control requests */\n\t\tif (ep == &dum->ep [0] && ep->setup_stage) {\n\t\t\tstruct usb_ctrlrequest\t\tsetup;\n\t\t\tint\t\t\t\tvalue = 1;\n\n\t\t\tsetup = *(struct usb_ctrlrequest*) urb->setup_packet;\n\t\t\t/* paranoia, in case of stale queued data */\n\t\t\tlist_for_each_entry (req, &ep->queue, queue) {\n\t\t\t\tlist_del_init (&req->queue);\n\t\t\t\treq->req.status = -EOVERFLOW;\n\t\t\t\tdev_dbg (udc_dev(dum), \"stale req = %p\\n\",\n\t\t\t\t\t\treq);\n\n\t\t\t\tspin_unlock (&dum->lock);\n\t\t\t\treq->req.complete (&ep->ep, &req->req);\n\t\t\t\tspin_lock (&dum->lock);\n\t\t\t\tep->already_seen = 0;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\t/* gadget driver never sees set_address or operations\n\t\t\t * on standard feature flags.  some hardware doesn't\n\t\t\t * even expose them.\n\t\t\t */\n\t\t\tep->last_io = jiffies;\n\t\t\tep->setup_stage = 0;\n\t\t\tep->halted = 0;\n\n\t\t\tvalue = handle_control_request(dum_hcd, urb, &setup,\n\t\t\t\t\t\t       &status);\n\n\t\t\t/* gadget driver handles all other requests.  block\n\t\t\t * until setup() returns; no reentrancy issues etc.\n\t\t\t */\n\t\t\tif (value > 0) {\n\t\t\t\tspin_unlock (&dum->lock);\n\t\t\t\tvalue = dum->driver->setup (&dum->gadget,\n\t\t\t\t\t\t&setup);\n\t\t\t\tspin_lock (&dum->lock);\n\n\t\t\t\tif (value >= 0) {\n\t\t\t\t\t/* no delays (max 64KB data stage) */\n\t\t\t\t\tlimit = 64*1024;\n\t\t\t\t\tgoto treat_control_like_bulk;\n\t\t\t\t}\n\t\t\t\t/* error, see below */\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\tif (value != -EOPNOTSUPP)\n\t\t\t\t\tdev_dbg (udc_dev(dum),\n\t\t\t\t\t\t\"setup --> %d\\n\",\n\t\t\t\t\t\tvalue);\n\t\t\t\tstatus = -EPIPE;\n\t\t\t\turb->actual_length = 0;\n\t\t\t}\n\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\t/* non-control requests */\n\t\tlimit = total;\n\t\tswitch (usb_pipetype (urb->pipe)) {\n\t\tcase PIPE_ISOCHRONOUS:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * use urb->iso_frame_desc[i].\n\t\t\t * complete whether or not ep has requests queued.\n\t\t\t * report random errors, to debug drivers.\n\t\t\t */\n\t\t\tlimit = max (limit, periodic_bytes (dum, ep));\n\t\t\tstatus = -ENOSYS;\n\t\t\tbreak;\n\n\t\tcase PIPE_INTERRUPT:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * this almost certainly polls too fast.\n\t\t\t */\n\t\t\tlimit = max (limit, periodic_bytes (dum, ep));\n\t\t\t/* FALLTHROUGH */\n\n\t\t// case PIPE_BULK:  case PIPE_CONTROL:\n\t\tdefault:\n\t\ttreat_control_like_bulk:\n\t\t\tep->last_io = jiffies;\n\t\t\ttotal = transfer(dum, urb, ep, limit, &status);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* incomplete transfer? */\n\t\tif (status == -EINPROGRESS)\n\t\t\tcontinue;\n\nreturn_urb:\n\t\tlist_del (&urbp->urbp_list);\n\t\tkfree (urbp);\n\t\tif (ep)\n\t\t\tep->already_seen = ep->setup_stage = 0;\n\n\t\tusb_hcd_unlink_urb_from_ep(dummy_hcd_to_hcd(dum_hcd), urb);\n\t\tspin_unlock (&dum->lock);\n\t\tusb_hcd_giveback_urb(dummy_hcd_to_hcd(dum_hcd), urb, status);\n\t\tspin_lock (&dum->lock);\n\n\t\tgoto restart;\n\t}\n\n\tif (list_empty(&dum_hcd->urbp_list)) {\n\t\tusb_put_dev(dum_hcd->udev);\n\t\tdum_hcd->udev = NULL;\n\t} else if (dum_hcd->rh_state == DUMMY_RH_RUNNING) {\n\t\t/* want a 1 msec delay here */\n\t\tmod_timer(&dum_hcd->timer, jiffies + msecs_to_jiffies(1));\n\t}\n\n\tspin_unlock_irqrestore (&dum->lock, flags);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstruct urbp\t\t\t*next_frame_urbp;\n\n\nif (!dum_hcd->next_frame_urbp)\n\n\ndum_hcd->next_frame_urbp = urbp;\n\n\nstatic void dummy_timer(unsigned long _dum_hcd)\n{\n\tstruct dummy_hcd\t*dum_hcd = (struct dummy_hcd *) _dum_hcd;\n\tstruct dummy\t\t*dum = dum_hcd->dum;\n\tstruct urbp\t\t*urbp, *tmp;\n\tunsigned long\t\tflags;\n\tint\t\t\tlimit, total;\n\tint\t\t\ti;\n\n\t/* simplistic model for one frame's bandwidth */\n\tswitch (dum->gadget.speed) {\n\tcase USB_SPEED_LOW:\n\t\ttotal = 8/*bytes*/ * 12/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_FULL:\n\t\ttotal = 64/*bytes*/ * 19/*packets*/;\n\t\tbreak;\n\tcase USB_SPEED_HIGH:\n\t\ttotal = 512/*bytes*/ * 13/*packets*/ * 8/*uframes*/;\n\t\tbreak;\n\tcase USB_SPEED_SUPER:\n\t\t/* Bus speed is 500000 bytes/ms, so use a little less */\n\t\ttotal = 490000;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dummy_dev(dum_hcd), \"bogus device speed\\n\");\n\t\treturn;\n\t}\n\n\t/* FIXME if HZ != 1000 this will probably misbehave ... */\n\n\t/* look at each urb queued by the host side driver */\n\tspin_lock_irqsave (&dum->lock, flags);\n\n\tif (!dum_hcd->udev) {\n\t\tdev_err(dummy_dev(dum_hcd),\n\t\t\t\t\"timer fired with no URBs pending?\\n\");\n\t\tspin_unlock_irqrestore (&dum->lock, flags);\n\t\treturn;\n\t}\n\tdum_hcd->next_frame_urbp = NULL;\n\n\tfor (i = 0; i < DUMMY_ENDPOINTS; i++) {\n\t\tif (!ep_name [i])\n\t\t\tbreak;\n\t\tdum->ep [i].already_seen = 0;\n\t}\n\nrestart:\n\tlist_for_each_entry_safe(urbp, tmp, &dum_hcd->urbp_list, urbp_list) {\n\t\tstruct urb\t\t*urb;\n\t\tstruct dummy_request\t*req;\n\t\tu8\t\t\taddress;\n\t\tstruct dummy_ep\t\t*ep = NULL;\n\t\tint\t\t\ttype;\n\t\tint\t\t\tstatus = -EINPROGRESS;\n\n\t\t/* stop when we reach URBs queued after the timer interrupt */\n\t\tif (urbp == dum_hcd->next_frame_urbp)\n\t\t\tbreak;\n\n\t\turb = urbp->urb;\n\t\tif (urb->unlinked)\n\t\t\tgoto return_urb;\n\t\telse if (dum_hcd->rh_state != DUMMY_RH_RUNNING)\n\t\t\tcontinue;\n\t\ttype = usb_pipetype (urb->pipe);\n\n\t\t/* used up this frame's non-periodic bandwidth?\n\t\t * FIXME there's infinite bandwidth for control and\n\t\t * periodic transfers ... unrealistic.\n\t\t */\n\t\tif (total <= 0 && type == PIPE_BULK)\n\t\t\tcontinue;\n\n\t\t/* find the gadget's ep for this request (if configured) */\n\t\taddress = usb_pipeendpoint (urb->pipe);\n\t\tif (usb_pipein (urb->pipe))\n\t\t\taddress |= USB_DIR_IN;\n\t\tep = find_endpoint(dum, address);\n\t\tif (!ep) {\n\t\t\t/* set_configuration() disagreement */\n\t\t\tdev_dbg(dummy_dev(dum_hcd),\n\t\t\t\t\"no ep configured for urb %p\\n\",\n\t\t\t\turb);\n\t\t\tstatus = -EPROTO;\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\tif (ep->already_seen)\n\t\t\tcontinue;\n\t\tep->already_seen = 1;\n\t\tif (ep == &dum->ep [0] && urb->error_count) {\n\t\t\tep->setup_stage = 1;\t/* a new urb */\n\t\t\turb->error_count = 0;\n\t\t}\n\t\tif (ep->halted && !ep->setup_stage) {\n\t\t\t/* NOTE: must not be iso! */\n\t\t\tdev_dbg(dummy_dev(dum_hcd), \"ep %s halted, urb %p\\n\",\n\t\t\t\t\tep->ep.name, urb);\n\t\t\tstatus = -EPIPE;\n\t\t\tgoto return_urb;\n\t\t}\n\t\t/* FIXME make sure both ends agree on maxpacket */\n\n\t\t/* handle control requests */\n\t\tif (ep == &dum->ep [0] && ep->setup_stage) {\n\t\t\tstruct usb_ctrlrequest\t\tsetup;\n\t\t\tint\t\t\t\tvalue = 1;\n\n\t\t\tsetup = *(struct usb_ctrlrequest*) urb->setup_packet;\n\t\t\t/* paranoia, in case of stale queued data */\n\t\t\tlist_for_each_entry (req, &ep->queue, queue) {\n\t\t\t\tlist_del_init (&req->queue);\n\t\t\t\treq->req.status = -EOVERFLOW;\n\t\t\t\tdev_dbg (udc_dev(dum), \"stale req = %p\\n\",\n\t\t\t\t\t\treq);\n\n\t\t\t\tspin_unlock (&dum->lock);\n\t\t\t\treq->req.complete (&ep->ep, &req->req);\n\t\t\t\tspin_lock (&dum->lock);\n\t\t\t\tep->already_seen = 0;\n\t\t\t\tgoto restart;\n\t\t\t}\n\n\t\t\t/* gadget driver never sees set_address or operations\n\t\t\t * on standard feature flags.  some hardware doesn't\n\t\t\t * even expose them.\n\t\t\t */\n\t\t\tep->last_io = jiffies;\n\t\t\tep->setup_stage = 0;\n\t\t\tep->halted = 0;\n\n\t\t\tvalue = handle_control_request(dum_hcd, urb, &setup,\n\t\t\t\t\t\t       &status);\n\n\t\t\t/* gadget driver handles all other requests.  block\n\t\t\t * until setup() returns; no reentrancy issues etc.\n\t\t\t */\n\t\t\tif (value > 0) {\n\t\t\t\tspin_unlock (&dum->lock);\n\t\t\t\tvalue = dum->driver->setup (&dum->gadget,\n\t\t\t\t\t\t&setup);\n\t\t\t\tspin_lock (&dum->lock);\n\n\t\t\t\tif (value >= 0) {\n\t\t\t\t\t/* no delays (max 64KB data stage) */\n\t\t\t\t\tlimit = 64*1024;\n\t\t\t\t\tgoto treat_control_like_bulk;\n\t\t\t\t}\n\t\t\t\t/* error, see below */\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\tif (value != -EOPNOTSUPP)\n\t\t\t\t\tdev_dbg (udc_dev(dum),\n\t\t\t\t\t\t\"setup --> %d\\n\",\n\t\t\t\t\t\tvalue);\n\t\t\t\tstatus = -EPIPE;\n\t\t\t\turb->actual_length = 0;\n\t\t\t}\n\n\t\t\tgoto return_urb;\n\t\t}\n\n\t\t/* non-control requests */\n\t\tlimit = total;\n\t\tswitch (usb_pipetype (urb->pipe)) {\n\t\tcase PIPE_ISOCHRONOUS:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * use urb->iso_frame_desc[i].\n\t\t\t * complete whether or not ep has requests queued.\n\t\t\t * report random errors, to debug drivers.\n\t\t\t */\n\t\t\tlimit = max (limit, periodic_bytes (dum, ep));\n\t\t\tstatus = -ENOSYS;\n\t\t\tbreak;\n\n\t\tcase PIPE_INTERRUPT:\n\t\t\t/* FIXME is it urb->interval since the last xfer?\n\t\t\t * this almost certainly polls too fast.\n\t\t\t */\n\t\t\tlimit = max (limit, periodic_bytes (dum, ep));\n\t\t\t/* FALLTHROUGH */\n\n\t\t// case PIPE_BULK:  case PIPE_CONTROL:\n\t\tdefault:\n\t\ttreat_control_like_bulk:\n\t\t\tep->last_io = jiffies;\n\t\t\ttotal = transfer(dum, urb, ep, limit, &status);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* incomplete transfer? */\n\t\tif (status == -EINPROGRESS)\n\t\t\tcontinue;\n\nreturn_urb:\n\t\tlist_del (&urbp->urbp_list);\n\t\tkfree (urbp);\n\t\tif (ep)\n\t\t\tep->already_seen = ep->setup_stage = 0;\n\n\t\tusb_hcd_unlink_urb_from_ep(dummy_hcd_to_hcd(dum_hcd), urb);\n\t\tspin_unlock (&dum->lock);\n\t\tusb_hcd_giveback_urb(dummy_hcd_to_hcd(dum_hcd), urb, status);\n\t\tspin_lock (&dum->lock);\n\n\t\tgoto restart;\n\t}\n\n\tif (list_empty(&dum_hcd->urbp_list)) {\n\t\tusb_put_dev(dum_hcd->udev);\n\t\tdum_hcd->udev = NULL;\n\t} else if (dum_hcd->rh_state == DUMMY_RH_RUNNING) {\n\t\t/* want a 1 msec delay here */\n\t\tmod_timer(&dum_hcd->timer, jiffies + msecs_to_jiffies(1));\n\t}\n\n\tspin_unlock_irqrestore (&dum->lock, flags);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int mwifiex_pcie_process_event_ready(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tconst struct mwifiex_pcie_card_reg *reg = card->pcie.reg;\n\tu32 rdptr = card->evtbd_rdptr & MWIFIEX_EVTBD_MASK;\n\tu32 wrptr, event;\n\tstruct mwifiex_evt_buf_desc *desc;\n\n\tif (!mwifiex_pcie_ok_to_access_hw(adapter))\n\t\tmwifiex_pm_wakeup_card(adapter);\n\n\tif (adapter->event_received) {\n\t\tmwifiex_dbg(adapter, EVENT,\n\t\t\t    \"info: Event being processed,\\t\"\n\t\t\t    \"do not process this interrupt just yet\\n\");\n\t\treturn 0;\n\t}\n\n\tif (rdptr >= MWIFIEX_MAX_EVT_BD) {\n\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t    \"info: Invalid read pointer...\\n\");\n\t\treturn -1;\n\t}\n\n\t/* Read the event ring write pointer set by firmware */\n\tif (mwifiex_read_reg(adapter, reg->evt_wrptr, &wrptr)) {\n\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t    \"EventReady: failed to read reg->evt_wrptr\\n\");\n\t\treturn -1;\n\t}\n\n\tmwifiex_dbg(adapter, EVENT,\n\t\t    \"info: EventReady: Initial <Rd: 0x%x, Wr: 0x%x>\",\n\t\t    card->evtbd_rdptr, wrptr);\n\tif (((wrptr & MWIFIEX_EVTBD_MASK) != (card->evtbd_rdptr\n\t\t\t\t\t      & MWIFIEX_EVTBD_MASK)) ||\n\t    ((wrptr & reg->evt_rollover_ind) ==\n\t     (card->evtbd_rdptr & reg->evt_rollover_ind))) {\n\t\tstruct sk_buff *skb_cmd;\n\t\t__le16 data_len = 0;\n\t\tu16 evt_len;\n\n\t\tmwifiex_dbg(adapter, INFO,\n\t\t\t    \"info: Read Index: %d\\n\", rdptr);\n\t\tskb_cmd = card->evt_buf_list[rdptr];\n\t\tmwifiex_unmap_pci_memory(adapter, skb_cmd, PCI_DMA_FROMDEVICE);\n\n\t\t/* Take the pointer and set it to event pointer in adapter\n\t\t   and will return back after event handling callback */\n\t\tcard->evt_buf_list[rdptr] = NULL;\n\t\tdesc = card->evtbd_ring[rdptr];\n\t\tmemset(desc, 0, sizeof(*desc));\n\n\t\tevent = get_unaligned_le32(\n\t\t\t&skb_cmd->data[adapter->intf_hdr_len]);\n\t\tadapter->event_cause = event;\n\t\t/* The first 4bytes will be the event transfer header\n\t\t   len is 2 bytes followed by type which is 2 bytes */\n\t\tmemcpy(&data_len, skb_cmd->data, sizeof(__le16));\n\t\tevt_len = le16_to_cpu(data_len);\n\t\tskb_trim(skb_cmd, evt_len);\n\t\tskb_pull(skb_cmd, adapter->intf_hdr_len);\n\t\tmwifiex_dbg(adapter, EVENT,\n\t\t\t    \"info: Event length: %d\\n\", evt_len);\n\n\t\tif ((evt_len > 0) && (evt_len  < MAX_EVENT_SIZE))\n\t\t\tmemcpy(adapter->event_body, skb_cmd->data +\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN, evt_len -\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN);\n\n\t\tadapter->event_received = true;\n\t\tadapter->event_skb = skb_cmd;\n\n\t\t/* Do not update the event read pointer here, wait till the\n\t\t   buffer is released. This is just to make things simpler,\n\t\t   we need to find a better method of managing these buffers.\n\t\t*/\n\t} else {\n\t\tif (mwifiex_write_reg(adapter, PCIE_CPU_INT_EVENT,\n\t\t\t\t      CPU_INTR_EVENT_DONE)) {\n\t\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t\t    \"Write register failed\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int mwifiex_pcie_process_event_ready(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tconst struct mwifiex_pcie_card_reg *reg = card->pcie.reg;\n\tu32 rdptr = card->evtbd_rdptr & MWIFIEX_EVTBD_MASK;\n\tu32 wrptr, event;\n\tstruct mwifiex_evt_buf_desc *desc;\n\n\tif (!mwifiex_pcie_ok_to_access_hw(adapter))\n\t\tmwifiex_pm_wakeup_card(adapter);\n\n\tif (adapter->event_received) {\n\t\tmwifiex_dbg(adapter, EVENT,\n\t\t\t    \"info: Event being processed,\\t\"\n\t\t\t    \"do not process this interrupt just yet\\n\");\n\t\treturn 0;\n\t}\n\n\tif (rdptr >= MWIFIEX_MAX_EVT_BD) {\n\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t    \"info: Invalid read pointer...\\n\");\n\t\treturn -1;\n\t}\n\n\t/* Read the event ring write pointer set by firmware */\n\tif (mwifiex_read_reg(adapter, reg->evt_wrptr, &wrptr)) {\n\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t    \"EventReady: failed to read reg->evt_wrptr\\n\");\n\t\treturn -1;\n\t}\n\n\tmwifiex_dbg(adapter, EVENT,\n\t\t    \"info: EventReady: Initial <Rd: 0x%x, Wr: 0x%x>\",\n\t\t    card->evtbd_rdptr, wrptr);\n\tif (((wrptr & MWIFIEX_EVTBD_MASK) != (card->evtbd_rdptr\n\t\t\t\t\t      & MWIFIEX_EVTBD_MASK)) ||\n\t    ((wrptr & reg->evt_rollover_ind) ==\n\t     (card->evtbd_rdptr & reg->evt_rollover_ind))) {\n\t\tstruct sk_buff *skb_cmd;\n\t\t__le16 data_len = 0;\n\t\tu16 evt_len;\n\n\t\tmwifiex_dbg(adapter, INFO,\n\t\t\t    \"info: Read Index: %d\\n\", rdptr);\n\t\tskb_cmd = card->evt_buf_list[rdptr];\n\t\tmwifiex_unmap_pci_memory(adapter, skb_cmd, PCI_DMA_FROMDEVICE);\n\n\t\t/* Take the pointer and set it to event pointer in adapter\n\t\t   and will return back after event handling callback */\n\t\tcard->evt_buf_list[rdptr] = NULL;\n\t\tdesc = card->evtbd_ring[rdptr];\n\t\tmemset(desc, 0, sizeof(*desc));\n\n\t\tevent = get_unaligned_le32(\n\t\t\t&skb_cmd->data[adapter->intf_hdr_len]);\n\t\tadapter->event_cause = event;\n\t\t/* The first 4bytes will be the event transfer header\n\t\t   len is 2 bytes followed by type which is 2 bytes */\n\t\tmemcpy(&data_len, skb_cmd->data, sizeof(__le16));\n\t\tevt_len = le16_to_cpu(data_len);\n\t\tskb_trim(skb_cmd, evt_len);\n\t\tskb_pull(skb_cmd, adapter->intf_hdr_len);\n\t\tmwifiex_dbg(adapter, EVENT,\n\t\t\t    \"info: Event length: %d\\n\", evt_len);\n\n\t\tif (evt_len > MWIFIEX_EVENT_HEADER_LEN &&\n\t\t    evt_len < MAX_EVENT_SIZE)\n\t\t\tmemcpy(adapter->event_body, skb_cmd->data +\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN, evt_len -\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN);\n\n\t\tadapter->event_received = true;\n\t\tadapter->event_skb = skb_cmd;\n\n\t\t/* Do not update the event read pointer here, wait till the\n\t\t   buffer is released. This is just to make things simpler,\n\t\t   we need to find a better method of managing these buffers.\n\t\t*/\n\t} else {\n\t\tif (mwifiex_write_reg(adapter, PCIE_CPU_INT_EVENT,\n\t\t\t\t      CPU_INTR_EVENT_DONE)) {\n\t\t\tmwifiex_dbg(adapter, ERROR,\n\t\t\t\t    \"Write register failed\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int mwifiex_pcie_process_event_ready(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tconst struct mwifiex_pcie_card_reg *reg = card->pcie.reg;\n\tu32 rdptr = card->evtbd_rdptr & MWIFIEX_EVTBD_MASK;\n\tu32 wrptr, event;\n\tstruct mwifiex_evt_buf_desc *desc;\n\n\tif (!mwifiex_pcie_ok_to_access_hw(adapter))\n\t\tmwifiex_pm_wakeup_card(adapter);\n\n\tif (adapter->event_received) {\n\t\tdev_dbg(adapter->dev, \"info: Event being processed, \"\n\t\t\t\"do not process this interrupt just yet\\n\");\n\t\treturn 0;\n\t}\n\n\tif (rdptr >= MWIFIEX_MAX_EVT_BD) {\n\t\tdev_dbg(adapter->dev, \"info: Invalid read pointer...\\n\");\n\t\treturn -1;\n\t}\n\n\t/* Read the event ring write pointer set by firmware */\n\tif (mwifiex_read_reg(adapter, reg->evt_wrptr, &wrptr)) {\n\t\tdev_err(adapter->dev,\n\t\t\t\"EventReady: failed to read reg->evt_wrptr\\n\");\n\t\treturn -1;\n\t}\n\n\tdev_dbg(adapter->dev, \"info: EventReady: Initial <Rd: 0x%x, Wr: 0x%x>\",\n\t\tcard->evtbd_rdptr, wrptr);\n\tif (((wrptr & MWIFIEX_EVTBD_MASK) != (card->evtbd_rdptr\n\t\t\t\t\t      & MWIFIEX_EVTBD_MASK)) ||\n\t    ((wrptr & reg->evt_rollover_ind) ==\n\t     (card->evtbd_rdptr & reg->evt_rollover_ind))) {\n\t\tstruct sk_buff *skb_cmd;\n\t\t__le16 data_len = 0;\n\t\tu16 evt_len;\n\n\t\tdev_dbg(adapter->dev, \"info: Read Index: %d\\n\", rdptr);\n\t\tskb_cmd = card->evt_buf_list[rdptr];\n\t\tmwifiex_unmap_pci_memory(adapter, skb_cmd, PCI_DMA_FROMDEVICE);\n\n\t\t/* Take the pointer and set it to event pointer in adapter\n\t\t   and will return back after event handling callback */\n\t\tcard->evt_buf_list[rdptr] = NULL;\n\t\tdesc = card->evtbd_ring[rdptr];\n\t\tmemset(desc, 0, sizeof(*desc));\n\n\t\tevent = *(u32 *) &skb_cmd->data[INTF_HEADER_LEN];\n\t\tadapter->event_cause = event;\n\t\t/* The first 4bytes will be the event transfer header\n\t\t   len is 2 bytes followed by type which is 2 bytes */\n\t\tmemcpy(&data_len, skb_cmd->data, sizeof(__le16));\n\t\tevt_len = le16_to_cpu(data_len);\n\n\t\tskb_pull(skb_cmd, INTF_HEADER_LEN);\n\t\tdev_dbg(adapter->dev, \"info: Event length: %d\\n\", evt_len);\n\n\t\tif ((evt_len > 0) && (evt_len  < MAX_EVENT_SIZE))\n\t\t\tmemcpy(adapter->event_body, skb_cmd->data +\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN, evt_len -\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN);\n\n\t\tadapter->event_received = true;\n\t\tadapter->event_skb = skb_cmd;\n\n\t\t/* Do not update the event read pointer here, wait till the\n\t\t   buffer is released. This is just to make things simpler,\n\t\t   we need to find a better method of managing these buffers.\n\t\t*/\n\t}\n\n\treturn 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int mwifiex_pcie_process_event_ready(struct mwifiex_adapter *adapter)\n{\n\tstruct pcie_service_card *card = adapter->card;\n\tconst struct mwifiex_pcie_card_reg *reg = card->pcie.reg;\n\tu32 rdptr = card->evtbd_rdptr & MWIFIEX_EVTBD_MASK;\n\tu32 wrptr, event;\n\tstruct mwifiex_evt_buf_desc *desc;\n\n\tif (!mwifiex_pcie_ok_to_access_hw(adapter))\n\t\tmwifiex_pm_wakeup_card(adapter);\n\n\tif (adapter->event_received) {\n\t\tdev_dbg(adapter->dev, \"info: Event being processed, \"\n\t\t\t\"do not process this interrupt just yet\\n\");\n\t\treturn 0;\n\t}\n\n\tif (rdptr >= MWIFIEX_MAX_EVT_BD) {\n\t\tdev_dbg(adapter->dev, \"info: Invalid read pointer...\\n\");\n\t\treturn -1;\n\t}\n\n\t/* Read the event ring write pointer set by firmware */\n\tif (mwifiex_read_reg(adapter, reg->evt_wrptr, &wrptr)) {\n\t\tdev_err(adapter->dev,\n\t\t\t\"EventReady: failed to read reg->evt_wrptr\\n\");\n\t\treturn -1;\n\t}\n\n\tdev_dbg(adapter->dev, \"info: EventReady: Initial <Rd: 0x%x, Wr: 0x%x>\",\n\t\tcard->evtbd_rdptr, wrptr);\n\tif (((wrptr & MWIFIEX_EVTBD_MASK) != (card->evtbd_rdptr\n\t\t\t\t\t      & MWIFIEX_EVTBD_MASK)) ||\n\t    ((wrptr & reg->evt_rollover_ind) ==\n\t     (card->evtbd_rdptr & reg->evt_rollover_ind))) {\n\t\tstruct sk_buff *skb_cmd;\n\t\t__le16 data_len = 0;\n\t\tu16 evt_len;\n\n\t\tdev_dbg(adapter->dev, \"info: Read Index: %d\\n\", rdptr);\n\t\tskb_cmd = card->evt_buf_list[rdptr];\n\t\tmwifiex_unmap_pci_memory(adapter, skb_cmd, PCI_DMA_FROMDEVICE);\n\n\t\t/* Take the pointer and set it to event pointer in adapter\n\t\t   and will return back after event handling callback */\n\t\tcard->evt_buf_list[rdptr] = NULL;\n\t\tdesc = card->evtbd_ring[rdptr];\n\t\tmemset(desc, 0, sizeof(*desc));\n\n\t\tevent = *(u32 *) &skb_cmd->data[INTF_HEADER_LEN];\n\t\tadapter->event_cause = event;\n\t\t/* The first 4bytes will be the event transfer header\n\t\t   len is 2 bytes followed by type which is 2 bytes */\n\t\tmemcpy(&data_len, skb_cmd->data, sizeof(__le16));\n\t\tevt_len = le16_to_cpu(data_len);\n\n\t\tskb_pull(skb_cmd, INTF_HEADER_LEN);\n\t\tdev_dbg(adapter->dev, \"info: Event length: %d\\n\", evt_len);\n\n\t\tif (evt_len > MWIFIEX_EVENT_HEADER_LEN &&\n\t\t    evt_len < MAX_EVENT_SIZE)\n\t\t\tmemcpy(adapter->event_body, skb_cmd->data +\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN, evt_len -\n\t\t\t       MWIFIEX_EVENT_HEADER_LEN);\n\n\t\tadapter->event_received = true;\n\t\tadapter->event_skb = skb_cmd;\n\n\t\t/* Do not update the event read pointer here, wait till the\n\t\t   buffer is released. This is just to make things simpler,\n\t\t   we need to find a better method of managing these buffers.\n\t\t*/\n\t}\n\n\treturn 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nint mac_partition(struct parsed_partitions *state)\n{\n\tSector sect;\n\tunsigned char *data;\n\tint slot, blocks_in_map;\n\tunsigned secsize;\n#ifdef CONFIG_PPC_PMAC\n\tint found_root = 0;\n\tint found_root_goodness = 0;\n#endif\n\tstruct mac_partition *part;\n\tstruct mac_driver_desc *md;\n\n\t/* Get 0th block and look at the first partition map entry. */\n\tmd = read_part_sector(state, 0, &sect);\n\tif (!md)\n\t\treturn -1;\n\tif (be16_to_cpu(md->signature) != MAC_DRIVER_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\tsecsize = be16_to_cpu(md->block_size);\n\tput_dev_sector(sect);\n\tdata = read_part_sector(state, secsize/512, &sect);\n\tif (!data)\n\t\treturn -1;\n\tpart = (struct mac_partition *) (data + secsize%512);\n\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\t\t/* not a MacOS disk */\n\t}\n\tblocks_in_map = be32_to_cpu(part->map_count);\n\tif (blocks_in_map < 0 || blocks_in_map >= DISK_MAX_PARTS) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\n\tif (blocks_in_map >= state->limit)\n\t\tblocks_in_map = state->limit - 1;\n\n\tstrlcat(state->pp_buf, \" [mac]\", PAGE_SIZE);\n\tfor (slot = 1; slot <= blocks_in_map; ++slot) {\n\t\tint pos = slot * secsize;\n\t\tput_dev_sector(sect);\n\t\tdata = read_part_sector(state, pos/512, &sect);\n\t\tif (!data)\n\t\t\treturn -1;\n\t\tpart = (struct mac_partition *) (data + pos%512);\n\t\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC)\n\t\t\tbreak;\n\t\tput_partition(state, slot,\n\t\t\tbe32_to_cpu(part->start_block) * (secsize/512),\n\t\t\tbe32_to_cpu(part->block_count) * (secsize/512));\n\n\t\tif (!strncasecmp(part->type, \"Linux_RAID\", 10))\n\t\t\tstate->parts[slot].flags = ADDPART_FLAG_RAID;\n#ifdef CONFIG_PPC_PMAC\n\t\t/*\n\t\t * If this is the first bootable partition, tell the\n\t\t * setup code, in case it wants to make this the root.\n\t\t */\n\t\tif (machine_is(powermac)) {\n\t\t\tint goodness = 0;\n\n\t\t\tmac_fix_string(part->processor, 16);\n\t\t\tmac_fix_string(part->name, 32);\n\t\t\tmac_fix_string(part->type, 32);\t\t\t\t\t\n\t\t    \n\t\t\tif ((be32_to_cpu(part->status) & MAC_STATUS_BOOTABLE)\n\t\t\t    && strcasecmp(part->processor, \"powerpc\") == 0)\n\t\t\t\tgoodness++;\n\n\t\t\tif (strcasecmp(part->type, \"Apple_UNIX_SVR2\") == 0\n\t\t\t    || (strncasecmp(part->type, \"Linux\", 5) == 0\n\t\t\t        && strcasecmp(part->type, \"Linux_swap\") != 0)) {\n\t\t\t\tint i, l;\n\n\t\t\t\tgoodness++;\n\t\t\t\tl = strlen(part->name);\n\t\t\t\tif (strcmp(part->name, \"/\") == 0)\n\t\t\t\t\tgoodness++;\n\t\t\t\tfor (i = 0; i <= l - 4; ++i) {\n\t\t\t\t\tif (strncasecmp(part->name + i, \"root\",\n\t\t\t\t\t\t     4) == 0) {\n\t\t\t\t\t\tgoodness += 2;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (strncasecmp(part->name, \"swap\", 4) == 0)\n\t\t\t\t\tgoodness--;\n\t\t\t}\n\n\t\t\tif (goodness > found_root_goodness) {\n\t\t\t\tfound_root = slot;\n\t\t\t\tfound_root_goodness = goodness;\n\t\t\t}\n\t\t}\n#endif /* CONFIG_PPC_PMAC */\n\t}\n#ifdef CONFIG_PPC_PMAC\n\tif (found_root_goodness)\n\t\tnote_bootable_part(state->bdev->bd_dev, found_root,\n\t\t\t\t   found_root_goodness);\n#endif\n\n\tput_dev_sector(sect);\n\tstrlcat(state->pp_buf, \"\\n\", PAGE_SIZE);\n\treturn 1;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nint mac_partition(struct parsed_partitions *state)\n{\n\tSector sect;\n\tunsigned char *data;\n\tint slot, blocks_in_map;\n\tunsigned secsize, datasize, partoffset;\n#ifdef CONFIG_PPC_PMAC\n\tint found_root = 0;\n\tint found_root_goodness = 0;\n#endif\n\tstruct mac_partition *part;\n\tstruct mac_driver_desc *md;\n\n\t/* Get 0th block and look at the first partition map entry. */\n\tmd = read_part_sector(state, 0, &sect);\n\tif (!md)\n\t\treturn -1;\n\tif (be16_to_cpu(md->signature) != MAC_DRIVER_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\tsecsize = be16_to_cpu(md->block_size);\n\tput_dev_sector(sect);\n\tdatasize = round_down(secsize, 512);\n\tdata = read_part_sector(state, datasize / 512, &sect);\n\tif (!data)\n\t\treturn -1;\n\tpartoffset = secsize % 512;\n\tif (partoffset + sizeof(*part) > datasize)\n\t\treturn -1;\n\tpart = (struct mac_partition *) (data + partoffset);\n\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\t\t/* not a MacOS disk */\n\t}\n\tblocks_in_map = be32_to_cpu(part->map_count);\n\tif (blocks_in_map < 0 || blocks_in_map >= DISK_MAX_PARTS) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\n\tif (blocks_in_map >= state->limit)\n\t\tblocks_in_map = state->limit - 1;\n\n\tstrlcat(state->pp_buf, \" [mac]\", PAGE_SIZE);\n\tfor (slot = 1; slot <= blocks_in_map; ++slot) {\n\t\tint pos = slot * secsize;\n\t\tput_dev_sector(sect);\n\t\tdata = read_part_sector(state, pos/512, &sect);\n\t\tif (!data)\n\t\t\treturn -1;\n\t\tpart = (struct mac_partition *) (data + pos%512);\n\t\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC)\n\t\t\tbreak;\n\t\tput_partition(state, slot,\n\t\t\tbe32_to_cpu(part->start_block) * (secsize/512),\n\t\t\tbe32_to_cpu(part->block_count) * (secsize/512));\n\n\t\tif (!strncasecmp(part->type, \"Linux_RAID\", 10))\n\t\t\tstate->parts[slot].flags = ADDPART_FLAG_RAID;\n#ifdef CONFIG_PPC_PMAC\n\t\t/*\n\t\t * If this is the first bootable partition, tell the\n\t\t * setup code, in case it wants to make this the root.\n\t\t */\n\t\tif (machine_is(powermac)) {\n\t\t\tint goodness = 0;\n\n\t\t\tmac_fix_string(part->processor, 16);\n\t\t\tmac_fix_string(part->name, 32);\n\t\t\tmac_fix_string(part->type, 32);\t\t\t\t\t\n\t\t    \n\t\t\tif ((be32_to_cpu(part->status) & MAC_STATUS_BOOTABLE)\n\t\t\t    && strcasecmp(part->processor, \"powerpc\") == 0)\n\t\t\t\tgoodness++;\n\n\t\t\tif (strcasecmp(part->type, \"Apple_UNIX_SVR2\") == 0\n\t\t\t    || (strncasecmp(part->type, \"Linux\", 5) == 0\n\t\t\t        && strcasecmp(part->type, \"Linux_swap\") != 0)) {\n\t\t\t\tint i, l;\n\n\t\t\t\tgoodness++;\n\t\t\t\tl = strlen(part->name);\n\t\t\t\tif (strcmp(part->name, \"/\") == 0)\n\t\t\t\t\tgoodness++;\n\t\t\t\tfor (i = 0; i <= l - 4; ++i) {\n\t\t\t\t\tif (strncasecmp(part->name + i, \"root\",\n\t\t\t\t\t\t     4) == 0) {\n\t\t\t\t\t\tgoodness += 2;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (strncasecmp(part->name, \"swap\", 4) == 0)\n\t\t\t\t\tgoodness--;\n\t\t\t}\n\n\t\t\tif (goodness > found_root_goodness) {\n\t\t\t\tfound_root = slot;\n\t\t\t\tfound_root_goodness = goodness;\n\t\t\t}\n\t\t}\n#endif /* CONFIG_PPC_PMAC */\n\t}\n#ifdef CONFIG_PPC_PMAC\n\tif (found_root_goodness)\n\t\tnote_bootable_part(state->bdev->bd_dev, found_root,\n\t\t\t\t   found_root_goodness);\n#endif\n\n\tput_dev_sector(sect);\n\tstrlcat(state->pp_buf, \"\\n\", PAGE_SIZE);\n\treturn 1;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nint mac_partition(struct parsed_partitions *state)\n{\n\tSector sect;\n\tunsigned char *data;\n\tint slot, blocks_in_map;\n\tunsigned secsize;\n#ifdef CONFIG_PPC_PMAC\n\tint found_root = 0;\n\tint found_root_goodness = 0;\n#endif\n\tstruct mac_partition *part;\n\tstruct mac_driver_desc *md;\n\n\t/* Get 0th block and look at the first partition map entry. */\n\tmd = read_part_sector(state, 0, &sect);\n\tif (!md)\n\t\treturn -1;\n\tif (be16_to_cpu(md->signature) != MAC_DRIVER_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\tsecsize = be16_to_cpu(md->block_size);\n\tput_dev_sector(sect);\n\tdata = read_part_sector(state, secsize/512, &sect);\n\tif (!data)\n\t\treturn -1;\n\tpart = (struct mac_partition *) (data + secsize%512);\n\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\t\t/* not a MacOS disk */\n\t}\n\tblocks_in_map = be32_to_cpu(part->map_count);\n\tif (blocks_in_map < 0 || blocks_in_map >= DISK_MAX_PARTS) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\tstrlcat(state->pp_buf, \" [mac]\", PAGE_SIZE);\n\tfor (slot = 1; slot <= blocks_in_map; ++slot) {\n\t\tint pos = slot * secsize;\n\t\tput_dev_sector(sect);\n\t\tdata = read_part_sector(state, pos/512, &sect);\n\t\tif (!data)\n\t\t\treturn -1;\n\t\tpart = (struct mac_partition *) (data + pos%512);\n\t\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC)\n\t\t\tbreak;\n\t\tput_partition(state, slot,\n\t\t\tbe32_to_cpu(part->start_block) * (secsize/512),\n\t\t\tbe32_to_cpu(part->block_count) * (secsize/512));\n\n\t\tif (!strnicmp(part->type, \"Linux_RAID\", 10))\n\t\t\tstate->parts[slot].flags = ADDPART_FLAG_RAID;\n#ifdef CONFIG_PPC_PMAC\n\t\t/*\n\t\t * If this is the first bootable partition, tell the\n\t\t * setup code, in case it wants to make this the root.\n\t\t */\n\t\tif (machine_is(powermac)) {\n\t\t\tint goodness = 0;\n\n\t\t\tmac_fix_string(part->processor, 16);\n\t\t\tmac_fix_string(part->name, 32);\n\t\t\tmac_fix_string(part->type, 32);\t\t\t\t\t\n\t\t    \n\t\t\tif ((be32_to_cpu(part->status) & MAC_STATUS_BOOTABLE)\n\t\t\t    && strcasecmp(part->processor, \"powerpc\") == 0)\n\t\t\t\tgoodness++;\n\n\t\t\tif (strcasecmp(part->type, \"Apple_UNIX_SVR2\") == 0\n\t\t\t    || (strnicmp(part->type, \"Linux\", 5) == 0\n\t\t\t        && strcasecmp(part->type, \"Linux_swap\") != 0)) {\n\t\t\t\tint i, l;\n\n\t\t\t\tgoodness++;\n\t\t\t\tl = strlen(part->name);\n\t\t\t\tif (strcmp(part->name, \"/\") == 0)\n\t\t\t\t\tgoodness++;\n\t\t\t\tfor (i = 0; i <= l - 4; ++i) {\n\t\t\t\t\tif (strnicmp(part->name + i, \"root\",\n\t\t\t\t\t\t     4) == 0) {\n\t\t\t\t\t\tgoodness += 2;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (strnicmp(part->name, \"swap\", 4) == 0)\n\t\t\t\t\tgoodness--;\n\t\t\t}\n\n\t\t\tif (goodness > found_root_goodness) {\n\t\t\t\tfound_root = slot;\n\t\t\t\tfound_root_goodness = goodness;\n\t\t\t}\n\t\t}\n#endif /* CONFIG_PPC_PMAC */\n\t}\n#ifdef CONFIG_PPC_PMAC\n\tif (found_root_goodness)\n\t\tnote_bootable_part(state->bdev->bd_dev, found_root,\n\t\t\t\t   found_root_goodness);\n#endif\n\n\tput_dev_sector(sect);\n\tstrlcat(state->pp_buf, \"\\n\", PAGE_SIZE);\n\treturn 1;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nint mac_partition(struct parsed_partitions *state)\n{\n\tSector sect;\n\tunsigned char *data;\n\tint slot, blocks_in_map;\n\tunsigned secsize, datasize, partoffset;\n#ifdef CONFIG_PPC_PMAC\n\tint found_root = 0;\n\tint found_root_goodness = 0;\n#endif\n\tstruct mac_partition *part;\n\tstruct mac_driver_desc *md;\n\n\t/* Get 0th block and look at the first partition map entry. */\n\tmd = read_part_sector(state, 0, &sect);\n\tif (!md)\n\t\treturn -1;\n\tif (be16_to_cpu(md->signature) != MAC_DRIVER_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\tsecsize = be16_to_cpu(md->block_size);\n\tput_dev_sector(sect);\n\tdatasize = round_down(secsize, 512);\n\tdata = read_part_sector(state, datasize / 512, &sect);\n\tif (!data)\n\t\treturn -1;\n\tpartoffset = secsize % 512;\n\tif (partoffset + sizeof(*part) > datasize)\n\t\treturn -1;\n\tpart = (struct mac_partition *) (data + partoffset);\n\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\t\t/* not a MacOS disk */\n\t}\n\tblocks_in_map = be32_to_cpu(part->map_count);\n\tif (blocks_in_map < 0 || blocks_in_map >= DISK_MAX_PARTS) {\n\t\tput_dev_sector(sect);\n\t\treturn 0;\n\t}\n\tstrlcat(state->pp_buf, \" [mac]\", PAGE_SIZE);\n\tfor (slot = 1; slot <= blocks_in_map; ++slot) {\n\t\tint pos = slot * secsize;\n\t\tput_dev_sector(sect);\n\t\tdata = read_part_sector(state, pos/512, &sect);\n\t\tif (!data)\n\t\t\treturn -1;\n\t\tpart = (struct mac_partition *) (data + pos%512);\n\t\tif (be16_to_cpu(part->signature) != MAC_PARTITION_MAGIC)\n\t\t\tbreak;\n\t\tput_partition(state, slot,\n\t\t\tbe32_to_cpu(part->start_block) * (secsize/512),\n\t\t\tbe32_to_cpu(part->block_count) * (secsize/512));\n\n\t\tif (!strnicmp(part->type, \"Linux_RAID\", 10))\n\t\t\tstate->parts[slot].flags = ADDPART_FLAG_RAID;\n#ifdef CONFIG_PPC_PMAC\n\t\t/*\n\t\t * If this is the first bootable partition, tell the\n\t\t * setup code, in case it wants to make this the root.\n\t\t */\n\t\tif (machine_is(powermac)) {\n\t\t\tint goodness = 0;\n\n\t\t\tmac_fix_string(part->processor, 16);\n\t\t\tmac_fix_string(part->name, 32);\n\t\t\tmac_fix_string(part->type, 32);\t\t\t\t\t\n\t\t    \n\t\t\tif ((be32_to_cpu(part->status) & MAC_STATUS_BOOTABLE)\n\t\t\t    && strcasecmp(part->processor, \"powerpc\") == 0)\n\t\t\t\tgoodness++;\n\n\t\t\tif (strcasecmp(part->type, \"Apple_UNIX_SVR2\") == 0\n\t\t\t    || (strnicmp(part->type, \"Linux\", 5) == 0\n\t\t\t        && strcasecmp(part->type, \"Linux_swap\") != 0)) {\n\t\t\t\tint i, l;\n\n\t\t\t\tgoodness++;\n\t\t\t\tl = strlen(part->name);\n\t\t\t\tif (strcmp(part->name, \"/\") == 0)\n\t\t\t\t\tgoodness++;\n\t\t\t\tfor (i = 0; i <= l - 4; ++i) {\n\t\t\t\t\tif (strnicmp(part->name + i, \"root\",\n\t\t\t\t\t\t     4) == 0) {\n\t\t\t\t\t\tgoodness += 2;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (strnicmp(part->name, \"swap\", 4) == 0)\n\t\t\t\t\tgoodness--;\n\t\t\t}\n\n\t\t\tif (goodness > found_root_goodness) {\n\t\t\t\tfound_root = slot;\n\t\t\t\tfound_root_goodness = goodness;\n\t\t\t}\n\t\t}\n#endif /* CONFIG_PPC_PMAC */\n\t}\n#ifdef CONFIG_PPC_PMAC\n\tif (found_root_goodness)\n\t\tnote_bootable_part(state->bdev->bd_dev, found_root,\n\t\t\t\t   found_root_goodness);\n#endif\n\n\tput_dev_sector(sect);\n\tstrlcat(state->pp_buf, \"\\n\", PAGE_SIZE);\n\treturn 1;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nint dev_pm_opp_get_opp_count(struct device *dev)\n{\n\tstruct opp_table *opp_table;\n\tstruct dev_pm_opp *temp_opp;\n\tint count = 0;\n\n\topp_table = _find_opp_table(dev);\n\tif (IS_ERR(opp_table)) {\n\t\tcount = PTR_ERR(opp_table);\n\t\tdev_err(dev, \"%s: OPP table not found (%d)\\n\",\n\t\t\t__func__, count);\n\t\treturn count;\n\t}\n\n\tmutex_lock(&opp_table->lock);\n\n\tlist_for_each_entry(temp_opp, &opp_table->opp_list, node) {\n\t\tif (temp_opp->available)\n\t\t\tcount++;\n\t}\n\n\tmutex_unlock(&opp_table->lock);\n\tdev_pm_opp_put_opp_table(opp_table);\n\n\treturn count;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nint dev_pm_opp_get_opp_count(struct device *dev)\n{\n\tstruct opp_table *opp_table;\n\tstruct dev_pm_opp *temp_opp;\n\tint count = 0;\n\n\topp_table = _find_opp_table(dev);\n\tif (IS_ERR(opp_table)) {\n\t\tcount = PTR_ERR(opp_table);\n\t\tdev_dbg(dev, \"%s: OPP table not found (%d)\\n\",\n\t\t\t__func__, count);\n\t\treturn count;\n\t}\n\n\tmutex_lock(&opp_table->lock);\n\n\tlist_for_each_entry(temp_opp, &opp_table->opp_list, node) {\n\t\tif (temp_opp->available)\n\t\t\tcount++;\n\t}\n\n\tmutex_unlock(&opp_table->lock);\n\tdev_pm_opp_put_opp_table(opp_table);\n\n\treturn count;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nint dev_pm_opp_get_opp_count(struct device *dev)\n{\n\tstruct opp_table *opp_table;\n\tstruct dev_pm_opp *temp_opp;\n\tint count = 0;\n\n\trcu_read_lock();\n\n\topp_table = _find_opp_table(dev);\n\tif (IS_ERR(opp_table)) {\n\t\tcount = PTR_ERR(opp_table);\n\t\tdev_err(dev, \"%s: OPP table not found (%d)\\n\",\n\t\t\t__func__, count);\n\t\tgoto out_unlock;\n\t}\n\n\tlist_for_each_entry_rcu(temp_opp, &opp_table->opp_list, node) {\n\t\tif (temp_opp->available)\n\t\t\tcount++;\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\treturn count;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nint dev_pm_opp_get_opp_count(struct device *dev)\n{\n\tstruct opp_table *opp_table;\n\tstruct dev_pm_opp *temp_opp;\n\tint count = 0;\n\n\trcu_read_lock();\n\n\topp_table = _find_opp_table(dev);\n\tif (IS_ERR(opp_table)) {\n\t\tcount = PTR_ERR(opp_table);\n\t\tdev_dbg(dev, \"%s: OPP table not found (%d)\\n\",\n\t\t\t__func__, count);\n\t\tgoto out_unlock;\n\t}\n\n\tlist_for_each_entry_rcu(temp_opp, &opp_table->opp_list, node) {\n\t\tif (temp_opp->available)\n\t\t\tcount++;\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\treturn count;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void __vb2_queue_cancel(struct vb2_queue *q)\n{\n\tunsigned int i;\n\n\t/*\n\t * Tell driver to stop all transactions and release all queued\n\t * buffers.\n\t */\n\tif (q->start_streaming_called)\n\t\tcall_void_qop(q, stop_streaming, q);\n\n\t/*\n\t * If you see this warning, then the driver isn't cleaning up properly\n\t * in stop_streaming(). See the stop_streaming() documentation in\n\t * videobuf2-core.h for more information how buffers should be returned\n\t * to vb2 in stop_streaming().\n\t */\n\tif (WARN_ON(atomic_read(&q->owned_by_drv_count))) {\n\t\tfor (i = 0; i < q->num_buffers; ++i)\n\t\t\tif (q->bufs[i]->state == VB2_BUF_STATE_ACTIVE) {\n\t\t\t\tpr_warn(\"driver bug: stop_streaming operation is leaving buf %p in active state\\n\",\n\t\t\t\t\tq->bufs[i]);\n\t\t\t\tvb2_buffer_done(q->bufs[i], VB2_BUF_STATE_ERROR);\n\t\t\t}\n\t\t/* Must be zero now */\n\t\tWARN_ON(atomic_read(&q->owned_by_drv_count));\n\t}\n\n\tq->streaming = 0;\n\tq->start_streaming_called = 0;\n\tq->queued_count = 0;\n\tq->error = 0;\n\n\t/*\n\t * Remove all buffers from videobuf's list...\n\t */\n\tINIT_LIST_HEAD(&q->queued_list);\n\t/*\n\t * ...and done list; userspace will not receive any buffers it\n\t * has not already dequeued before initiating cancel.\n\t */\n\tINIT_LIST_HEAD(&q->done_list);\n\tatomic_set(&q->owned_by_drv_count, 0);\n\twake_up_all(&q->done_wq);\n\n\t/*\n\t * Reinitialize all buffers for next use.\n\t * Make sure to call buf_finish for any queued buffers. Normally\n\t * that's done in dqbuf, but that's not going to happen when we\n\t * cancel the whole queue. Note: this code belongs here, not in\n\t * __vb2_dqbuf() since in vb2_core_dqbuf() there is a critical\n\t * call to __fill_user_buffer() after buf_finish(). That order can't\n\t * be changed, so we can't move the buf_finish() to __vb2_dqbuf().\n\t */\n\tfor (i = 0; i < q->num_buffers; ++i) {\n\t\tstruct vb2_buffer *vb = q->bufs[i];\n\n\t\tif (vb->state != VB2_BUF_STATE_DEQUEUED) {\n\t\t\tvb->state = VB2_BUF_STATE_PREPARED;\n\t\t\tcall_void_vb_qop(vb, buf_finish, vb);\n\t\t}\n\t\t__vb2_dqbuf(vb);\n\t}\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void __vb2_queue_cancel(struct vb2_queue *q)\n{\n\tunsigned int i;\n\n\t/*\n\t * Tell driver to stop all transactions and release all queued\n\t * buffers.\n\t */\n\tif (q->start_streaming_called)\n\t\tcall_void_qop(q, stop_streaming, q);\n\n\t/*\n\t * If you see this warning, then the driver isn't cleaning up properly\n\t * in stop_streaming(). See the stop_streaming() documentation in\n\t * videobuf2-core.h for more information how buffers should be returned\n\t * to vb2 in stop_streaming().\n\t */\n\tif (WARN_ON(atomic_read(&q->owned_by_drv_count))) {\n\t\tfor (i = 0; i < q->num_buffers; ++i)\n\t\t\tif (q->bufs[i]->state == VB2_BUF_STATE_ACTIVE) {\n\t\t\t\tpr_warn(\"driver bug: stop_streaming operation is leaving buf %p in active state\\n\",\n\t\t\t\t\tq->bufs[i]);\n\t\t\t\tvb2_buffer_done(q->bufs[i], VB2_BUF_STATE_ERROR);\n\t\t\t}\n\t\t/* Must be zero now */\n\t\tWARN_ON(atomic_read(&q->owned_by_drv_count));\n\t}\n\n\tq->streaming = 0;\n\tq->start_streaming_called = 0;\n\tq->queued_count = 0;\n\tq->error = 0;\n\n\t/*\n\t * Remove all buffers from videobuf's list...\n\t */\n\tINIT_LIST_HEAD(&q->queued_list);\n\t/*\n\t * ...and done list; userspace will not receive any buffers it\n\t * has not already dequeued before initiating cancel.\n\t */\n\tINIT_LIST_HEAD(&q->done_list);\n\tatomic_set(&q->owned_by_drv_count, 0);\n\twake_up_all(&q->done_wq);\n\n\t/*\n\t * Reinitialize all buffers for next use.\n\t * Make sure to call buf_finish for any queued buffers. Normally\n\t * that's done in dqbuf, but that's not going to happen when we\n\t * cancel the whole queue. Note: this code belongs here, not in\n\t * __vb2_dqbuf() since in vb2_core_dqbuf() there is a critical\n\t * call to __fill_user_buffer() after buf_finish(). That order can't\n\t * be changed, so we can't move the buf_finish() to __vb2_dqbuf().\n\t */\n\tfor (i = 0; i < q->num_buffers; ++i) {\n\t\tstruct vb2_buffer *vb = q->bufs[i];\n\n\t\tif (vb->state == VB2_BUF_STATE_PREPARED ||\n\t\t    vb->state == VB2_BUF_STATE_QUEUED) {\n\t\t\tunsigned int plane;\n\n\t\t\tfor (plane = 0; plane < vb->num_planes; ++plane)\n\t\t\t\tcall_void_memop(vb, finish,\n\t\t\t\t\t\tvb->planes[plane].mem_priv);\n\t\t}\n\n\t\tif (vb->state != VB2_BUF_STATE_DEQUEUED) {\n\t\t\tvb->state = VB2_BUF_STATE_PREPARED;\n\t\t\tcall_void_vb_qop(vb, buf_finish, vb);\n\t\t}\n\t\t__vb2_dqbuf(vb);\n\t}\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void __vb2_queue_cancel(struct vb2_queue *q)\n{\n\tunsigned int i;\n\n\t/*\n\t * Tell driver to stop all transactions and release all queued\n\t * buffers.\n\t */\n\tif (q->start_streaming_called)\n\t\tcall_void_qop(q, stop_streaming, q);\n\n\t/*\n\t * If you see this warning, then the driver isn't cleaning up properly\n\t * in stop_streaming(). See the stop_streaming() documentation in\n\t * videobuf2-core.h for more information how buffers should be returned\n\t * to vb2 in stop_streaming().\n\t */\n\tif (WARN_ON(atomic_read(&q->owned_by_drv_count))) {\n\t\tfor (i = 0; i < q->num_buffers; ++i)\n\t\t\tif (q->bufs[i]->state == VB2_BUF_STATE_ACTIVE)\n\t\t\t\tvb2_buffer_done(q->bufs[i], VB2_BUF_STATE_ERROR);\n\t\t/* Must be zero now */\n\t\tWARN_ON(atomic_read(&q->owned_by_drv_count));\n\t}\n\n\tq->streaming = 0;\n\tq->start_streaming_called = 0;\n\tq->queued_count = 0;\n\tq->error = 0;\n\n\t/*\n\t * Remove all buffers from videobuf's list...\n\t */\n\tINIT_LIST_HEAD(&q->queued_list);\n\t/*\n\t * ...and done list; userspace will not receive any buffers it\n\t * has not already dequeued before initiating cancel.\n\t */\n\tINIT_LIST_HEAD(&q->done_list);\n\tatomic_set(&q->owned_by_drv_count, 0);\n\twake_up_all(&q->done_wq);\n\n\t/*\n\t * Reinitialize all buffers for next use.\n\t * Make sure to call buf_finish for any queued buffers. Normally\n\t * that's done in dqbuf, but that's not going to happen when we\n\t * cancel the whole queue. Note: this code belongs here, not in\n\t * __vb2_dqbuf() since in vb2_core_dqbuf() there is a critical\n\t * call to __fill_user_buffer() after buf_finish(). That order can't\n\t * be changed, so we can't move the buf_finish() to __vb2_dqbuf().\n\t */\n\tfor (i = 0; i < q->num_buffers; ++i) {\n\t\tstruct vb2_buffer *vb = q->bufs[i];\n\n\t\tif (vb->state != VB2_BUF_STATE_DEQUEUED) {\n\t\t\tvb->state = VB2_BUF_STATE_PREPARED;\n\t\t\tcall_void_vb_qop(vb, buf_finish, vb);\n\t\t}\n\t\t__vb2_dqbuf(vb);\n\t}\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void __vb2_queue_cancel(struct vb2_queue *q)\n{\n\tunsigned int i;\n\n\t/*\n\t * Tell driver to stop all transactions and release all queued\n\t * buffers.\n\t */\n\tif (q->start_streaming_called)\n\t\tcall_void_qop(q, stop_streaming, q);\n\n\t/*\n\t * If you see this warning, then the driver isn't cleaning up properly\n\t * in stop_streaming(). See the stop_streaming() documentation in\n\t * videobuf2-core.h for more information how buffers should be returned\n\t * to vb2 in stop_streaming().\n\t */\n\tif (WARN_ON(atomic_read(&q->owned_by_drv_count))) {\n\t\tfor (i = 0; i < q->num_buffers; ++i)\n\t\t\tif (q->bufs[i]->state == VB2_BUF_STATE_ACTIVE)\n\t\t\t\tvb2_buffer_done(q->bufs[i], VB2_BUF_STATE_ERROR);\n\t\t/* Must be zero now */\n\t\tWARN_ON(atomic_read(&q->owned_by_drv_count));\n\t}\n\n\tq->streaming = 0;\n\tq->start_streaming_called = 0;\n\tq->queued_count = 0;\n\tq->error = 0;\n\n\t/*\n\t * Remove all buffers from videobuf's list...\n\t */\n\tINIT_LIST_HEAD(&q->queued_list);\n\t/*\n\t * ...and done list; userspace will not receive any buffers it\n\t * has not already dequeued before initiating cancel.\n\t */\n\tINIT_LIST_HEAD(&q->done_list);\n\tatomic_set(&q->owned_by_drv_count, 0);\n\twake_up_all(&q->done_wq);\n\n\t/*\n\t * Reinitialize all buffers for next use.\n\t * Make sure to call buf_finish for any queued buffers. Normally\n\t * that's done in dqbuf, but that's not going to happen when we\n\t * cancel the whole queue. Note: this code belongs here, not in\n\t * __vb2_dqbuf() since in vb2_core_dqbuf() there is a critical\n\t * call to __fill_user_buffer() after buf_finish(). That order can't\n\t * be changed, so we can't move the buf_finish() to __vb2_dqbuf().\n\t */\n\tfor (i = 0; i < q->num_buffers; ++i) {\n\t\tstruct vb2_buffer *vb = q->bufs[i];\n\n\t\tif (vb->state == VB2_BUF_STATE_PREPARED ||\n\t\t    vb->state == VB2_BUF_STATE_QUEUED) {\n\t\t\tunsigned int plane;\n\n\t\t\tfor (plane = 0; plane < vb->num_planes; ++plane)\n\t\t\t\tcall_void_memop(vb, finish,\n\t\t\t\t\t\tvb->planes[plane].mem_priv);\n\t\t}\n\n\t\tif (vb->state != VB2_BUF_STATE_DEQUEUED) {\n\t\t\tvb->state = VB2_BUF_STATE_PREPARED;\n\t\t\tcall_void_vb_qop(vb, buf_finish, vb);\n\t\t}\n\t\t__vb2_dqbuf(vb);\n\t}\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\n__u32\t     reserved[4];\n'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\n__u32\t     capabilities;\n\n\n__u32\t     reserved[3];\n'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\n__u32\t     reserved[4];\n'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\n__u32\t     capabilities;\n\n\n__u32\t     reserved[3];\n'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void __vb2_queue_cancel(struct vb2_queue *q)\n{\n\tunsigned int i;\n\n\t/*\n\t * Tell driver to stop all transactions and release all queued\n\t * buffers.\n\t */\n\tif (q->start_streaming_called)\n\t\tcall_void_qop(q, stop_streaming, q);\n\n\t/*\n\t * If you see this warning, then the driver isn't cleaning up properly\n\t * in stop_streaming(). See the stop_streaming() documentation in\n\t * videobuf2-core.h for more information how buffers should be returned\n\t * to vb2 in stop_streaming().\n\t */\n\tif (WARN_ON(atomic_read(&q->owned_by_drv_count))) {\n\t\tfor (i = 0; i < q->num_buffers; ++i)\n\t\t\tif (q->bufs[i]->state == VB2_BUF_STATE_ACTIVE) {\n\t\t\t\tpr_warn(\"driver bug: stop_streaming operation is leaving buf %p in active state\\n\",\n\t\t\t\t\tq->bufs[i]);\n\t\t\t\tvb2_buffer_done(q->bufs[i], VB2_BUF_STATE_ERROR);\n\t\t\t}\n\t\t/* Must be zero now */\n\t\tWARN_ON(atomic_read(&q->owned_by_drv_count));\n\t}\n\n\tq->streaming = 0;\n\tq->start_streaming_called = 0;\n\tq->queued_count = 0;\n\tq->error = 0;\n\tq->uses_requests = 0;\n\tq->uses_qbuf = 0;\n\n\t/*\n\t * Remove all buffers from videobuf's list...\n\t */\n\tINIT_LIST_HEAD(&q->queued_list);\n\t/*\n\t * ...and done list; userspace will not receive any buffers it\n\t * has not already dequeued before initiating cancel.\n\t */\n\tINIT_LIST_HEAD(&q->done_list);\n\tatomic_set(&q->owned_by_drv_count, 0);\n\twake_up_all(&q->done_wq);\n\n\t/*\n\t * Reinitialize all buffers for next use.\n\t * Make sure to call buf_finish for any queued buffers. Normally\n\t * that's done in dqbuf, but that's not going to happen when we\n\t * cancel the whole queue. Note: this code belongs here, not in\n\t * __vb2_dqbuf() since in vb2_core_dqbuf() there is a critical\n\t * call to __fill_user_buffer() after buf_finish(). That order can't\n\t * be changed, so we can't move the buf_finish() to __vb2_dqbuf().\n\t */\n\tfor (i = 0; i < q->num_buffers; ++i) {\n\t\tstruct vb2_buffer *vb = q->bufs[i];\n\t\tstruct media_request *req = vb->req_obj.req;\n\n\t\t/*\n\t\t * If a request is associated with this buffer, then\n\t\t * call buf_request_cancel() to give the driver to complete()\n\t\t * related request objects. Otherwise those objects would\n\t\t * never complete.\n\t\t */\n\t\tif (req) {\n\t\t\tenum media_request_state state;\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&req->lock, flags);\n\t\t\tstate = req->state;\n\t\t\tspin_unlock_irqrestore(&req->lock, flags);\n\n\t\t\tif (state == MEDIA_REQUEST_STATE_QUEUED)\n\t\t\t\tcall_void_vb_qop(vb, buf_request_complete, vb);\n\t\t}\n\n\t\tif (vb->synced) {\n\t\t\tunsigned int plane;\n\n\t\t\tfor (plane = 0; plane < vb->num_planes; ++plane)\n\t\t\t\tcall_void_memop(vb, finish,\n\t\t\t\t\t\tvb->planes[plane].mem_priv);\n\t\t\tvb->synced = false;\n\t\t}\n\n\t\tif (vb->prepared) {\n\t\t\tcall_void_vb_qop(vb, buf_finish, vb);\n\t\t\tvb->prepared = false;\n\t\t}\n\t\t__vb2_dqbuf(vb);\n\t}\n}\n\nint vb2_core_streamon(struct vb2_queue *q, unsigned int type)\n{\n\tint ret;\n\n\tif (type != q->type) {\n\t\tdprintk(1, \"invalid stream type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->streaming) {\n\t\tdprintk(3, \"already streaming\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!q->num_buffers) {\n\t\tdprintk(1, \"no buffers have been allocated\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->num_buffers < q->min_buffers_needed) {\n\t\tdprintk(1, \"need at least %u allocated buffers\\n\",\n\t\t\t\tq->min_buffers_needed);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Tell driver to start streaming provided sufficient buffers\n\t * are available.\n\t */\n\tif (q->queued_count >= q->min_buffers_needed) {\n\t\tret = v4l_vb2q_enable_media_source(q);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = vb2_start_streaming(q);\n\t\tif (ret) {\n\t\t\t__vb2_queue_cancel(q);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tq->streaming = 1;\n\n\tdprintk(3, \"successful\\n\");\n\treturn 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nint vb2_core_streamon(struct vb2_queue *q, unsigned int type)\n{\n\tint ret;\n\n\tif (type != q->type) {\n\t\tdprintk(1, \"invalid stream type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->streaming) {\n\t\tdprintk(3, \"already streaming\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!q->num_buffers) {\n\t\tdprintk(1, \"no buffers have been allocated\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->num_buffers < q->min_buffers_needed) {\n\t\tdprintk(1, \"need at least %u allocated buffers\\n\",\n\t\t\t\tq->min_buffers_needed);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Tell driver to start streaming provided sufficient buffers\n\t * are available.\n\t */\n\tif (q->queued_count >= q->min_buffers_needed) {\n\t\tret = v4l_vb2q_enable_media_source(q);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = vb2_start_streaming(q);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tq->streaming = 1;\n\n\tdprintk(3, \"successful\\n\");\n\treturn 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void __vb2_queue_cancel(struct vb2_queue *q)\n{\n\tunsigned int i;\n\n\t/*\n\t * Tell driver to stop all transactions and release all queued\n\t * buffers.\n\t */\n\tif (q->start_streaming_called)\n\t\tcall_void_qop(q, stop_streaming, q);\n\n\tif (WARN_ON(atomic_read(&q->owned_by_drv_count))) {\n\t\tfor (i = 0; i < q->num_buffers; ++i)\n\t\t\tif (q->bufs[i]->state == VB2_BUF_STATE_ACTIVE)\n\t\t\t\tvb2_buffer_done(q->bufs[i], VB2_BUF_STATE_ERROR);\n\t\t/* Must be zero now */\n\t\tWARN_ON(atomic_read(&q->owned_by_drv_count));\n\t}\n\n\tq->streaming = 0;\n\tq->start_streaming_called = 0;\n\tq->queued_count = 0;\n\n\t/*\n\t * Remove all buffers from videobuf's list...\n\t */\n\tINIT_LIST_HEAD(&q->queued_list);\n\t/*\n\t * ...and done list; userspace will not receive any buffers it\n\t * has not already dequeued before initiating cancel.\n\t */\n\tINIT_LIST_HEAD(&q->done_list);\n\tatomic_set(&q->owned_by_drv_count, 0);\n\twake_up_all(&q->done_wq);\n\n\t/*\n\t * Reinitialize all buffers for next use.\n\t * Make sure to call buf_finish for any queued buffers. Normally\n\t * that's done in dqbuf, but that's not going to happen when we\n\t * cancel the whole queue. Note: this code belongs here, not in\n\t * __vb2_dqbuf() since in vb2_internal_dqbuf() there is a critical\n\t * call to __fill_v4l2_buffer() after buf_finish(). That order can't\n\t * be changed, so we can't move the buf_finish() to __vb2_dqbuf().\n\t */\n\tfor (i = 0; i < q->num_buffers; ++i) {\n\t\tstruct vb2_buffer *vb = q->bufs[i];\n\n\t\tif (vb->state != VB2_BUF_STATE_DEQUEUED) {\n\t\t\tvb->state = VB2_BUF_STATE_PREPARED;\n\t\t\tcall_void_vb_qop(vb, buf_finish, vb);\n\t\t}\n\t\t__vb2_dqbuf(vb);\n\t}\n}\n\nstatic int vb2_internal_streamon(struct vb2_queue *q, enum v4l2_buf_type type)\n{\n\tint ret;\n\n\tif (type != q->type) {\n\t\tdprintk(1, \"invalid stream type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->streaming) {\n\t\tdprintk(3, \"already streaming\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!q->num_buffers) {\n\t\tdprintk(1, \"no buffers have been allocated\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->num_buffers < q->min_buffers_needed) {\n\t\tdprintk(1, \"need at least %u allocated buffers\\n\",\n\t\t\t\tq->min_buffers_needed);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Tell driver to start streaming provided sufficient buffers\n\t * are available.\n\t */\n\tif (q->queued_count >= q->min_buffers_needed) {\n\t\tret = vb2_start_streaming(q);\n\t\tif (ret) {\n\t\t\t__vb2_queue_cancel(q);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tq->streaming = 1;\n\n\tdprintk(3, \"successful\\n\");\n\treturn 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int vb2_internal_streamon(struct vb2_queue *q, enum v4l2_buf_type type)\n{\n\tint ret;\n\n\tif (type != q->type) {\n\t\tdprintk(1, \"invalid stream type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->streaming) {\n\t\tdprintk(3, \"already streaming\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!q->num_buffers) {\n\t\tdprintk(1, \"no buffers have been allocated\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (q->num_buffers < q->min_buffers_needed) {\n\t\tdprintk(1, \"need at least %u allocated buffers\\n\",\n\t\t\t\tq->min_buffers_needed);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Tell driver to start streaming provided sufficient buffers\n\t * are available.\n\t */\n\tif (q->queued_count >= q->min_buffers_needed) {\n\t\tret = vb2_start_streaming(q);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tq->streaming = 1;\n\n\tdprintk(3, \"successful\\n\");\n\treturn 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\n#define DMA_MAX_SIZE\t\t(0x10000 - PAGE_SIZE)\n'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\n#define DMA_MAX_SIZE\t\t(0x10000 - 512)\n'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\n#define DMA_MAX_SIZE\t\t(0x10000 - PAGE_SIZE)\n'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\n#define DMA_MAX_SIZE\t\t(0x10000 - 512)\n'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void i2c_dw_xfer_init(struct dw_i2c_dev *dev)\n{\n\tstruct i2c_msg *msgs = dev->msgs;\n\tu32 ic_con, ic_tar = 0;\n\n\t/* Disable the adapter */\n\t__i2c_dw_enable_and_wait(dev, false);\n\n\t/* If the slave address is ten bit address, enable 10BITADDR */\n\tic_con = dw_readl(dev, DW_IC_CON);\n\tif (msgs[dev->msg_write_idx].flags & I2C_M_TEN) {\n\t\tic_con |= DW_IC_CON_10BITADDR_MASTER;\n\t\t/*\n\t\t * If I2C_DYNAMIC_TAR_UPDATE is set, the 10-bit addressing\n\t\t * mode has to be enabled via bit 12 of IC_TAR register.\n\t\t * We set it always as I2C_DYNAMIC_TAR_UPDATE can't be\n\t\t * detected from registers.\n\t\t */\n\t\tic_tar = DW_IC_TAR_10BITADDR_MASTER;\n\t} else {\n\t\tic_con &= ~DW_IC_CON_10BITADDR_MASTER;\n\t}\n\n\tdw_writel(dev, ic_con, DW_IC_CON);\n\n\t/*\n\t * Set the slave (target) address and enable 10-bit addressing mode\n\t * if applicable.\n\t */\n\tdw_writel(dev, msgs[dev->msg_write_idx].addr | ic_tar, DW_IC_TAR);\n\n\t/* Enforce disabled interrupts (due to HW issues) */\n\ti2c_dw_disable_int(dev);\n\n\t/* Enable the adapter */\n\t__i2c_dw_enable_and_wait(dev, true);\n\n\t/* Clear and enable interrupts */\n\tdw_readl(dev, DW_IC_CLR_INTR);\n\tdw_writel(dev, DW_IC_INTR_MASTER_MASK, DW_IC_INTR_MASK);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void i2c_dw_xfer_init(struct dw_i2c_dev *dev)\n{\n\tstruct i2c_msg *msgs = dev->msgs;\n\tu32 ic_con, ic_tar = 0;\n\n\t/* Disable the adapter */\n\t__i2c_dw_enable_and_wait(dev, false);\n\n\t/* If the slave address is ten bit address, enable 10BITADDR */\n\tic_con = dw_readl(dev, DW_IC_CON);\n\tif (msgs[dev->msg_write_idx].flags & I2C_M_TEN) {\n\t\tic_con |= DW_IC_CON_10BITADDR_MASTER;\n\t\t/*\n\t\t * If I2C_DYNAMIC_TAR_UPDATE is set, the 10-bit addressing\n\t\t * mode has to be enabled via bit 12 of IC_TAR register.\n\t\t * We set it always as I2C_DYNAMIC_TAR_UPDATE can't be\n\t\t * detected from registers.\n\t\t */\n\t\tic_tar = DW_IC_TAR_10BITADDR_MASTER;\n\t} else {\n\t\tic_con &= ~DW_IC_CON_10BITADDR_MASTER;\n\t}\n\n\tdw_writel(dev, ic_con, DW_IC_CON);\n\n\t/*\n\t * Set the slave (target) address and enable 10-bit addressing mode\n\t * if applicable.\n\t */\n\tdw_writel(dev, msgs[dev->msg_write_idx].addr | ic_tar, DW_IC_TAR);\n\n\t/* Enforce disabled interrupts (due to HW issues) */\n\ti2c_dw_disable_int(dev);\n\n\t/* Enable the adapter */\n\t__i2c_dw_enable(dev, true);\n\n\t/* Dummy read to avoid the register getting stuck on Bay Trail */\n\tdw_readl(dev, DW_IC_ENABLE_STATUS);\n\n\t/* Clear and enable interrupts */\n\tdw_readl(dev, DW_IC_CLR_INTR);\n\tdw_writel(dev, DW_IC_INTR_MASTER_MASK, DW_IC_INTR_MASK);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void i2c_dw_xfer_init(struct dw_i2c_dev *dev)\n{\n\tstruct i2c_msg *msgs = dev->msgs;\n\tu32 ic_con, ic_tar = 0;\n\n\t/* Disable the adapter */\n\t__i2c_dw_enable_and_wait(dev, false);\n\n\t/* if the slave address is ten bit address, enable 10BITADDR */\n\tic_con = dw_readl(dev, DW_IC_CON);\n\tif (msgs[dev->msg_write_idx].flags & I2C_M_TEN) {\n\t\tic_con |= DW_IC_CON_10BITADDR_MASTER;\n\t\t/*\n\t\t * If I2C_DYNAMIC_TAR_UPDATE is set, the 10-bit addressing\n\t\t * mode has to be enabled via bit 12 of IC_TAR register.\n\t\t * We set it always as I2C_DYNAMIC_TAR_UPDATE can't be\n\t\t * detected from registers.\n\t\t */\n\t\tic_tar = DW_IC_TAR_10BITADDR_MASTER;\n\t} else {\n\t\tic_con &= ~DW_IC_CON_10BITADDR_MASTER;\n\t}\n\n\tdw_writel(dev, ic_con, DW_IC_CON);\n\n\t/*\n\t * Set the slave (target) address and enable 10-bit addressing mode\n\t * if applicable.\n\t */\n\tdw_writel(dev, msgs[dev->msg_write_idx].addr | ic_tar, DW_IC_TAR);\n\n\t/* enforce disabled interrupts (due to HW issues) */\n\ti2c_dw_disable_int(dev);\n\n\t/* Enable the adapter */\n\t__i2c_dw_enable_and_wait(dev, true);\n\n\t/* Clear and enable interrupts */\n\tdw_readl(dev, DW_IC_CLR_INTR);\n\tdw_writel(dev, DW_IC_INTR_DEFAULT_MASK, DW_IC_INTR_MASK);\n}\n\nstatic void __i2c_dw_enable_and_wait(struct dw_i2c_dev *dev, bool enable)\n{\n\tint timeout = 100;\n\n\tdo {\n\t\t__i2c_dw_enable(dev, enable);\n\t\tif ((dw_readl(dev, DW_IC_ENABLE_STATUS) & 1) == enable)\n\t\t\treturn;\n\n\t\t/*\n\t\t * Wait 10 times the signaling period of the highest I2C\n\t\t * transfer supported by the driver (for 400KHz this is\n\t\t * 25us) as described in the DesignWare I2C databook.\n\t\t */\n\t\tusleep_range(25, 250);\n\t} while (timeout--);\n\n\tdev_warn(dev->dev, \"timeout in %sabling adapter\\n\",\n\t\t enable ? \"en\" : \"dis\");\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void i2c_dw_xfer_init(struct dw_i2c_dev *dev)\n{\n\tstruct i2c_msg *msgs = dev->msgs;\n\tu32 ic_con, ic_tar = 0;\n\n\t/* Disable the adapter */\n\t__i2c_dw_enable_and_wait(dev, false);\n\n\t/* if the slave address is ten bit address, enable 10BITADDR */\n\tic_con = dw_readl(dev, DW_IC_CON);\n\tif (msgs[dev->msg_write_idx].flags & I2C_M_TEN) {\n\t\tic_con |= DW_IC_CON_10BITADDR_MASTER;\n\t\t/*\n\t\t * If I2C_DYNAMIC_TAR_UPDATE is set, the 10-bit addressing\n\t\t * mode has to be enabled via bit 12 of IC_TAR register.\n\t\t * We set it always as I2C_DYNAMIC_TAR_UPDATE can't be\n\t\t * detected from registers.\n\t\t */\n\t\tic_tar = DW_IC_TAR_10BITADDR_MASTER;\n\t} else {\n\t\tic_con &= ~DW_IC_CON_10BITADDR_MASTER;\n\t}\n\n\tdw_writel(dev, ic_con, DW_IC_CON);\n\n\t/*\n\t * Set the slave (target) address and enable 10-bit addressing mode\n\t * if applicable.\n\t */\n\tdw_writel(dev, msgs[dev->msg_write_idx].addr | ic_tar, DW_IC_TAR);\n\n\t/* enforce disabled interrupts (due to HW issues) */\n\ti2c_dw_disable_int(dev);\n\n\t/* Enable the adapter */\n\t__i2c_dw_enable(dev, true);\n\n\t/* Dummy read to avoid the register getting stuck on Bay Trail */\n\tdw_readl(dev, DW_IC_ENABLE_STATUS);\n\n\t/* Clear and enable interrupts */\n\tdw_readl(dev, DW_IC_CLR_INTR);\n\tdw_writel(dev, DW_IC_INTR_DEFAULT_MASK, DW_IC_INTR_MASK);\n}\n\nstatic u32 dw_readl(struct dw_i2c_dev *dev, int offset)\n{\n\tu32 value;\n\n\tif (dev->accessor_flags & ACCESS_16BIT)\n\t\tvalue = readw_relaxed(dev->base + offset) |\n\t\t\t(readw_relaxed(dev->base + offset + 2) << 16);\n\telse\n\t\tvalue = readl_relaxed(dev->base + offset);\n\n\tif (dev->accessor_flags & ACCESS_SWAP)\n\t\treturn swab32(value);\n\telse\n\t\treturn value;\n}\n\nstatic void __i2c_dw_enable(struct dw_i2c_dev *dev, bool enable)\n{\n\tdw_writel(dev, enable, DW_IC_ENABLE);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nint rtl88e_init_sw_vars(struct ieee80211_hw *hw)\n{\n\tint err = 0;\n\tstruct rtl_priv *rtlpriv = rtl_priv(hw);\n\tstruct rtl_pci *rtlpci = rtl_pcidev(rtl_pcipriv(hw));\n\tu8 tid;\n\n\trtl8188ee_bt_reg_init(hw);\n\trtlpci->msi_support = rtlpriv->cfg->mod_params->msi_support;\n\n\trtlpriv->dm.dm_initialgain_enable = 1;\n\trtlpriv->dm.dm_flag = 0;\n\trtlpriv->dm.disable_framebursting = 0;\n\trtlpriv->dm.thermalvalue = 0;\n\trtlpci->transmit_config = CFENDFORM | BIT(15);\n\n\t/* compatible 5G band 88ce just 2.4G band & smsp */\n\trtlpriv->rtlhal.current_bandtype = BAND_ON_2_4G;\n\trtlpriv->rtlhal.bandset = BAND_ON_2_4G;\n\trtlpriv->rtlhal.macphymode = SINGLEMAC_SINGLEPHY;\n\n\trtlpci->receive_config = (RCR_APPFCS |\n\t\t\t\t  RCR_APP_MIC |\n\t\t\t\t  RCR_APP_ICV |\n\t\t\t\t  RCR_APP_PHYST_RXFF |\n\t\t\t\t  RCR_HTC_LOC_CTRL |\n\t\t\t\t  RCR_AMF |\n\t\t\t\t  RCR_ACF |\n\t\t\t\t  RCR_ADF |\n\t\t\t\t  RCR_AICV |\n\t\t\t\t  RCR_ACRC32 |\n\t\t\t\t  RCR_AB |\n\t\t\t\t  RCR_AM |\n\t\t\t\t  RCR_APM |\n\t\t\t\t  0);\n\n\trtlpci->irq_mask[0] =\n\t\t\t\t(u32)(IMR_PSTIMEOUT\t|\n\t\t\t\tIMR_HSISR_IND_ON_INT\t|\n\t\t\t\tIMR_C2HCMD\t\t|\n\t\t\t\tIMR_HIGHDOK\t\t|\n\t\t\t\tIMR_MGNTDOK\t\t|\n\t\t\t\tIMR_BKDOK\t\t|\n\t\t\t\tIMR_BEDOK\t\t|\n\t\t\t\tIMR_VIDOK\t\t|\n\t\t\t\tIMR_VODOK\t\t|\n\t\t\t\tIMR_RDU\t\t\t|\n\t\t\t\tIMR_ROK\t\t\t|\n\t\t\t\t0);\n\trtlpci->irq_mask[1] = (u32) (IMR_RXFOVW | 0);\n\trtlpci->sys_irq_mask = (u32) (HSIMR_PDN_INT_EN | HSIMR_RON_INT_EN);\n\n\t/* for debug level */\n\trtlpriv->dbg.global_debuglevel = rtlpriv->cfg->mod_params->debug;\n\t/* for LPS & IPS */\n\trtlpriv->psc.inactiveps = rtlpriv->cfg->mod_params->inactiveps;\n\trtlpriv->psc.swctrl_lps = rtlpriv->cfg->mod_params->swctrl_lps;\n\trtlpriv->psc.fwctrl_lps = rtlpriv->cfg->mod_params->fwctrl_lps;\n\tif (rtlpriv->cfg->mod_params->disable_watchdog)\n\t\tpr_info(\"watchdog disabled\\n\");\n\tif (!rtlpriv->psc.inactiveps)\n\t\tpr_info(\"rtl8188ee: Power Save off (module option)\\n\");\n\tif (!rtlpriv->psc.fwctrl_lps)\n\t\tpr_info(\"rtl8188ee: FW Power Save off (module option)\\n\");\n\trtlpriv->psc.reg_fwctrl_lps = 3;\n\trtlpriv->psc.reg_max_lps_awakeintvl = 5;\n\t/* for ASPM, you can close aspm through\n\t * set const_support_pciaspm = 0\n\t */\n\trtl88e_init_aspm_vars(hw);\n\n\tif (rtlpriv->psc.reg_fwctrl_lps == 1)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MIN_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 2)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MAX_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 3)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_DTIM_MODE;\n\n\t/* for firmware buf */\n\trtlpriv->rtlhal.pfirmware = vzalloc(0x8000);\n\tif (!rtlpriv->rtlhal.pfirmware) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Can't alloc buffer for fw.\\n\");\n\t\treturn 1;\n\t}\n\n\trtlpriv->cfg->fw_name = \"rtlwifi/rtl8188efw.bin\";\n\trtlpriv->max_fw_size = 0x8000;\n\tpr_info(\"Using firmware %s\\n\", rtlpriv->cfg->fw_name);\n\terr = request_firmware_nowait(THIS_MODULE, 1, rtlpriv->cfg->fw_name,\n\t\t\t\t      rtlpriv->io.dev, GFP_KERNEL, hw,\n\t\t\t\t      rtl_fw_cb);\n\tif (err) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Failed to request firmware!\\n\");\n\t\treturn 1;\n\t}\n\n\t/* for early mode */\n\trtlpriv->rtlhal.earlymode_enable = false;\n\trtlpriv->rtlhal.max_earlymode_num = 10;\n\tfor (tid = 0; tid < 8; tid++)\n\t\tskb_queue_head_init(&rtlpriv->mac80211.skb_waitq[tid]);\n\n\t/*low power */\n\trtlpriv->psc.low_power_enable = false;\n\tif (rtlpriv->psc.low_power_enable) {\n\t\tinit_timer(&rtlpriv->works.fw_clockoff_timer);\n\t\tsetup_timer(&rtlpriv->works.fw_clockoff_timer,\n\t\t\t    rtl88ee_fw_clk_off_timer_callback,\n\t\t\t    (unsigned long)hw);\n\t}\n\n\tinit_timer(&rtlpriv->works.fast_antenna_training_timer);\n\tsetup_timer(&rtlpriv->works.fast_antenna_training_timer,\n\t\t    rtl88e_dm_fast_antenna_training_callback,\n\t\t\t(unsigned long)hw);\n\treturn err;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nint rtl88e_init_sw_vars(struct ieee80211_hw *hw)\n{\n\tint err = 0;\n\tstruct rtl_priv *rtlpriv = rtl_priv(hw);\n\tstruct rtl_pci *rtlpci = rtl_pcidev(rtl_pcipriv(hw));\n\tu8 tid;\n\n\trtl8188ee_bt_reg_init(hw);\n\trtlpriv->dm.dm_initialgain_enable = 1;\n\trtlpriv->dm.dm_flag = 0;\n\trtlpriv->dm.disable_framebursting = 0;\n\trtlpriv->dm.thermalvalue = 0;\n\trtlpci->transmit_config = CFENDFORM | BIT(15);\n\n\t/* compatible 5G band 88ce just 2.4G band & smsp */\n\trtlpriv->rtlhal.current_bandtype = BAND_ON_2_4G;\n\trtlpriv->rtlhal.bandset = BAND_ON_2_4G;\n\trtlpriv->rtlhal.macphymode = SINGLEMAC_SINGLEPHY;\n\n\trtlpci->receive_config = (RCR_APPFCS |\n\t\t\t\t  RCR_APP_MIC |\n\t\t\t\t  RCR_APP_ICV |\n\t\t\t\t  RCR_APP_PHYST_RXFF |\n\t\t\t\t  RCR_HTC_LOC_CTRL |\n\t\t\t\t  RCR_AMF |\n\t\t\t\t  RCR_ACF |\n\t\t\t\t  RCR_ADF |\n\t\t\t\t  RCR_AICV |\n\t\t\t\t  RCR_ACRC32 |\n\t\t\t\t  RCR_AB |\n\t\t\t\t  RCR_AM |\n\t\t\t\t  RCR_APM |\n\t\t\t\t  0);\n\n\trtlpci->irq_mask[0] =\n\t\t\t\t(u32)(IMR_PSTIMEOUT\t|\n\t\t\t\tIMR_HSISR_IND_ON_INT\t|\n\t\t\t\tIMR_C2HCMD\t\t|\n\t\t\t\tIMR_HIGHDOK\t\t|\n\t\t\t\tIMR_MGNTDOK\t\t|\n\t\t\t\tIMR_BKDOK\t\t|\n\t\t\t\tIMR_BEDOK\t\t|\n\t\t\t\tIMR_VIDOK\t\t|\n\t\t\t\tIMR_VODOK\t\t|\n\t\t\t\tIMR_RDU\t\t\t|\n\t\t\t\tIMR_ROK\t\t\t|\n\t\t\t\t0);\n\trtlpci->irq_mask[1] = (u32) (IMR_RXFOVW | 0);\n\trtlpci->sys_irq_mask = (u32) (HSIMR_PDN_INT_EN | HSIMR_RON_INT_EN);\n\n\t/* for debug level */\n\trtlpriv->dbg.global_debuglevel = rtlpriv->cfg->mod_params->debug;\n\t/* for LPS & IPS */\n\trtlpriv->psc.inactiveps = rtlpriv->cfg->mod_params->inactiveps;\n\trtlpriv->psc.swctrl_lps = rtlpriv->cfg->mod_params->swctrl_lps;\n\trtlpriv->psc.fwctrl_lps = rtlpriv->cfg->mod_params->fwctrl_lps;\n\trtlpci->msi_support = rtlpriv->cfg->mod_params->msi_support;\n\trtlpriv->cfg->mod_params->sw_crypto =\n\t\trtlpriv->cfg->mod_params->sw_crypto;\n\trtlpriv->cfg->mod_params->disable_watchdog =\n\t\trtlpriv->cfg->mod_params->disable_watchdog;\n\tif (rtlpriv->cfg->mod_params->disable_watchdog)\n\t\tpr_info(\"watchdog disabled\\n\");\n\tif (!rtlpriv->psc.inactiveps)\n\t\tpr_info(\"rtl8188ee: Power Save off (module option)\\n\");\n\tif (!rtlpriv->psc.fwctrl_lps)\n\t\tpr_info(\"rtl8188ee: FW Power Save off (module option)\\n\");\n\trtlpriv->psc.reg_fwctrl_lps = 3;\n\trtlpriv->psc.reg_max_lps_awakeintvl = 5;\n\t/* for ASPM, you can close aspm through\n\t * set const_support_pciaspm = 0\n\t */\n\trtl88e_init_aspm_vars(hw);\n\n\tif (rtlpriv->psc.reg_fwctrl_lps == 1)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MIN_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 2)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MAX_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 3)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_DTIM_MODE;\n\n\t/* for firmware buf */\n\trtlpriv->rtlhal.pfirmware = vzalloc(0x8000);\n\tif (!rtlpriv->rtlhal.pfirmware) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Can't alloc buffer for fw.\\n\");\n\t\treturn 1;\n\t}\n\n\trtlpriv->cfg->fw_name = \"rtlwifi/rtl8188efw.bin\";\n\trtlpriv->max_fw_size = 0x8000;\n\tpr_info(\"Using firmware %s\\n\", rtlpriv->cfg->fw_name);\n\terr = request_firmware_nowait(THIS_MODULE, 1, rtlpriv->cfg->fw_name,\n\t\t\t\t      rtlpriv->io.dev, GFP_KERNEL, hw,\n\t\t\t\t      rtl_fw_cb);\n\tif (err) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Failed to request firmware!\\n\");\n\t\treturn 1;\n\t}\n\n\t/* for early mode */\n\trtlpriv->rtlhal.earlymode_enable = false;\n\trtlpriv->rtlhal.max_earlymode_num = 10;\n\tfor (tid = 0; tid < 8; tid++)\n\t\tskb_queue_head_init(&rtlpriv->mac80211.skb_waitq[tid]);\n\n\t/*low power */\n\trtlpriv->psc.low_power_enable = false;\n\tif (rtlpriv->psc.low_power_enable) {\n\t\tinit_timer(&rtlpriv->works.fw_clockoff_timer);\n\t\tsetup_timer(&rtlpriv->works.fw_clockoff_timer,\n\t\t\t    rtl88ee_fw_clk_off_timer_callback,\n\t\t\t    (unsigned long)hw);\n\t}\n\n\tinit_timer(&rtlpriv->works.fast_antenna_training_timer);\n\tsetup_timer(&rtlpriv->works.fast_antenna_training_timer,\n\t\t    rtl88e_dm_fast_antenna_training_callback,\n\t\t\t(unsigned long)hw);\n\treturn err;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nint rtl88e_init_sw_vars(struct ieee80211_hw *hw)\n{\n\tint err = 0;\n\tstruct rtl_priv *rtlpriv = rtl_priv(hw);\n\tstruct rtl_pci *rtlpci = rtl_pcidev(rtl_pcipriv(hw));\n\tu8 tid;\n\n\trtl8188ee_bt_reg_init(hw);\n\trtlpci->msi_support = rtlpriv->cfg->mod_params->msi_support;\n\n\trtlpriv->dm.dm_initialgain_enable = 1;\n\trtlpriv->dm.dm_flag = 0;\n\trtlpriv->dm.disable_framebursting = 0;\n\trtlpriv->dm.thermalvalue = 0;\n\trtlpci->transmit_config = CFENDFORM | BIT(15);\n\n\t/* compatible 5G band 88ce just 2.4G band & smsp */\n\trtlpriv->rtlhal.current_bandtype = BAND_ON_2_4G;\n\trtlpriv->rtlhal.bandset = BAND_ON_2_4G;\n\trtlpriv->rtlhal.macphymode = SINGLEMAC_SINGLEPHY;\n\n\trtlpci->receive_config = (RCR_APPFCS |\n\t\t\t\t  RCR_APP_MIC |\n\t\t\t\t  RCR_APP_ICV |\n\t\t\t\t  RCR_APP_PHYST_RXFF |\n\t\t\t\t  RCR_HTC_LOC_CTRL |\n\t\t\t\t  RCR_AMF |\n\t\t\t\t  RCR_ACF |\n\t\t\t\t  RCR_ADF |\n\t\t\t\t  RCR_AICV |\n\t\t\t\t  RCR_ACRC32 |\n\t\t\t\t  RCR_AB |\n\t\t\t\t  RCR_AM |\n\t\t\t\t  RCR_APM |\n\t\t\t\t  0);\n\n\trtlpci->irq_mask[0] =\n\t\t\t\t(u32)(IMR_PSTIMEOUT\t|\n\t\t\t\tIMR_HSISR_IND_ON_INT\t|\n\t\t\t\tIMR_C2HCMD\t\t|\n\t\t\t\tIMR_HIGHDOK\t\t|\n\t\t\t\tIMR_MGNTDOK\t\t|\n\t\t\t\tIMR_BKDOK\t\t|\n\t\t\t\tIMR_BEDOK\t\t|\n\t\t\t\tIMR_VIDOK\t\t|\n\t\t\t\tIMR_VODOK\t\t|\n\t\t\t\tIMR_RDU\t\t\t|\n\t\t\t\tIMR_ROK\t\t\t|\n\t\t\t\t0);\n\trtlpci->irq_mask[1] = (u32) (IMR_RXFOVW | 0);\n\trtlpci->sys_irq_mask = (u32) (HSIMR_PDN_INT_EN | HSIMR_RON_INT_EN);\n\n\t/* for debug level */\n\trtlpriv->dbg.global_debuglevel = rtlpriv->cfg->mod_params->debug;\n\t/* for LPS & IPS */\n\trtlpriv->psc.inactiveps = rtlpriv->cfg->mod_params->inactiveps;\n\trtlpriv->psc.swctrl_lps = rtlpriv->cfg->mod_params->swctrl_lps;\n\trtlpriv->psc.fwctrl_lps = rtlpriv->cfg->mod_params->fwctrl_lps;\n\tif (rtlpriv->cfg->mod_params->disable_watchdog)\n\t\tpr_info(\"watchdog disabled\\n\");\n\tif (!rtlpriv->psc.inactiveps)\n\t\tpr_info(\"rtl8188ee: Power Save off (module option)\\n\");\n\tif (!rtlpriv->psc.fwctrl_lps)\n\t\tpr_info(\"rtl8188ee: FW Power Save off (module option)\\n\");\n\trtlpriv->psc.reg_fwctrl_lps = 3;\n\trtlpriv->psc.reg_max_lps_awakeintvl = 5;\n\t/* for ASPM, you can close aspm through\n\t * set const_support_pciaspm = 0\n\t */\n\trtl88e_init_aspm_vars(hw);\n\n\tif (rtlpriv->psc.reg_fwctrl_lps == 1)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MIN_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 2)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MAX_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 3)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_DTIM_MODE;\n\n\t/* for firmware buf */\n\trtlpriv->rtlhal.pfirmware = vzalloc(0x8000);\n\tif (!rtlpriv->rtlhal.pfirmware) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Can't alloc buffer for fw.\\n\");\n\t\treturn 1;\n\t}\n\n\trtlpriv->cfg->fw_name = \"rtlwifi/rtl8188efw.bin\";\n\trtlpriv->max_fw_size = 0x8000;\n\tpr_info(\"Using firmware %s\\n\", rtlpriv->cfg->fw_name);\n\terr = request_firmware_nowait(THIS_MODULE, 1, rtlpriv->cfg->fw_name,\n\t\t\t\t      rtlpriv->io.dev, GFP_KERNEL, hw,\n\t\t\t\t      rtl_fw_cb);\n\tif (err) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Failed to request firmware!\\n\");\n\t\treturn 1;\n\t}\n\n\t/* for early mode */\n\trtlpriv->rtlhal.earlymode_enable = false;\n\trtlpriv->rtlhal.max_earlymode_num = 10;\n\tfor (tid = 0; tid < 8; tid++)\n\t\tskb_queue_head_init(&rtlpriv->mac80211.skb_waitq[tid]);\n\n\t/*low power */\n\trtlpriv->psc.low_power_enable = false;\n\tif (rtlpriv->psc.low_power_enable) {\n\t\tinit_timer(&rtlpriv->works.fw_clockoff_timer);\n\t\tsetup_timer(&rtlpriv->works.fw_clockoff_timer,\n\t\t\t    rtl88ee_fw_clk_off_timer_callback,\n\t\t\t    (unsigned long)hw);\n\t}\n\n\tinit_timer(&rtlpriv->works.fast_antenna_training_timer);\n\tsetup_timer(&rtlpriv->works.fast_antenna_training_timer,\n\t\t    rtl88e_dm_fast_antenna_training_callback,\n\t\t\t(unsigned long)hw);\n\treturn err;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nint rtl88e_init_sw_vars(struct ieee80211_hw *hw)\n{\n\tint err = 0;\n\tstruct rtl_priv *rtlpriv = rtl_priv(hw);\n\tstruct rtl_pci *rtlpci = rtl_pcidev(rtl_pcipriv(hw));\n\tu8 tid;\n\n\trtl8188ee_bt_reg_init(hw);\n\trtlpriv->dm.dm_initialgain_enable = 1;\n\trtlpriv->dm.dm_flag = 0;\n\trtlpriv->dm.disable_framebursting = 0;\n\trtlpriv->dm.thermalvalue = 0;\n\trtlpci->transmit_config = CFENDFORM | BIT(15);\n\n\t/* compatible 5G band 88ce just 2.4G band & smsp */\n\trtlpriv->rtlhal.current_bandtype = BAND_ON_2_4G;\n\trtlpriv->rtlhal.bandset = BAND_ON_2_4G;\n\trtlpriv->rtlhal.macphymode = SINGLEMAC_SINGLEPHY;\n\n\trtlpci->receive_config = (RCR_APPFCS |\n\t\t\t\t  RCR_APP_MIC |\n\t\t\t\t  RCR_APP_ICV |\n\t\t\t\t  RCR_APP_PHYST_RXFF |\n\t\t\t\t  RCR_HTC_LOC_CTRL |\n\t\t\t\t  RCR_AMF |\n\t\t\t\t  RCR_ACF |\n\t\t\t\t  RCR_ADF |\n\t\t\t\t  RCR_AICV |\n\t\t\t\t  RCR_ACRC32 |\n\t\t\t\t  RCR_AB |\n\t\t\t\t  RCR_AM |\n\t\t\t\t  RCR_APM |\n\t\t\t\t  0);\n\n\trtlpci->irq_mask[0] =\n\t\t\t\t(u32)(IMR_PSTIMEOUT\t|\n\t\t\t\tIMR_HSISR_IND_ON_INT\t|\n\t\t\t\tIMR_C2HCMD\t\t|\n\t\t\t\tIMR_HIGHDOK\t\t|\n\t\t\t\tIMR_MGNTDOK\t\t|\n\t\t\t\tIMR_BKDOK\t\t|\n\t\t\t\tIMR_BEDOK\t\t|\n\t\t\t\tIMR_VIDOK\t\t|\n\t\t\t\tIMR_VODOK\t\t|\n\t\t\t\tIMR_RDU\t\t\t|\n\t\t\t\tIMR_ROK\t\t\t|\n\t\t\t\t0);\n\trtlpci->irq_mask[1] = (u32) (IMR_RXFOVW | 0);\n\trtlpci->sys_irq_mask = (u32) (HSIMR_PDN_INT_EN | HSIMR_RON_INT_EN);\n\n\t/* for debug level */\n\trtlpriv->dbg.global_debuglevel = rtlpriv->cfg->mod_params->debug;\n\t/* for LPS & IPS */\n\trtlpriv->psc.inactiveps = rtlpriv->cfg->mod_params->inactiveps;\n\trtlpriv->psc.swctrl_lps = rtlpriv->cfg->mod_params->swctrl_lps;\n\trtlpriv->psc.fwctrl_lps = rtlpriv->cfg->mod_params->fwctrl_lps;\n\trtlpci->msi_support = rtlpriv->cfg->mod_params->msi_support;\n\trtlpriv->cfg->mod_params->sw_crypto =\n\t\trtlpriv->cfg->mod_params->sw_crypto;\n\trtlpriv->cfg->mod_params->disable_watchdog =\n\t\trtlpriv->cfg->mod_params->disable_watchdog;\n\tif (rtlpriv->cfg->mod_params->disable_watchdog)\n\t\tpr_info(\"watchdog disabled\\n\");\n\tif (!rtlpriv->psc.inactiveps)\n\t\tpr_info(\"rtl8188ee: Power Save off (module option)\\n\");\n\tif (!rtlpriv->psc.fwctrl_lps)\n\t\tpr_info(\"rtl8188ee: FW Power Save off (module option)\\n\");\n\trtlpriv->psc.reg_fwctrl_lps = 3;\n\trtlpriv->psc.reg_max_lps_awakeintvl = 5;\n\t/* for ASPM, you can close aspm through\n\t * set const_support_pciaspm = 0\n\t */\n\trtl88e_init_aspm_vars(hw);\n\n\tif (rtlpriv->psc.reg_fwctrl_lps == 1)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MIN_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 2)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_MAX_MODE;\n\telse if (rtlpriv->psc.reg_fwctrl_lps == 3)\n\t\trtlpriv->psc.fwctrl_psmode = FW_PS_DTIM_MODE;\n\n\t/* for firmware buf */\n\trtlpriv->rtlhal.pfirmware = vzalloc(0x8000);\n\tif (!rtlpriv->rtlhal.pfirmware) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Can't alloc buffer for fw.\\n\");\n\t\treturn 1;\n\t}\n\n\trtlpriv->cfg->fw_name = \"rtlwifi/rtl8188efw.bin\";\n\trtlpriv->max_fw_size = 0x8000;\n\tpr_info(\"Using firmware %s\\n\", rtlpriv->cfg->fw_name);\n\terr = request_firmware_nowait(THIS_MODULE, 1, rtlpriv->cfg->fw_name,\n\t\t\t\t      rtlpriv->io.dev, GFP_KERNEL, hw,\n\t\t\t\t      rtl_fw_cb);\n\tif (err) {\n\t\tRT_TRACE(rtlpriv, COMP_ERR, DBG_EMERG,\n\t\t\t \"Failed to request firmware!\\n\");\n\t\treturn 1;\n\t}\n\n\t/* for early mode */\n\trtlpriv->rtlhal.earlymode_enable = false;\n\trtlpriv->rtlhal.max_earlymode_num = 10;\n\tfor (tid = 0; tid < 8; tid++)\n\t\tskb_queue_head_init(&rtlpriv->mac80211.skb_waitq[tid]);\n\n\t/*low power */\n\trtlpriv->psc.low_power_enable = false;\n\tif (rtlpriv->psc.low_power_enable) {\n\t\tinit_timer(&rtlpriv->works.fw_clockoff_timer);\n\t\tsetup_timer(&rtlpriv->works.fw_clockoff_timer,\n\t\t\t    rtl88ee_fw_clk_off_timer_callback,\n\t\t\t    (unsigned long)hw);\n\t}\n\n\tinit_timer(&rtlpriv->works.fast_antenna_training_timer);\n\tsetup_timer(&rtlpriv->works.fast_antenna_training_timer,\n\t\t    rtl88e_dm_fast_antenna_training_callback,\n\t\t\t(unsigned long)hw);\n\treturn err;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nint bitmap_resize(struct bitmap *bitmap, sector_t blocks,\n\t\t  int chunksize, int init)\n{\n\t/* If chunk_size is 0, choose an appropriate chunk size.\n\t * Then possibly allocate new storage space.\n\t * Then quiesce, copy bits, replace bitmap, and re-start\n\t *\n\t * This function is called both to set up the initial bitmap\n\t * and to resize the bitmap while the array is active.\n\t * If this happens as a result of the array being resized,\n\t * chunksize will be zero, and we need to choose a suitable\n\t * chunksize, otherwise we use what we are given.\n\t */\n\tstruct bitmap_storage store;\n\tstruct bitmap_counts old_counts;\n\tunsigned long chunks;\n\tsector_t block;\n\tsector_t old_blocks, new_blocks;\n\tint chunkshift;\n\tint ret = 0;\n\tlong pages;\n\tstruct bitmap_page *new_bp;\n\n\tif (bitmap->storage.file && !init) {\n\t\tpr_info(\"md: cannot resize file-based bitmap\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (chunksize == 0) {\n\t\t/* If there is enough space, leave the chunk size unchanged,\n\t\t * else increase by factor of two until there is enough space.\n\t\t */\n\t\tlong bytes;\n\t\tlong space = bitmap->mddev->bitmap_info.space;\n\n\t\tif (space == 0) {\n\t\t\t/* We don't know how much space there is, so limit\n\t\t\t * to current size - in sectors.\n\t\t\t */\n\t\t\tbytes = DIV_ROUND_UP(bitmap->counts.chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t\tspace = DIV_ROUND_UP(bytes, 512);\n\t\t\tbitmap->mddev->bitmap_info.space = space;\n\t\t}\n\t\tchunkshift = bitmap->counts.chunkshift;\n\t\tchunkshift--;\n\t\tdo {\n\t\t\t/* 'chunkshift' is shift from block size to chunk size */\n\t\t\tchunkshift++;\n\t\t\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\t\t\tbytes = DIV_ROUND_UP(chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t} while (bytes > (space << 9));\n\t} else\n\t\tchunkshift = ffz(~chunksize) - BITMAP_BLOCK_SHIFT;\n\n\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\tmemset(&store, 0, sizeof(store));\n\tif (bitmap->mddev->bitmap_info.offset || bitmap->mddev->bitmap_info.file)\n\t\tret = bitmap_storage_alloc(&store, chunks,\n\t\t\t\t\t   !bitmap->mddev->bitmap_info.external,\n\t\t\t\t\t   mddev_is_clustered(bitmap->mddev)\n\t\t\t\t\t   ? bitmap->cluster_slot : 0);\n\tif (ret) {\n\t\tbitmap_file_unmap(&store);\n\t\tgoto err;\n\t}\n\n\tpages = DIV_ROUND_UP(chunks, PAGE_COUNTER_RATIO);\n\n\tnew_bp = kzalloc(pages * sizeof(*new_bp), GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!new_bp) {\n\t\tbitmap_file_unmap(&store);\n\t\tgoto err;\n\t}\n\n\tif (!init)\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 1);\n\n\tstore.file = bitmap->storage.file;\n\tbitmap->storage.file = NULL;\n\n\tif (store.sb_page && bitmap->storage.sb_page)\n\t\tmemcpy(page_address(store.sb_page),\n\t\t       page_address(bitmap->storage.sb_page),\n\t\t       sizeof(bitmap_super_t));\n\tbitmap_file_unmap(&bitmap->storage);\n\tbitmap->storage = store;\n\n\told_counts = bitmap->counts;\n\tbitmap->counts.bp = new_bp;\n\tbitmap->counts.pages = pages;\n\tbitmap->counts.missing_pages = pages;\n\tbitmap->counts.chunkshift = chunkshift;\n\tbitmap->counts.chunks = chunks;\n\tbitmap->mddev->bitmap_info.chunksize = 1 << (chunkshift +\n\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\n\tblocks = min(old_counts.chunks << old_counts.chunkshift,\n\t\t     chunks << chunkshift);\n\n\tspin_lock_irq(&bitmap->counts.lock);\n\t/* For cluster raid, need to pre-allocate bitmap */\n\tif (mddev_is_clustered(bitmap->mddev)) {\n\t\tunsigned long page;\n\t\tfor (page = 0; page < pages; page++) {\n\t\t\tret = bitmap_checkpage(&bitmap->counts, page, 1, 1);\n\t\t\tif (ret) {\n\t\t\t\tunsigned long k;\n\n\t\t\t\t/* deallocate the page memory */\n\t\t\t\tfor (k = 0; k < page; k++) {\n\t\t\t\t\tkfree(new_bp[k].map);\n\t\t\t\t}\n\n\t\t\t\t/* restore some fields from old_counts */\n\t\t\t\tbitmap->counts.bp = old_counts.bp;\n\t\t\t\tbitmap->counts.pages = old_counts.pages;\n\t\t\t\tbitmap->counts.missing_pages = old_counts.pages;\n\t\t\t\tbitmap->counts.chunkshift = old_counts.chunkshift;\n\t\t\t\tbitmap->counts.chunks = old_counts.chunks;\n\t\t\t\tbitmap->mddev->bitmap_info.chunksize = 1 << (old_counts.chunkshift +\n\t\t\t\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\t\t\t\tblocks = old_counts.chunks << old_counts.chunkshift;\n\t\t\t\tpr_warn(\"Could not pre-allocate in-memory bitmap for cluster raid\\n\");\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tbitmap->counts.bp[page].count += 1;\n\t\t}\n\t}\n\n\tfor (block = 0; block < blocks; ) {\n\t\tbitmap_counter_t *bmc_old, *bmc_new;\n\t\tint set;\n\n\t\tbmc_old = bitmap_get_counter(&old_counts, block,\n\t\t\t\t\t     &old_blocks, 0);\n\t\tset = bmc_old && NEEDED(*bmc_old);\n\n\t\tif (set) {\n\t\t\tbmc_new = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t     &new_blocks, 1);\n\t\t\tif (*bmc_new == 0) {\n\t\t\t\t/* need to set on-disk bits too. */\n\t\t\t\tsector_t end = block + new_blocks;\n\t\t\t\tsector_t start = block >> chunkshift;\n\t\t\t\tstart <<= chunkshift;\n\t\t\t\twhile (start < end) {\n\t\t\t\t\tbitmap_file_set_bit(bitmap, block);\n\t\t\t\t\tstart += 1 << chunkshift;\n\t\t\t\t}\n\t\t\t\t*bmc_new = 2;\n\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t  block, 1);\n\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t   block);\n\t\t\t}\n\t\t\t*bmc_new |= NEEDED_MASK;\n\t\t\tif (new_blocks < old_blocks)\n\t\t\t\told_blocks = new_blocks;\n\t\t}\n\t\tblock += old_blocks;\n\t}\n\n\tif (!init) {\n\t\tint i;\n\t\twhile (block < (chunks << chunkshift)) {\n\t\t\tbitmap_counter_t *bmc;\n\t\t\tbmc = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t &new_blocks, 1);\n\t\t\tif (bmc) {\n\t\t\t\t/* new space.  It needs to be resynced, so\n\t\t\t\t * we set NEEDED_MASK.\n\t\t\t\t */\n\t\t\t\tif (*bmc == 0) {\n\t\t\t\t\t*bmc = NEEDED_MASK | 2;\n\t\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t\t  block, 1);\n\t\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t\t   block);\n\t\t\t\t}\n\t\t\t}\n\t\t\tblock += new_blocks;\n\t\t}\n\t\tfor (i = 0; i < bitmap->storage.file_pages; i++)\n\t\t\tset_page_attr(bitmap, i, BITMAP_PAGE_DIRTY);\n\t}\n\tspin_unlock_irq(&bitmap->counts.lock);\n\n\tif (!init) {\n\t\tbitmap_unplug(bitmap);\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 0);\n\t}\n\tret = 0;\nerr:\n\treturn ret;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nint bitmap_resize(struct bitmap *bitmap, sector_t blocks,\n\t\t  int chunksize, int init)\n{\n\t/* If chunk_size is 0, choose an appropriate chunk size.\n\t * Then possibly allocate new storage space.\n\t * Then quiesce, copy bits, replace bitmap, and re-start\n\t *\n\t * This function is called both to set up the initial bitmap\n\t * and to resize the bitmap while the array is active.\n\t * If this happens as a result of the array being resized,\n\t * chunksize will be zero, and we need to choose a suitable\n\t * chunksize, otherwise we use what we are given.\n\t */\n\tstruct bitmap_storage store;\n\tstruct bitmap_counts old_counts;\n\tunsigned long chunks;\n\tsector_t block;\n\tsector_t old_blocks, new_blocks;\n\tint chunkshift;\n\tint ret = 0;\n\tlong pages;\n\tstruct bitmap_page *new_bp;\n\n\tif (bitmap->storage.file && !init) {\n\t\tpr_info(\"md: cannot resize file-based bitmap\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (chunksize == 0) {\n\t\t/* If there is enough space, leave the chunk size unchanged,\n\t\t * else increase by factor of two until there is enough space.\n\t\t */\n\t\tlong bytes;\n\t\tlong space = bitmap->mddev->bitmap_info.space;\n\n\t\tif (space == 0) {\n\t\t\t/* We don't know how much space there is, so limit\n\t\t\t * to current size - in sectors.\n\t\t\t */\n\t\t\tbytes = DIV_ROUND_UP(bitmap->counts.chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t\tspace = DIV_ROUND_UP(bytes, 512);\n\t\t\tbitmap->mddev->bitmap_info.space = space;\n\t\t}\n\t\tchunkshift = bitmap->counts.chunkshift;\n\t\tchunkshift--;\n\t\tdo {\n\t\t\t/* 'chunkshift' is shift from block size to chunk size */\n\t\t\tchunkshift++;\n\t\t\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\t\t\tbytes = DIV_ROUND_UP(chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t} while (bytes > (space << 9));\n\t} else\n\t\tchunkshift = ffz(~chunksize) - BITMAP_BLOCK_SHIFT;\n\n\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\tmemset(&store, 0, sizeof(store));\n\tif (bitmap->mddev->bitmap_info.offset || bitmap->mddev->bitmap_info.file)\n\t\tret = bitmap_storage_alloc(&store, chunks,\n\t\t\t\t\t   !bitmap->mddev->bitmap_info.external,\n\t\t\t\t\t   mddev_is_clustered(bitmap->mddev)\n\t\t\t\t\t   ? bitmap->cluster_slot : 0);\n\tif (ret) {\n\t\tbitmap_file_unmap(&store);\n\t\tgoto err;\n\t}\n\n\tpages = DIV_ROUND_UP(chunks, PAGE_COUNTER_RATIO);\n\n\tnew_bp = kzalloc(pages * sizeof(*new_bp), GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!new_bp) {\n\t\tbitmap_file_unmap(&store);\n\t\tgoto err;\n\t}\n\n\tif (!init)\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 1);\n\n\tstore.file = bitmap->storage.file;\n\tbitmap->storage.file = NULL;\n\n\tif (store.sb_page && bitmap->storage.sb_page)\n\t\tmemcpy(page_address(store.sb_page),\n\t\t       page_address(bitmap->storage.sb_page),\n\t\t       sizeof(bitmap_super_t));\n\tbitmap_file_unmap(&bitmap->storage);\n\tbitmap->storage = store;\n\n\told_counts = bitmap->counts;\n\tbitmap->counts.bp = new_bp;\n\tbitmap->counts.pages = pages;\n\tbitmap->counts.missing_pages = pages;\n\tbitmap->counts.chunkshift = chunkshift;\n\tbitmap->counts.chunks = chunks;\n\tbitmap->mddev->bitmap_info.chunksize = 1 << (chunkshift +\n\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\n\tblocks = min(old_counts.chunks << old_counts.chunkshift,\n\t\t     chunks << chunkshift);\n\n\tspin_lock_irq(&bitmap->counts.lock);\n\t/* For cluster raid, need to pre-allocate bitmap */\n\tif (mddev_is_clustered(bitmap->mddev)) {\n\t\tunsigned long page;\n\t\tfor (page = 0; page < pages; page++) {\n\t\t\tret = bitmap_checkpage(&bitmap->counts, page, 1, 1);\n\t\t\tif (ret) {\n\t\t\t\tunsigned long k;\n\n\t\t\t\t/* deallocate the page memory */\n\t\t\t\tfor (k = 0; k < page; k++) {\n\t\t\t\t\tkfree(new_bp[k].map);\n\t\t\t\t}\n\t\t\t\tkfree(new_bp);\n\n\t\t\t\t/* restore some fields from old_counts */\n\t\t\t\tbitmap->counts.bp = old_counts.bp;\n\t\t\t\tbitmap->counts.pages = old_counts.pages;\n\t\t\t\tbitmap->counts.missing_pages = old_counts.pages;\n\t\t\t\tbitmap->counts.chunkshift = old_counts.chunkshift;\n\t\t\t\tbitmap->counts.chunks = old_counts.chunks;\n\t\t\t\tbitmap->mddev->bitmap_info.chunksize = 1 << (old_counts.chunkshift +\n\t\t\t\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\t\t\t\tblocks = old_counts.chunks << old_counts.chunkshift;\n\t\t\t\tpr_warn(\"Could not pre-allocate in-memory bitmap for cluster raid\\n\");\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tbitmap->counts.bp[page].count += 1;\n\t\t}\n\t}\n\n\tfor (block = 0; block < blocks; ) {\n\t\tbitmap_counter_t *bmc_old, *bmc_new;\n\t\tint set;\n\n\t\tbmc_old = bitmap_get_counter(&old_counts, block,\n\t\t\t\t\t     &old_blocks, 0);\n\t\tset = bmc_old && NEEDED(*bmc_old);\n\n\t\tif (set) {\n\t\t\tbmc_new = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t     &new_blocks, 1);\n\t\t\tif (*bmc_new == 0) {\n\t\t\t\t/* need to set on-disk bits too. */\n\t\t\t\tsector_t end = block + new_blocks;\n\t\t\t\tsector_t start = block >> chunkshift;\n\t\t\t\tstart <<= chunkshift;\n\t\t\t\twhile (start < end) {\n\t\t\t\t\tbitmap_file_set_bit(bitmap, block);\n\t\t\t\t\tstart += 1 << chunkshift;\n\t\t\t\t}\n\t\t\t\t*bmc_new = 2;\n\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t  block, 1);\n\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t   block);\n\t\t\t}\n\t\t\t*bmc_new |= NEEDED_MASK;\n\t\t\tif (new_blocks < old_blocks)\n\t\t\t\told_blocks = new_blocks;\n\t\t}\n\t\tblock += old_blocks;\n\t}\n\n\tif (bitmap->counts.bp != old_counts.bp) {\n\t\tunsigned long k;\n\t\tfor (k = 0; k < old_counts.pages; k++)\n\t\t\tif (!old_counts.bp[k].hijacked)\n\t\t\t\tkfree(old_counts.bp[k].map);\n\t\tkfree(old_counts.bp);\n\t}\n\n\tif (!init) {\n\t\tint i;\n\t\twhile (block < (chunks << chunkshift)) {\n\t\t\tbitmap_counter_t *bmc;\n\t\t\tbmc = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t &new_blocks, 1);\n\t\t\tif (bmc) {\n\t\t\t\t/* new space.  It needs to be resynced, so\n\t\t\t\t * we set NEEDED_MASK.\n\t\t\t\t */\n\t\t\t\tif (*bmc == 0) {\n\t\t\t\t\t*bmc = NEEDED_MASK | 2;\n\t\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t\t  block, 1);\n\t\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t\t   block);\n\t\t\t\t}\n\t\t\t}\n\t\t\tblock += new_blocks;\n\t\t}\n\t\tfor (i = 0; i < bitmap->storage.file_pages; i++)\n\t\t\tset_page_attr(bitmap, i, BITMAP_PAGE_DIRTY);\n\t}\n\tspin_unlock_irq(&bitmap->counts.lock);\n\n\tif (!init) {\n\t\tbitmap_unplug(bitmap);\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 0);\n\t}\n\tret = 0;\nerr:\n\treturn ret;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nint bitmap_resize(struct bitmap *bitmap, sector_t blocks,\n\t\t  int chunksize, int init)\n{\n\t/* If chunk_size is 0, choose an appropriate chunk size.\n\t * Then possibly allocate new storage space.\n\t * Then quiesce, copy bits, replace bitmap, and re-start\n\t *\n\t * This function is called both to set up the initial bitmap\n\t * and to resize the bitmap while the array is active.\n\t * If this happens as a result of the array being resized,\n\t * chunksize will be zero, and we need to choose a suitable\n\t * chunksize, otherwise we use what we are given.\n\t */\n\tstruct bitmap_storage store;\n\tstruct bitmap_counts old_counts;\n\tunsigned long chunks;\n\tsector_t block;\n\tsector_t old_blocks, new_blocks;\n\tint chunkshift;\n\tint ret = 0;\n\tlong pages;\n\tstruct bitmap_page *new_bp;\n\n\tif (bitmap->storage.file && !init) {\n\t\tpr_info(\"md: cannot resize file-based bitmap\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (chunksize == 0) {\n\t\t/* If there is enough space, leave the chunk size unchanged,\n\t\t * else increase by factor of two until there is enough space.\n\t\t */\n\t\tlong bytes;\n\t\tlong space = bitmap->mddev->bitmap_info.space;\n\n\t\tif (space == 0) {\n\t\t\t/* We don't know how much space there is, so limit\n\t\t\t * to current size - in sectors.\n\t\t\t */\n\t\t\tbytes = DIV_ROUND_UP(bitmap->counts.chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t\tspace = DIV_ROUND_UP(bytes, 512);\n\t\t\tbitmap->mddev->bitmap_info.space = space;\n\t\t}\n\t\tchunkshift = bitmap->counts.chunkshift;\n\t\tchunkshift--;\n\t\tdo {\n\t\t\t/* 'chunkshift' is shift from block size to chunk size */\n\t\t\tchunkshift++;\n\t\t\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\t\t\tbytes = DIV_ROUND_UP(chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t} while (bytes > (space << 9));\n\t} else\n\t\tchunkshift = ffz(~chunksize) - BITMAP_BLOCK_SHIFT;\n\n\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\tmemset(&store, 0, sizeof(store));\n\tif (bitmap->mddev->bitmap_info.offset || bitmap->mddev->bitmap_info.file)\n\t\tret = bitmap_storage_alloc(&store, chunks,\n\t\t\t\t\t   !bitmap->mddev->bitmap_info.external,\n\t\t\t\t\t   mddev_is_clustered(bitmap->mddev)\n\t\t\t\t\t   ? bitmap->cluster_slot : 0);\n\tif (ret)\n\t\tgoto err;\n\n\tpages = DIV_ROUND_UP(chunks, PAGE_COUNTER_RATIO);\n\n\tnew_bp = kzalloc(pages * sizeof(*new_bp), GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!new_bp) {\n\t\tbitmap_file_unmap(&store);\n\t\tgoto err;\n\t}\n\n\tif (!init)\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 1);\n\n\tstore.file = bitmap->storage.file;\n\tbitmap->storage.file = NULL;\n\n\tif (store.sb_page && bitmap->storage.sb_page)\n\t\tmemcpy(page_address(store.sb_page),\n\t\t       page_address(bitmap->storage.sb_page),\n\t\t       sizeof(bitmap_super_t));\n\tbitmap_file_unmap(&bitmap->storage);\n\tbitmap->storage = store;\n\n\told_counts = bitmap->counts;\n\tbitmap->counts.bp = new_bp;\n\tbitmap->counts.pages = pages;\n\tbitmap->counts.missing_pages = pages;\n\tbitmap->counts.chunkshift = chunkshift;\n\tbitmap->counts.chunks = chunks;\n\tbitmap->mddev->bitmap_info.chunksize = 1 << (chunkshift +\n\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\n\tblocks = min(old_counts.chunks << old_counts.chunkshift,\n\t\t     chunks << chunkshift);\n\n\tspin_lock_irq(&bitmap->counts.lock);\n\t/* For cluster raid, need to pre-allocate bitmap */\n\tif (mddev_is_clustered(bitmap->mddev)) {\n\t\tunsigned long page;\n\t\tfor (page = 0; page < pages; page++) {\n\t\t\tret = bitmap_checkpage(&bitmap->counts, page, 1, 1);\n\t\t\tif (ret) {\n\t\t\t\tunsigned long k;\n\n\t\t\t\t/* deallocate the page memory */\n\t\t\t\tfor (k = 0; k < page; k++) {\n\t\t\t\t\tkfree(new_bp[k].map);\n\t\t\t\t}\n\n\t\t\t\t/* restore some fields from old_counts */\n\t\t\t\tbitmap->counts.bp = old_counts.bp;\n\t\t\t\tbitmap->counts.pages = old_counts.pages;\n\t\t\t\tbitmap->counts.missing_pages = old_counts.pages;\n\t\t\t\tbitmap->counts.chunkshift = old_counts.chunkshift;\n\t\t\t\tbitmap->counts.chunks = old_counts.chunks;\n\t\t\t\tbitmap->mddev->bitmap_info.chunksize = 1 << (old_counts.chunkshift +\n\t\t\t\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\t\t\t\tblocks = old_counts.chunks << old_counts.chunkshift;\n\t\t\t\tpr_err(\"Could not pre-allocate in-memory bitmap for cluster raid\\n\");\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tbitmap->counts.bp[page].count += 1;\n\t\t}\n\t}\n\n\tfor (block = 0; block < blocks; ) {\n\t\tbitmap_counter_t *bmc_old, *bmc_new;\n\t\tint set;\n\n\t\tbmc_old = bitmap_get_counter(&old_counts, block,\n\t\t\t\t\t     &old_blocks, 0);\n\t\tset = bmc_old && NEEDED(*bmc_old);\n\n\t\tif (set) {\n\t\t\tbmc_new = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t     &new_blocks, 1);\n\t\t\tif (*bmc_new == 0) {\n\t\t\t\t/* need to set on-disk bits too. */\n\t\t\t\tsector_t end = block + new_blocks;\n\t\t\t\tsector_t start = block >> chunkshift;\n\t\t\t\tstart <<= chunkshift;\n\t\t\t\twhile (start < end) {\n\t\t\t\t\tbitmap_file_set_bit(bitmap, block);\n\t\t\t\t\tstart += 1 << chunkshift;\n\t\t\t\t}\n\t\t\t\t*bmc_new = 2;\n\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t  block, 1);\n\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t   block);\n\t\t\t}\n\t\t\t*bmc_new |= NEEDED_MASK;\n\t\t\tif (new_blocks < old_blocks)\n\t\t\t\told_blocks = new_blocks;\n\t\t}\n\t\tblock += old_blocks;\n\t}\n\n\tif (!init) {\n\t\tint i;\n\t\twhile (block < (chunks << chunkshift)) {\n\t\t\tbitmap_counter_t *bmc;\n\t\t\tbmc = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t &new_blocks, 1);\n\t\t\tif (bmc) {\n\t\t\t\t/* new space.  It needs to be resynced, so\n\t\t\t\t * we set NEEDED_MASK.\n\t\t\t\t */\n\t\t\t\tif (*bmc == 0) {\n\t\t\t\t\t*bmc = NEEDED_MASK | 2;\n\t\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t\t  block, 1);\n\t\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t\t   block);\n\t\t\t\t}\n\t\t\t}\n\t\t\tblock += new_blocks;\n\t\t}\n\t\tfor (i = 0; i < bitmap->storage.file_pages; i++)\n\t\t\tset_page_attr(bitmap, i, BITMAP_PAGE_DIRTY);\n\t}\n\tspin_unlock_irq(&bitmap->counts.lock);\n\n\tif (!init) {\n\t\tbitmap_unplug(bitmap);\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 0);\n\t}\n\tret = 0;\nerr:\n\treturn ret;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nint bitmap_resize(struct bitmap *bitmap, sector_t blocks,\n\t\t  int chunksize, int init)\n{\n\t/* If chunk_size is 0, choose an appropriate chunk size.\n\t * Then possibly allocate new storage space.\n\t * Then quiesce, copy bits, replace bitmap, and re-start\n\t *\n\t * This function is called both to set up the initial bitmap\n\t * and to resize the bitmap while the array is active.\n\t * If this happens as a result of the array being resized,\n\t * chunksize will be zero, and we need to choose a suitable\n\t * chunksize, otherwise we use what we are given.\n\t */\n\tstruct bitmap_storage store;\n\tstruct bitmap_counts old_counts;\n\tunsigned long chunks;\n\tsector_t block;\n\tsector_t old_blocks, new_blocks;\n\tint chunkshift;\n\tint ret = 0;\n\tlong pages;\n\tstruct bitmap_page *new_bp;\n\n\tif (bitmap->storage.file && !init) {\n\t\tpr_info(\"md: cannot resize file-based bitmap\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (chunksize == 0) {\n\t\t/* If there is enough space, leave the chunk size unchanged,\n\t\t * else increase by factor of two until there is enough space.\n\t\t */\n\t\tlong bytes;\n\t\tlong space = bitmap->mddev->bitmap_info.space;\n\n\t\tif (space == 0) {\n\t\t\t/* We don't know how much space there is, so limit\n\t\t\t * to current size - in sectors.\n\t\t\t */\n\t\t\tbytes = DIV_ROUND_UP(bitmap->counts.chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t\tspace = DIV_ROUND_UP(bytes, 512);\n\t\t\tbitmap->mddev->bitmap_info.space = space;\n\t\t}\n\t\tchunkshift = bitmap->counts.chunkshift;\n\t\tchunkshift--;\n\t\tdo {\n\t\t\t/* 'chunkshift' is shift from block size to chunk size */\n\t\t\tchunkshift++;\n\t\t\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\t\t\tbytes = DIV_ROUND_UP(chunks, 8);\n\t\t\tif (!bitmap->mddev->bitmap_info.external)\n\t\t\t\tbytes += sizeof(bitmap_super_t);\n\t\t} while (bytes > (space << 9));\n\t} else\n\t\tchunkshift = ffz(~chunksize) - BITMAP_BLOCK_SHIFT;\n\n\tchunks = DIV_ROUND_UP_SECTOR_T(blocks, 1 << chunkshift);\n\tmemset(&store, 0, sizeof(store));\n\tif (bitmap->mddev->bitmap_info.offset || bitmap->mddev->bitmap_info.file)\n\t\tret = bitmap_storage_alloc(&store, chunks,\n\t\t\t\t\t   !bitmap->mddev->bitmap_info.external,\n\t\t\t\t\t   mddev_is_clustered(bitmap->mddev)\n\t\t\t\t\t   ? bitmap->cluster_slot : 0);\n\tif (ret)\n\t\tgoto err;\n\n\tpages = DIV_ROUND_UP(chunks, PAGE_COUNTER_RATIO);\n\n\tnew_bp = kzalloc(pages * sizeof(*new_bp), GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!new_bp) {\n\t\tbitmap_file_unmap(&store);\n\t\tgoto err;\n\t}\n\n\tif (!init)\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 1);\n\n\tstore.file = bitmap->storage.file;\n\tbitmap->storage.file = NULL;\n\n\tif (store.sb_page && bitmap->storage.sb_page)\n\t\tmemcpy(page_address(store.sb_page),\n\t\t       page_address(bitmap->storage.sb_page),\n\t\t       sizeof(bitmap_super_t));\n\tbitmap_file_unmap(&bitmap->storage);\n\tbitmap->storage = store;\n\n\told_counts = bitmap->counts;\n\tbitmap->counts.bp = new_bp;\n\tbitmap->counts.pages = pages;\n\tbitmap->counts.missing_pages = pages;\n\tbitmap->counts.chunkshift = chunkshift;\n\tbitmap->counts.chunks = chunks;\n\tbitmap->mddev->bitmap_info.chunksize = 1 << (chunkshift +\n\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\n\tblocks = min(old_counts.chunks << old_counts.chunkshift,\n\t\t     chunks << chunkshift);\n\n\tspin_lock_irq(&bitmap->counts.lock);\n\t/* For cluster raid, need to pre-allocate bitmap */\n\tif (mddev_is_clustered(bitmap->mddev)) {\n\t\tunsigned long page;\n\t\tfor (page = 0; page < pages; page++) {\n\t\t\tret = bitmap_checkpage(&bitmap->counts, page, 1, 1);\n\t\t\tif (ret) {\n\t\t\t\tunsigned long k;\n\n\t\t\t\t/* deallocate the page memory */\n\t\t\t\tfor (k = 0; k < page; k++) {\n\t\t\t\t\tkfree(new_bp[k].map);\n\t\t\t\t}\n\t\t\t\tkfree(new_bp);\n\n\t\t\t\t/* restore some fields from old_counts */\n\t\t\t\tbitmap->counts.bp = old_counts.bp;\n\t\t\t\tbitmap->counts.pages = old_counts.pages;\n\t\t\t\tbitmap->counts.missing_pages = old_counts.pages;\n\t\t\t\tbitmap->counts.chunkshift = old_counts.chunkshift;\n\t\t\t\tbitmap->counts.chunks = old_counts.chunks;\n\t\t\t\tbitmap->mddev->bitmap_info.chunksize = 1 << (old_counts.chunkshift +\n\t\t\t\t\t\t\t\t\t     BITMAP_BLOCK_SHIFT);\n\t\t\t\tblocks = old_counts.chunks << old_counts.chunkshift;\n\t\t\t\tpr_err(\"Could not pre-allocate in-memory bitmap for cluster raid\\n\");\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tbitmap->counts.bp[page].count += 1;\n\t\t}\n\t}\n\n\tfor (block = 0; block < blocks; ) {\n\t\tbitmap_counter_t *bmc_old, *bmc_new;\n\t\tint set;\n\n\t\tbmc_old = bitmap_get_counter(&old_counts, block,\n\t\t\t\t\t     &old_blocks, 0);\n\t\tset = bmc_old && NEEDED(*bmc_old);\n\n\t\tif (set) {\n\t\t\tbmc_new = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t     &new_blocks, 1);\n\t\t\tif (*bmc_new == 0) {\n\t\t\t\t/* need to set on-disk bits too. */\n\t\t\t\tsector_t end = block + new_blocks;\n\t\t\t\tsector_t start = block >> chunkshift;\n\t\t\t\tstart <<= chunkshift;\n\t\t\t\twhile (start < end) {\n\t\t\t\t\tbitmap_file_set_bit(bitmap, block);\n\t\t\t\t\tstart += 1 << chunkshift;\n\t\t\t\t}\n\t\t\t\t*bmc_new = 2;\n\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t  block, 1);\n\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t   block);\n\t\t\t}\n\t\t\t*bmc_new |= NEEDED_MASK;\n\t\t\tif (new_blocks < old_blocks)\n\t\t\t\told_blocks = new_blocks;\n\t\t}\n\t\tblock += old_blocks;\n\t}\n\n\tif (bitmap->counts.bp != old_counts.bp) {\n\t\tunsigned long k;\n\t\tfor (k = 0; k < old_counts.pages; k++)\n\t\t\tif (!old_counts.bp[k].hijacked)\n\t\t\t\tkfree(old_counts.bp[k].map);\n\t\tkfree(old_counts.bp);\n\t}\n\n\tif (!init) {\n\t\tint i;\n\t\twhile (block < (chunks << chunkshift)) {\n\t\t\tbitmap_counter_t *bmc;\n\t\t\tbmc = bitmap_get_counter(&bitmap->counts, block,\n\t\t\t\t\t\t &new_blocks, 1);\n\t\t\tif (bmc) {\n\t\t\t\t/* new space.  It needs to be resynced, so\n\t\t\t\t * we set NEEDED_MASK.\n\t\t\t\t */\n\t\t\t\tif (*bmc == 0) {\n\t\t\t\t\t*bmc = NEEDED_MASK | 2;\n\t\t\t\t\tbitmap_count_page(&bitmap->counts,\n\t\t\t\t\t\t\t  block, 1);\n\t\t\t\t\tbitmap_set_pending(&bitmap->counts,\n\t\t\t\t\t\t\t   block);\n\t\t\t\t}\n\t\t\t}\n\t\t\tblock += new_blocks;\n\t\t}\n\t\tfor (i = 0; i < bitmap->storage.file_pages; i++)\n\t\t\tset_page_attr(bitmap, i, BITMAP_PAGE_DIRTY);\n\t}\n\tspin_unlock_irq(&bitmap->counts.lock);\n\n\tif (!init) {\n\t\tbitmap_unplug(bitmap);\n\t\tbitmap->mddev->pers->quiesce(bitmap->mddev, 0);\n\t}\n\tret = 0;\nerr:\n\treturn ret;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nif (len < (USB_DT_CONFIG_SIZE + USB_DT_DEVICE_SIZE + 4))\n'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nif ((len < (USB_DT_CONFIG_SIZE + USB_DT_DEVICE_SIZE + 4)) ||\n\n\n(len > PAGE_SIZE * 4))\n'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nif (len < (USB_DT_CONFIG_SIZE + USB_DT_DEVICE_SIZE + 4))\n'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nif ((len < (USB_DT_CONFIG_SIZE + USB_DT_DEVICE_SIZE + 4)) ||\n\n\n(len > PAGE_SIZE * 4))\n'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nu32 mesh_accept_plinks_update(struct ieee80211_sub_if_data *sdata)\n{\n\tbool free_plinks;\n\tu32 changed = 0;\n\n\t/* In case mesh_plink_free_count > 0 and mesh_plinktbl_capacity == 0,\n\t * the mesh interface might be able to establish plinks with peers that\n\t * are already on the table but are not on PLINK_ESTAB state. However,\n\t * in general the mesh interface is not accepting peer link requests\n\t * from new peers, and that must be reflected in the beacon\n\t */\n\tfree_plinks = mesh_plink_availables(sdata);\n\n\tif (free_plinks != sdata->u.mesh.accepting_plinks) {\n\t\tsdata->u.mesh.accepting_plinks = free_plinks;\n\t\tchanged = BSS_CHANGED_BEACON;\n\t}\n\n\treturn changed;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nu32 mesh_accept_plinks_update(struct ieee80211_sub_if_data *sdata)\n{\n\tbool free_plinks;\n\tu32 changed = 0;\n\n\t/* In case mesh_plink_free_count > 0 and mesh_plinktbl_capacity == 0,\n\t * the mesh interface might be able to establish plinks with peers that\n\t * are already on the table but are not on PLINK_ESTAB state. However,\n\t * in general the mesh interface is not accepting peer link requests\n\t * from new peers, and that must be reflected in the beacon\n\t */\n\tfree_plinks = mesh_plink_availables(sdata);\n\n\tif (free_plinks != sdata->u.mesh.accepting_plinks) {\n\t\tsdata->u.mesh.accepting_plinks = free_plinks;\n\t\tchanged = BSS_CHANGED_BEACON;\n\t}\n\n\treturn changed;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nu32 mesh_accept_plinks_update(struct ieee80211_sub_if_data *sdata)\n{\n\tbool free_plinks;\n\tu32 changed = 0;\n\n\t/* In case mesh_plink_free_count > 0 and mesh_plinktbl_capacity == 0,\n\t * the mesh interface might be able to establish plinks with peers that\n\t * are already on the table but are not on PLINK_ESTAB state. However,\n\t * in general the mesh interface is not accepting peer link requests\n\t * from new peers, and that must be reflected in the beacon\n\t */\n\tfree_plinks = mesh_plink_availables(sdata);\n\n\tif (free_plinks != sdata->u.mesh.accepting_plinks) {\n\t\tsdata->u.mesh.accepting_plinks = free_plinks;\n\t\tchanged = BSS_CHANGED_BEACON;\n\t}\n\n\treturn changed;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nu32 mesh_accept_plinks_update(struct ieee80211_sub_if_data *sdata)\n{\n\tbool free_plinks;\n\tu32 changed = 0;\n\n\t/* In case mesh_plink_free_count > 0 and mesh_plinktbl_capacity == 0,\n\t * the mesh interface might be able to establish plinks with peers that\n\t * are already on the table but are not on PLINK_ESTAB state. However,\n\t * in general the mesh interface is not accepting peer link requests\n\t * from new peers, and that must be reflected in the beacon\n\t */\n\tfree_plinks = mesh_plink_availables(sdata);\n\n\tif (free_plinks != sdata->u.mesh.accepting_plinks) {\n\t\tsdata->u.mesh.accepting_plinks = free_plinks;\n\t\tchanged = BSS_CHANGED_BEACON;\n\t}\n\n\treturn changed;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void acm_read_bulk_callback(struct urb *urb)\n{\n\tstruct acm_rb *rb = urb->context;\n\tstruct acm *acm = rb->instance;\n\tunsigned long flags;\n\tint status = urb->status;\n\n\tdev_vdbg(&acm->data->dev, \"%s - urb %d, len %d\\n\", __func__,\n\t\t\t\t\trb->index, urb->actual_length);\n\n\tif (!acm->dev) {\n\t\tset_bit(rb->index, &acm->read_urbs_free);\n\t\tdev_dbg(&acm->data->dev, \"%s - disconnected\\n\", __func__);\n\t\treturn;\n\t}\n\n\tif (status) {\n\t\tset_bit(rb->index, &acm->read_urbs_free);\n\t\tdev_dbg(&acm->data->dev, \"%s - non-zero urb status: %d\\n\",\n\t\t\t\t\t\t\t__func__, status);\n\t\treturn;\n\t}\n\n\tusb_mark_last_busy(acm->dev);\n\n\tacm_process_read_urb(acm, urb);\n\t/*\n\t * Unthrottle may run on another CPU which needs to see events\n\t * in the same order. Submission has an implict barrier\n\t */\n\tsmp_mb__before_atomic();\n\tset_bit(rb->index, &acm->read_urbs_free);\n\n\t/* throttle device if requested by tty */\n\tspin_lock_irqsave(&acm->read_lock, flags);\n\tacm->throttled = acm->throttle_req;\n\tif (!acm->throttled) {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t\tacm_submit_read_urb(acm, rb->index, GFP_ATOMIC);\n\t} else {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t}\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void acm_read_bulk_callback(struct urb *urb)\n{\n\tstruct acm_rb *rb = urb->context;\n\tstruct acm *acm = rb->instance;\n\tunsigned long flags;\n\tint status = urb->status;\n\n\tdev_vdbg(&acm->data->dev, \"%s - urb %d, len %d\\n\", __func__,\n\t\t\t\t\trb->index, urb->actual_length);\n\n\tif (!acm->dev) {\n\t\tset_bit(rb->index, &acm->read_urbs_free);\n\t\tdev_dbg(&acm->data->dev, \"%s - disconnected\\n\", __func__);\n\t\treturn;\n\t}\n\n\tif (status) {\n\t\tset_bit(rb->index, &acm->read_urbs_free);\n\t\tdev_dbg(&acm->data->dev, \"%s - non-zero urb status: %d\\n\",\n\t\t\t\t\t\t\t__func__, status);\n\t\tif ((status != -ENOENT) || (urb->actual_length == 0))\n\t\t\treturn;\n\t}\n\n\tusb_mark_last_busy(acm->dev);\n\n\tacm_process_read_urb(acm, urb);\n\t/*\n\t * Unthrottle may run on another CPU which needs to see events\n\t * in the same order. Submission has an implict barrier\n\t */\n\tsmp_mb__before_atomic();\n\tset_bit(rb->index, &acm->read_urbs_free);\n\n\t/* throttle device if requested by tty */\n\tspin_lock_irqsave(&acm->read_lock, flags);\n\tacm->throttled = acm->throttle_req;\n\tif (!acm->throttled) {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t\tacm_submit_read_urb(acm, rb->index, GFP_ATOMIC);\n\t} else {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t}\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void acm_read_bulk_callback(struct urb *urb)\n{\n\tstruct acm_rb *rb = urb->context;\n\tstruct acm *acm = rb->instance;\n\tunsigned long flags;\n\n\tdev_vdbg(&acm->data->dev, \"%s - urb %d, len %d\\n\", __func__,\n\t\t\t\t\trb->index, urb->actual_length);\n\tset_bit(rb->index, &acm->read_urbs_free);\n\n\tif (!acm->dev) {\n\t\tdev_dbg(&acm->data->dev, \"%s - disconnected\\n\", __func__);\n\t\treturn;\n\t}\n\n\tif (urb->status) {\n\t\tdev_dbg(&acm->data->dev, \"%s - non-zero urb status: %d\\n\",\n\t\t\t\t\t\t\t__func__, urb->status);\n\t\treturn;\n\t}\n\n\tusb_mark_last_busy(acm->dev);\n\n\tacm_process_read_urb(acm, urb);\n\n\t/* throttle device if requested by tty */\n\tspin_lock_irqsave(&acm->read_lock, flags);\n\tacm->throttled = acm->throttle_req;\n\tif (!acm->throttled) {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t\tacm_submit_read_urb(acm, rb->index, GFP_ATOMIC);\n\t} else {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t}\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void acm_read_bulk_callback(struct urb *urb)\n{\n\tstruct acm_rb *rb = urb->context;\n\tstruct acm *acm = rb->instance;\n\tunsigned long flags;\n\n\tdev_vdbg(&acm->data->dev, \"%s - urb %d, len %d\\n\", __func__,\n\t\t\t\t\trb->index, urb->actual_length);\n\tset_bit(rb->index, &acm->read_urbs_free);\n\n\tif (!acm->dev) {\n\t\tdev_dbg(&acm->data->dev, \"%s - disconnected\\n\", __func__);\n\t\treturn;\n\t}\n\n\tif (urb->status) {\n\t\tdev_dbg(&acm->data->dev, \"%s - non-zero urb status: %d\\n\",\n\t\t\t\t\t\t\t__func__, urb->status);\n\t\tif ((urb->status != -ENOENT) || (urb->actual_length == 0))\n\t\t\treturn;\n\t}\n\n\tusb_mark_last_busy(acm->dev);\n\n\tacm_process_read_urb(acm, urb);\n\n\t/* throttle device if requested by tty */\n\tspin_lock_irqsave(&acm->read_lock, flags);\n\tacm->throttled = acm->throttle_req;\n\tif (!acm->throttled) {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t\tacm_submit_read_urb(acm, rb->index, GFP_ATOMIC);\n\t} else {\n\t\tspin_unlock_irqrestore(&acm->read_lock, flags);\n\t}\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void edge_interrupt_callback(struct urb *urb)\n{\n\tstruct edgeport_serial *edge_serial = urb->context;\n\tstruct usb_serial_port *port;\n\tstruct edgeport_port *edge_port;\n\tstruct device *dev;\n\tunsigned char *data = urb->transfer_buffer;\n\tint length = urb->actual_length;\n\tint port_number;\n\tint function;\n\tint retval;\n\t__u8 lsr;\n\t__u8 msr;\n\tint status = urb->status;\n\n\tswitch (status) {\n\tcase 0:\n\t\t/* success */\n\t\tbreak;\n\tcase -ECONNRESET:\n\tcase -ENOENT:\n\tcase -ESHUTDOWN:\n\t\t/* this urb is terminated, clean up */\n\t\tdev_dbg(&urb->dev->dev, \"%s - urb shutting down with status: %d\\n\",\n\t\t    __func__, status);\n\t\treturn;\n\tdefault:\n\t\tdev_err(&urb->dev->dev, \"%s - nonzero urb status received: \"\n\t\t\t\"%d\\n\", __func__, status);\n\t\tgoto exit;\n\t}\n\n\tif (!length) {\n\t\tdev_dbg(&urb->dev->dev, \"%s - no data in urb\\n\", __func__);\n\t\tgoto exit;\n\t}\n\n\tdev = &edge_serial->serial->dev->dev;\n\tusb_serial_debug_data(dev, __func__, length, data);\n\n\tif (length != 2) {\n\t\tdev_dbg(dev, \"%s - expecting packet of size 2, got %d\\n\", __func__, length);\n\t\tgoto exit;\n\t}\n\n\tport_number = TIUMP_GET_PORT_FROM_CODE(data[0]);\n\tfunction    = TIUMP_GET_FUNC_FROM_CODE(data[0]);\n\tdev_dbg(dev, \"%s - port_number %d, function %d, info 0x%x\\n\", __func__,\n\t\tport_number, function, data[1]);\n\tport = edge_serial->serial->port[port_number];\n\tedge_port = usb_get_serial_port_data(port);\n\tif (!edge_port) {\n\t\tdev_dbg(dev, \"%s - edge_port not found\\n\", __func__);\n\t\treturn;\n\t}\n\tswitch (function) {\n\tcase TIUMP_INTERRUPT_CODE_LSR:\n\t\tlsr = map_line_status(data[1]);\n\t\tif (lsr & UMP_UART_LSR_DATA_MASK) {\n\t\t\t/*\n\t\t\t * Save the LSR event for bulk read completion routine\n\t\t\t */\n\t\t\tdev_dbg(dev, \"%s - LSR Event Port %u LSR Status = %02x\\n\",\n\t\t\t\t__func__, port_number, lsr);\n\t\t\tedge_port->lsr_event = 1;\n\t\t\tedge_port->lsr_mask = lsr;\n\t\t} else {\n\t\t\tdev_dbg(dev, \"%s - ===== Port %d LSR Status = %02x ======\\n\",\n\t\t\t\t__func__, port_number, lsr);\n\t\t\thandle_new_lsr(edge_port, 0, lsr, 0);\n\t\t}\n\t\tbreak;\n\n\tcase TIUMP_INTERRUPT_CODE_MSR:\t/* MSR */\n\t\t/* Copy MSR from UMP */\n\t\tmsr = data[1];\n\t\tdev_dbg(dev, \"%s - ===== Port %u MSR Status = %02x ======\\n\",\n\t\t\t__func__, port_number, msr);\n\t\thandle_new_msr(edge_port, msr);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - Unknown Interrupt code from UMP %x\\n\",\n\t\t\t__func__, data[1]);\n\t\tbreak;\n\n\t}\n\nexit:\n\tretval = usb_submit_urb(urb, GFP_ATOMIC);\n\tif (retval)\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - usb_submit_urb failed with result %d\\n\",\n\t\t\t __func__, retval);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void edge_interrupt_callback(struct urb *urb)\n{\n\tstruct edgeport_serial *edge_serial = urb->context;\n\tstruct usb_serial_port *port;\n\tstruct edgeport_port *edge_port;\n\tstruct device *dev;\n\tunsigned char *data = urb->transfer_buffer;\n\tint length = urb->actual_length;\n\tint port_number;\n\tint function;\n\tint retval;\n\t__u8 lsr;\n\t__u8 msr;\n\tint status = urb->status;\n\n\tswitch (status) {\n\tcase 0:\n\t\t/* success */\n\t\tbreak;\n\tcase -ECONNRESET:\n\tcase -ENOENT:\n\tcase -ESHUTDOWN:\n\t\t/* this urb is terminated, clean up */\n\t\tdev_dbg(&urb->dev->dev, \"%s - urb shutting down with status: %d\\n\",\n\t\t    __func__, status);\n\t\treturn;\n\tdefault:\n\t\tdev_err(&urb->dev->dev, \"%s - nonzero urb status received: \"\n\t\t\t\"%d\\n\", __func__, status);\n\t\tgoto exit;\n\t}\n\n\tif (!length) {\n\t\tdev_dbg(&urb->dev->dev, \"%s - no data in urb\\n\", __func__);\n\t\tgoto exit;\n\t}\n\n\tdev = &edge_serial->serial->dev->dev;\n\tusb_serial_debug_data(dev, __func__, length, data);\n\n\tif (length != 2) {\n\t\tdev_dbg(dev, \"%s - expecting packet of size 2, got %d\\n\", __func__, length);\n\t\tgoto exit;\n\t}\n\n\tport_number = TIUMP_GET_PORT_FROM_CODE(data[0]);\n\tfunction    = TIUMP_GET_FUNC_FROM_CODE(data[0]);\n\tdev_dbg(dev, \"%s - port_number %d, function %d, info 0x%x\\n\", __func__,\n\t\tport_number, function, data[1]);\n\n\tif (port_number >= edge_serial->serial->num_ports) {\n\t\tdev_err(dev, \"bad port number %d\\n\", port_number);\n\t\tgoto exit;\n\t}\n\n\tport = edge_serial->serial->port[port_number];\n\tedge_port = usb_get_serial_port_data(port);\n\tif (!edge_port) {\n\t\tdev_dbg(dev, \"%s - edge_port not found\\n\", __func__);\n\t\treturn;\n\t}\n\tswitch (function) {\n\tcase TIUMP_INTERRUPT_CODE_LSR:\n\t\tlsr = map_line_status(data[1]);\n\t\tif (lsr & UMP_UART_LSR_DATA_MASK) {\n\t\t\t/*\n\t\t\t * Save the LSR event for bulk read completion routine\n\t\t\t */\n\t\t\tdev_dbg(dev, \"%s - LSR Event Port %u LSR Status = %02x\\n\",\n\t\t\t\t__func__, port_number, lsr);\n\t\t\tedge_port->lsr_event = 1;\n\t\t\tedge_port->lsr_mask = lsr;\n\t\t} else {\n\t\t\tdev_dbg(dev, \"%s - ===== Port %d LSR Status = %02x ======\\n\",\n\t\t\t\t__func__, port_number, lsr);\n\t\t\thandle_new_lsr(edge_port, 0, lsr, 0);\n\t\t}\n\t\tbreak;\n\n\tcase TIUMP_INTERRUPT_CODE_MSR:\t/* MSR */\n\t\t/* Copy MSR from UMP */\n\t\tmsr = data[1];\n\t\tdev_dbg(dev, \"%s - ===== Port %u MSR Status = %02x ======\\n\",\n\t\t\t__func__, port_number, msr);\n\t\thandle_new_msr(edge_port, msr);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - Unknown Interrupt code from UMP %x\\n\",\n\t\t\t__func__, data[1]);\n\t\tbreak;\n\n\t}\n\nexit:\n\tretval = usb_submit_urb(urb, GFP_ATOMIC);\n\tif (retval)\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - usb_submit_urb failed with result %d\\n\",\n\t\t\t __func__, retval);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void edge_interrupt_callback(struct urb *urb)\n{\n\tstruct edgeport_serial *edge_serial = urb->context;\n\tstruct usb_serial_port *port;\n\tstruct edgeport_port *edge_port;\n\tunsigned char *data = urb->transfer_buffer;\n\tint length = urb->actual_length;\n\tint port_number;\n\tint function;\n\tint retval;\n\t__u8 lsr;\n\t__u8 msr;\n\tint status = urb->status;\n\n\tdbg(\"%s\", __func__);\n\n\tswitch (status) {\n\tcase 0:\n\t\t/* success */\n\t\tbreak;\n\tcase -ECONNRESET:\n\tcase -ENOENT:\n\tcase -ESHUTDOWN:\n\t\t/* this urb is terminated, clean up */\n\t\tdbg(\"%s - urb shutting down with status: %d\",\n\t\t    __func__, status);\n\t\treturn;\n\tdefault:\n\t\tdev_err(&urb->dev->dev, \"%s - nonzero urb status received: \"\n\t\t\t\"%d\\n\", __func__, status);\n\t\tgoto exit;\n\t}\n\n\tif (!length) {\n\t\tdbg(\"%s - no data in urb\", __func__);\n\t\tgoto exit;\n\t}\n\n\tusb_serial_debug_data(debug, &edge_serial->serial->dev->dev,\n\t\t\t\t\t\t__func__, length, data);\n\n\tif (length != 2) {\n\t\tdbg(\"%s - expecting packet of size 2, got %d\",\n\t\t\t\t\t\t\t__func__, length);\n\t\tgoto exit;\n\t}\n\n\tport_number = TIUMP_GET_PORT_FROM_CODE(data[0]);\n\tfunction    = TIUMP_GET_FUNC_FROM_CODE(data[0]);\n\tdbg(\"%s - port_number %d, function %d, info 0x%x\",\n\t     __func__, port_number, function, data[1]);\n\tport = edge_serial->serial->port[port_number];\n\tedge_port = usb_get_serial_port_data(port);\n\tif (!edge_port) {\n\t\tdbg(\"%s - edge_port not found\", __func__);\n\t\treturn;\n\t}\n\tswitch (function) {\n\tcase TIUMP_INTERRUPT_CODE_LSR:\n\t\tlsr = map_line_status(data[1]);\n\t\tif (lsr & UMP_UART_LSR_DATA_MASK) {\n\t\t\t/* Save the LSR event for bulk read\n\t\t\t   completion routine */\n\t\t\tdbg(\"%s - LSR Event Port %u LSR Status = %02x\",\n\t\t\t     __func__, port_number, lsr);\n\t\t\tedge_port->lsr_event = 1;\n\t\t\tedge_port->lsr_mask = lsr;\n\t\t} else {\n\t\t\tdbg(\"%s - ===== Port %d LSR Status = %02x ======\",\n\t\t\t     __func__, port_number, lsr);\n\t\t\thandle_new_lsr(edge_port, 0, lsr, 0);\n\t\t}\n\t\tbreak;\n\n\tcase TIUMP_INTERRUPT_CODE_MSR:\t/* MSR */\n\t\t/* Copy MSR from UMP */\n\t\tmsr = data[1];\n\t\tdbg(\"%s - ===== Port %u MSR Status = %02x ======\",\n\t\t     __func__, port_number, msr);\n\t\thandle_new_msr(edge_port, msr);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - Unknown Interrupt code from UMP %x\\n\",\n\t\t\t__func__, data[1]);\n\t\tbreak;\n\n\t}\n\nexit:\n\tretval = usb_submit_urb(urb, GFP_ATOMIC);\n\tif (retval)\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - usb_submit_urb failed with result %d\\n\",\n\t\t\t __func__, retval);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void edge_interrupt_callback(struct urb *urb)\n{\n\tstruct edgeport_serial *edge_serial = urb->context;\n\tstruct usb_serial_port *port;\n\tstruct edgeport_port *edge_port;\n\tunsigned char *data = urb->transfer_buffer;\n\tint length = urb->actual_length;\n\tint port_number;\n\tint function;\n\tint retval;\n\t__u8 lsr;\n\t__u8 msr;\n\tint status = urb->status;\n\n\tdbg(\"%s\", __func__);\n\n\tswitch (status) {\n\tcase 0:\n\t\t/* success */\n\t\tbreak;\n\tcase -ECONNRESET:\n\tcase -ENOENT:\n\tcase -ESHUTDOWN:\n\t\t/* this urb is terminated, clean up */\n\t\tdbg(\"%s - urb shutting down with status: %d\",\n\t\t    __func__, status);\n\t\treturn;\n\tdefault:\n\t\tdev_err(&urb->dev->dev, \"%s - nonzero urb status received: \"\n\t\t\t\"%d\\n\", __func__, status);\n\t\tgoto exit;\n\t}\n\n\tif (!length) {\n\t\tdbg(\"%s - no data in urb\", __func__);\n\t\tgoto exit;\n\t}\n\n\tusb_serial_debug_data(debug, &edge_serial->serial->dev->dev,\n\t\t\t\t\t\t__func__, length, data);\n\n\tif (length != 2) {\n\t\tdbg(\"%s - expecting packet of size 2, got %d\",\n\t\t\t\t\t\t\t__func__, length);\n\t\tgoto exit;\n\t}\n\n\tport_number = TIUMP_GET_PORT_FROM_CODE(data[0]);\n\tfunction    = TIUMP_GET_FUNC_FROM_CODE(data[0]);\n\tdbg(\"%s - port_number %d, function %d, info 0x%x\",\n\t     __func__, port_number, function, data[1]);\n\n\tif (port_number >= edge_serial->serial->num_ports) {\n\t\tdev_err(&urb->dev->dev, \"bad port number %d\\n\", port_number);\n\t\tgoto exit;\n\t}\n\n\tport = edge_serial->serial->port[port_number];\n\tedge_port = usb_get_serial_port_data(port);\n\tif (!edge_port) {\n\t\tdbg(\"%s - edge_port not found\", __func__);\n\t\treturn;\n\t}\n\tswitch (function) {\n\tcase TIUMP_INTERRUPT_CODE_LSR:\n\t\tlsr = map_line_status(data[1]);\n\t\tif (lsr & UMP_UART_LSR_DATA_MASK) {\n\t\t\t/* Save the LSR event for bulk read\n\t\t\t   completion routine */\n\t\t\tdbg(\"%s - LSR Event Port %u LSR Status = %02x\",\n\t\t\t     __func__, port_number, lsr);\n\t\t\tedge_port->lsr_event = 1;\n\t\t\tedge_port->lsr_mask = lsr;\n\t\t} else {\n\t\t\tdbg(\"%s - ===== Port %d LSR Status = %02x ======\",\n\t\t\t     __func__, port_number, lsr);\n\t\t\thandle_new_lsr(edge_port, 0, lsr, 0);\n\t\t}\n\t\tbreak;\n\n\tcase TIUMP_INTERRUPT_CODE_MSR:\t/* MSR */\n\t\t/* Copy MSR from UMP */\n\t\tmsr = data[1];\n\t\tdbg(\"%s - ===== Port %u MSR Status = %02x ======\",\n\t\t     __func__, port_number, msr);\n\t\thandle_new_msr(edge_port, msr);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - Unknown Interrupt code from UMP %x\\n\",\n\t\t\t__func__, data[1]);\n\t\tbreak;\n\n\t}\n\nexit:\n\tretval = usb_submit_urb(urb, GFP_ATOMIC);\n\tif (retval)\n\t\tdev_err(&urb->dev->dev,\n\t\t\t\"%s - usb_submit_urb failed with result %d\\n\",\n\t\t\t __func__, retval);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,\n\t\t\t\t u32 index, int *nent, int maxnent)\n{\n\tint r;\n\tunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\n\tunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n\t\t\t\t? F(GBPAGES) : 0;\n\tunsigned f_lm = F(LM);\n#else\n\tunsigned f_gbpages = 0;\n\tunsigned f_lm = 0;\n#endif\n\tunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\n\tunsigned f_invpcid = kvm_x86_ops->invpcid_supported() ? F(INVPCID) : 0;\n\tunsigned f_mpx = kvm_mpx_supported() ? F(MPX) : 0;\n\tunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\n\n\t/* cpuid 1.edx */\n\tconst u32 kvm_cpuid_1_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |\n\t\t0 /* Reserved, DS, ACPI */ | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0 /* HTT, TM, Reserved, PBE */;\n\t/* cpuid 0x80000001.edx */\n\tconst u32 kvm_cpuid_8000_0001_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* Reserved */ |\n\t\tf_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n\t\t0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW);\n\t/* cpuid 1.ecx */\n\tconst u32 kvm_cpuid_1_ecx_x86_features =\n\t\t/* NOTE: MONITOR (and MWAIT) are emulated as NOP,\n\t\t * but *not* advertised to guests via CPUID ! */\n\t\tF(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |\n\t\t0 /* DS-CPL, VMX, SMX, EST */ |\n\t\t0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |\n\t\tF(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |\n\t\tF(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |\n\t\tF(F16C) | F(RDRAND);\n\t/* cpuid 0x80000001.ecx */\n\tconst u32 kvm_cpuid_8000_0001_ecx_x86_features =\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |\n\t\t0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);\n\n\t/* cpuid 0xC0000001.edx */\n\tconst u32 kvm_cpuid_C000_0001_edx_x86_features =\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN);\n\n\t/* cpuid 7.0.ebx */\n\tconst u32 kvm_cpuid_7_0_ebx_x86_features =\n\t\tF(FSGSBASE) | F(BMI1) | F(HLE) | F(AVX2) | F(SMEP) |\n\t\tF(BMI2) | F(ERMS) | f_invpcid | F(RTM) | f_mpx | F(RDSEED) |\n\t\tF(ADX) | F(SMAP) | F(AVX512F) | F(AVX512PF) | F(AVX512ER) |\n\t\tF(AVX512CD) | F(CLFLUSHOPT) | F(CLWB) | F(PCOMMIT);\n\n\t/* cpuid 0xD.1.eax */\n\tconst u32 kvm_cpuid_D_1_eax_x86_features =\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\n\n\t/* cpuid 7.0.ecx*/\n\tconst u32 kvm_cpuid_7_0_ecx_x86_features = F(PKU) | 0 /*OSPKE*/;\n\n\t/* all calls to cpuid_count() should be made on the same cpu */\n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tif (*nent >= maxnent)\n\t\tgoto out;\n\n\tdo_cpuid_1_ent(entry, function, index);\n\t++*nent;\n\n\tswitch (function) {\n\tcase 0:\n\t\tentry->eax = min(entry->eax, (u32)0xd);\n\t\tbreak;\n\tcase 1:\n\t\tentry->edx &= kvm_cpuid_1_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_1_EDX);\n\t\tentry->ecx &= kvm_cpuid_1_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_1_ECX);\n\t\t/* we support x2apic emulation even if host does not support\n\t\t * it since we emulate x2apic in software */\n\t\tentry->ecx |= F(X2APIC);\n\t\tbreak;\n\t/* function 2 entries are STATEFUL. That is, repeated cpuid commands\n\t * may return different values. This forces us to get_cpu() before\n\t * issuing the first command, and also to emulate this annoying behavior\n\t * in kvm_emulate_cpuid() using KVM_CPUID_FLAG_STATE_READ_NEXT */\n\tcase 2: {\n\t\tint t, times = entry->eax & 0xff;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\tentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\n\t\tfor (t = 1; t < times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[t], function, 0);\n\t\t\tentry[t].flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\t/* function 4 has additional index. */\n\tcase 4: {\n\t\tint i, cache_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until cache_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tcache_type = entry[i - 1].eax & 0x1f;\n\t\t\tif (!cache_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 6: /* Thermal management */\n\t\tentry->eax = 0x4; /* allow ARAT */\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 7: {\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* Mask ebx against host capability word 9 */\n\t\tif (index == 0) {\n\t\t\tentry->ebx &= kvm_cpuid_7_0_ebx_x86_features;\n\t\t\tcpuid_mask(&entry->ebx, CPUID_7_0_EBX);\n\t\t\t// TSC_ADJUST is emulated\n\t\t\tentry->ebx |= F(TSC_ADJUST);\n\t\t\tentry->ecx &= kvm_cpuid_7_0_ecx_x86_features;\n\t\t\tcpuid_mask(&entry->ecx, CPUID_7_ECX);\n\t\t\t/* PKU is not yet implemented for shadow paging. */\n\t\t\tif (!tdp_enabled)\n\t\t\t\tentry->ecx &= ~F(PKU);\n\t\t} else {\n\t\t\tentry->ebx = 0;\n\t\t\tentry->ecx = 0;\n\t\t}\n\t\tentry->eax = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 9:\n\t\tbreak;\n\tcase 0xa: { /* Architectural Performance Monitoring */\n\t\tstruct x86_pmu_capability cap;\n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tperf_get_x86_pmu_capability(&cap);\n\n\t\t/*\n\t\t * Only support guest architectural pmu on a host\n\t\t * with architectural pmu.\n\t\t */\n\t\tif (!cap.version)\n\t\t\tmemset(&cap, 0, sizeof(cap));\n\n\t\teax.split.version_id = min(cap.version, 2);\n\t\teax.split.num_counters = cap.num_counters_gp;\n\t\teax.split.bit_width = cap.bit_width_gp;\n\t\teax.split.mask_length = cap.events_mask_len;\n\n\t\tedx.split.num_counters_fixed = cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = cap.bit_width_fixed;\n\t\tedx.split.reserved = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\t/* function 0xb has additional index. */\n\tcase 0xb: {\n\t\tint i, level_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until level_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tlevel_type = entry[i - 1].ecx & 0xff00;\n\t\t\tif (!level_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0xd: {\n\t\tint idx, i;\n\t\tu64 supported = kvm_supported_xcr0();\n\n\t\tentry->eax &= supported;\n\t\tentry->ebx = xstate_required_size(supported, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= supported >> 32;\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\tif (!supported)\n\t\t\tbreak;\n\n\t\tfor (idx = 1, i = 1; idx < 64; ++idx) {\n\t\t\tu64 mask = ((u64)1 << idx);\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[i], function, idx);\n\t\t\tif (idx == 1) {\n\t\t\t\tentry[i].eax &= kvm_cpuid_D_1_eax_x86_features;\n\t\t\t\tentry[i].ebx = 0;\n\t\t\t\tif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\t\t\tentry[i].ebx =\n\t\t\t\t\t\txstate_required_size(supported,\n\t\t\t\t\t\t\t\t     true);\n\t\t\t} else {\n\t\t\t\tif (entry[i].eax == 0 || !(supported & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (WARN_ON_ONCE(entry[i].ecx & 1))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tentry[i].ecx = 0;\n\t\t\tentry[i].edx = 0;\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t\t++i;\n\t\t}\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\n\t\tconst u32 *sigptr = (const u32 *)signature;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x8000001a);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->edx &= kvm_cpuid_8000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_8000_0001_EDX);\n\t\tentry->ecx &= kvm_cpuid_8000_0001_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_8000_0001_ECX);\n\t\tbreak;\n\tcase 0x80000007: /* Advanced power management */\n\t\t/* invariant TSC is CPUID.80000007H:EDX[8] */\n\t\tentry->edx &= (1 << 8);\n\t\t/* mask against host */\n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\tif (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->ebx = entry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\t\tbreak;\n\tcase 0x8000001d:\n\t\tbreak;\n\t/*Add support for Centaur's CPUID instruction*/\n\tcase 0xC0000000:\n\t\t/*Just support up to 0xC0000004 now*/\n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tentry->edx &= kvm_cpuid_C000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_C000_0001_EDX);\n\t\tbreak;\n\tcase 3: /* Processor serial number */\n\tcase 5: /* MONITOR/MWAIT */\n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tkvm_x86_ops->set_supported_cpuid(function, entry);\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,\n\t\t\t\t u32 index, int *nent, int maxnent)\n{\n\tint r;\n\tunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\n\tunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n\t\t\t\t? F(GBPAGES) : 0;\n\tunsigned f_lm = F(LM);\n#else\n\tunsigned f_gbpages = 0;\n\tunsigned f_lm = 0;\n#endif\n\tunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\n\tunsigned f_invpcid = kvm_x86_ops->invpcid_supported() ? F(INVPCID) : 0;\n\tunsigned f_mpx = kvm_mpx_supported() ? F(MPX) : 0;\n\tunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\n\n\t/* cpuid 1.edx */\n\tconst u32 kvm_cpuid_1_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |\n\t\t0 /* Reserved, DS, ACPI */ | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0 /* HTT, TM, Reserved, PBE */;\n\t/* cpuid 0x80000001.edx */\n\tconst u32 kvm_cpuid_8000_0001_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* Reserved */ |\n\t\tf_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n\t\t0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW);\n\t/* cpuid 1.ecx */\n\tconst u32 kvm_cpuid_1_ecx_x86_features =\n\t\t/* NOTE: MONITOR (and MWAIT) are emulated as NOP,\n\t\t * but *not* advertised to guests via CPUID ! */\n\t\tF(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |\n\t\t0 /* DS-CPL, VMX, SMX, EST */ |\n\t\t0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |\n\t\tF(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |\n\t\tF(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |\n\t\tF(F16C) | F(RDRAND);\n\t/* cpuid 0x80000001.ecx */\n\tconst u32 kvm_cpuid_8000_0001_ecx_x86_features =\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |\n\t\t0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);\n\n\t/* cpuid 0xC0000001.edx */\n\tconst u32 kvm_cpuid_C000_0001_edx_x86_features =\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN);\n\n\t/* cpuid 7.0.ebx */\n\tconst u32 kvm_cpuid_7_0_ebx_x86_features =\n\t\tF(FSGSBASE) | F(BMI1) | F(HLE) | F(AVX2) | F(SMEP) |\n\t\tF(BMI2) | F(ERMS) | f_invpcid | F(RTM) | f_mpx | F(RDSEED) |\n\t\tF(ADX) | F(SMAP) | F(AVX512F) | F(AVX512PF) | F(AVX512ER) |\n\t\tF(AVX512CD) | F(CLFLUSHOPT) | F(CLWB) | F(PCOMMIT);\n\n\t/* cpuid 0xD.1.eax */\n\tconst u32 kvm_cpuid_D_1_eax_x86_features =\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\n\n\t/* cpuid 7.0.ecx*/\n\tconst u32 kvm_cpuid_7_0_ecx_x86_features = F(PKU) | 0 /*OSPKE*/;\n\n\t/* all calls to cpuid_count() should be made on the same cpu */\n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tif (*nent >= maxnent)\n\t\tgoto out;\n\n\tdo_cpuid_1_ent(entry, function, index);\n\t++*nent;\n\n\tswitch (function) {\n\tcase 0:\n\t\tentry->eax = min(entry->eax, (u32)0xd);\n\t\tbreak;\n\tcase 1:\n\t\tentry->edx &= kvm_cpuid_1_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_1_EDX);\n\t\tentry->ecx &= kvm_cpuid_1_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_1_ECX);\n\t\t/* we support x2apic emulation even if host does not support\n\t\t * it since we emulate x2apic in software */\n\t\tentry->ecx |= F(X2APIC);\n\t\tbreak;\n\t/* function 2 entries are STATEFUL. That is, repeated cpuid commands\n\t * may return different values. This forces us to get_cpu() before\n\t * issuing the first command, and also to emulate this annoying behavior\n\t * in kvm_emulate_cpuid() using KVM_CPUID_FLAG_STATE_READ_NEXT */\n\tcase 2: {\n\t\tint t, times = entry->eax & 0xff;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\tentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\n\t\tfor (t = 1; t < times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[t], function, 0);\n\t\t\tentry[t].flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\t/* function 4 has additional index. */\n\tcase 4: {\n\t\tint i, cache_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until cache_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tcache_type = entry[i - 1].eax & 0x1f;\n\t\t\tif (!cache_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 6: /* Thermal management */\n\t\tentry->eax = 0x4; /* allow ARAT */\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 7: {\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* Mask ebx against host capability word 9 */\n\t\tif (index == 0) {\n\t\t\tentry->ebx &= kvm_cpuid_7_0_ebx_x86_features;\n\t\t\tcpuid_mask(&entry->ebx, CPUID_7_0_EBX);\n\t\t\t// TSC_ADJUST is emulated\n\t\t\tentry->ebx |= F(TSC_ADJUST);\n\t\t\tentry->ecx &= kvm_cpuid_7_0_ecx_x86_features;\n\t\t\tcpuid_mask(&entry->ecx, CPUID_7_ECX);\n\t\t\t/* PKU is not yet implemented for shadow paging. */\n\t\t\tif (!tdp_enabled)\n\t\t\t\tentry->ecx &= ~F(PKU);\n\t\t} else {\n\t\t\tentry->ebx = 0;\n\t\t\tentry->ecx = 0;\n\t\t}\n\t\tentry->eax = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 9:\n\t\tbreak;\n\tcase 0xa: { /* Architectural Performance Monitoring */\n\t\tstruct x86_pmu_capability cap;\n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tperf_get_x86_pmu_capability(&cap);\n\n\t\t/*\n\t\t * Only support guest architectural pmu on a host\n\t\t * with architectural pmu.\n\t\t */\n\t\tif (!cap.version)\n\t\t\tmemset(&cap, 0, sizeof(cap));\n\n\t\teax.split.version_id = min(cap.version, 2);\n\t\teax.split.num_counters = cap.num_counters_gp;\n\t\teax.split.bit_width = cap.bit_width_gp;\n\t\teax.split.mask_length = cap.events_mask_len;\n\n\t\tedx.split.num_counters_fixed = cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = cap.bit_width_fixed;\n\t\tedx.split.reserved = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\t/* function 0xb has additional index. */\n\tcase 0xb: {\n\t\tint i, level_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until level_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tlevel_type = entry[i - 1].ecx & 0xff00;\n\t\t\tif (!level_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0xd: {\n\t\tint idx, i;\n\t\tu64 supported = kvm_supported_xcr0();\n\n\t\tentry->eax &= supported;\n\t\tentry->ebx = xstate_required_size(supported, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= supported >> 32;\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\tif (!supported)\n\t\t\tbreak;\n\n\t\tfor (idx = 1, i = 1; idx < 64; ++idx) {\n\t\t\tu64 mask = ((u64)1 << idx);\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[i], function, idx);\n\t\t\tif (idx == 1) {\n\t\t\t\tentry[i].eax &= kvm_cpuid_D_1_eax_x86_features;\n\t\t\t\tcpuid_mask(&entry[i].eax, CPUID_D_1_EAX);\n\t\t\t\tentry[i].ebx = 0;\n\t\t\t\tif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\t\t\tentry[i].ebx =\n\t\t\t\t\t\txstate_required_size(supported,\n\t\t\t\t\t\t\t\t     true);\n\t\t\t} else {\n\t\t\t\tif (entry[i].eax == 0 || !(supported & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (WARN_ON_ONCE(entry[i].ecx & 1))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tentry[i].ecx = 0;\n\t\t\tentry[i].edx = 0;\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t\t++i;\n\t\t}\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\n\t\tconst u32 *sigptr = (const u32 *)signature;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x8000001a);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->edx &= kvm_cpuid_8000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_8000_0001_EDX);\n\t\tentry->ecx &= kvm_cpuid_8000_0001_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_8000_0001_ECX);\n\t\tbreak;\n\tcase 0x80000007: /* Advanced power management */\n\t\t/* invariant TSC is CPUID.80000007H:EDX[8] */\n\t\tentry->edx &= (1 << 8);\n\t\t/* mask against host */\n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\tif (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->ebx = entry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\t\tbreak;\n\tcase 0x8000001d:\n\t\tbreak;\n\t/*Add support for Centaur's CPUID instruction*/\n\tcase 0xC0000000:\n\t\t/*Just support up to 0xC0000004 now*/\n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tentry->edx &= kvm_cpuid_C000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_C000_0001_EDX);\n\t\tbreak;\n\tcase 3: /* Processor serial number */\n\tcase 5: /* MONITOR/MWAIT */\n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tkvm_x86_ops->set_supported_cpuid(function, entry);\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}\n\nstatic void cpuid_mask(u32 *word, int wordnum)\n{\n\t*word &= boot_cpu_data.x86_capability[wordnum];\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,\n\t\t\t\t u32 index, int *nent, int maxnent)\n{\n\tint r;\n\tunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\n\tunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n\t\t\t\t? F(GBPAGES) : 0;\n\tunsigned f_lm = F(LM);\n#else\n\tunsigned f_gbpages = 0;\n\tunsigned f_lm = 0;\n#endif\n\tunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\n\tunsigned f_invpcid = kvm_x86_ops->invpcid_supported() ? F(INVPCID) : 0;\n\tunsigned f_mpx = kvm_x86_ops->mpx_supported() ? F(MPX) : 0;\n\tunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\n\n\t/* cpuid 1.edx */\n\tconst u32 kvm_supported_word0_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |\n\t\t0 /* Reserved, DS, ACPI */ | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0 /* HTT, TM, Reserved, PBE */;\n\t/* cpuid 0x80000001.edx */\n\tconst u32 kvm_supported_word1_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* Reserved */ |\n\t\tf_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n\t\t0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW);\n\t/* cpuid 1.ecx */\n\tconst u32 kvm_supported_word4_x86_features =\n\t\t/* NOTE: MONITOR (and MWAIT) are emulated as NOP,\n\t\t * but *not* advertised to guests via CPUID ! */\n\t\tF(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |\n\t\t0 /* DS-CPL, VMX, SMX, EST */ |\n\t\t0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |\n\t\tF(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |\n\t\tF(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |\n\t\tF(F16C) | F(RDRAND);\n\t/* cpuid 0x80000001.ecx */\n\tconst u32 kvm_supported_word6_x86_features =\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |\n\t\t0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);\n\n\t/* cpuid 0xC0000001.edx */\n\tconst u32 kvm_supported_word5_x86_features =\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN);\n\n\t/* cpuid 7.0.ebx */\n\tconst u32 kvm_supported_word9_x86_features =\n\t\tF(FSGSBASE) | F(BMI1) | F(HLE) | F(AVX2) | F(SMEP) |\n\t\tF(BMI2) | F(ERMS) | f_invpcid | F(RTM) | f_mpx | F(RDSEED) |\n\t\tF(ADX) | F(SMAP) | F(AVX512F) | F(AVX512PF) | F(AVX512ER) |\n\t\tF(AVX512CD);\n\n\t/* cpuid 0xD.1.eax */\n\tconst u32 kvm_supported_word10_x86_features =\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\n\n\t/* all calls to cpuid_count() should be made on the same cpu */\n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tif (*nent >= maxnent)\n\t\tgoto out;\n\n\tdo_cpuid_1_ent(entry, function, index);\n\t++*nent;\n\n\tswitch (function) {\n\tcase 0:\n\t\tentry->eax = min(entry->eax, (u32)0xd);\n\t\tbreak;\n\tcase 1:\n\t\tentry->edx &= kvm_supported_word0_x86_features;\n\t\tcpuid_mask(&entry->edx, 0);\n\t\tentry->ecx &= kvm_supported_word4_x86_features;\n\t\tcpuid_mask(&entry->ecx, 4);\n\t\t/* we support x2apic emulation even if host does not support\n\t\t * it since we emulate x2apic in software */\n\t\tentry->ecx |= F(X2APIC);\n\t\tbreak;\n\t/* function 2 entries are STATEFUL. That is, repeated cpuid commands\n\t * may return different values. This forces us to get_cpu() before\n\t * issuing the first command, and also to emulate this annoying behavior\n\t * in kvm_emulate_cpuid() using KVM_CPUID_FLAG_STATE_READ_NEXT */\n\tcase 2: {\n\t\tint t, times = entry->eax & 0xff;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\tentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\n\t\tfor (t = 1; t < times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[t], function, 0);\n\t\t\tentry[t].flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\t/* function 4 has additional index. */\n\tcase 4: {\n\t\tint i, cache_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until cache_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tcache_type = entry[i - 1].eax & 0x1f;\n\t\t\tif (!cache_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 7: {\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* Mask ebx against host capability word 9 */\n\t\tif (index == 0) {\n\t\t\tentry->ebx &= kvm_supported_word9_x86_features;\n\t\t\tcpuid_mask(&entry->ebx, 9);\n\t\t\t// TSC_ADJUST is emulated\n\t\t\tentry->ebx |= F(TSC_ADJUST);\n\t\t} else\n\t\t\tentry->ebx = 0;\n\t\tentry->eax = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 9:\n\t\tbreak;\n\tcase 0xa: { /* Architectural Performance Monitoring */\n\t\tstruct x86_pmu_capability cap;\n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tperf_get_x86_pmu_capability(&cap);\n\n\t\t/*\n\t\t * Only support guest architectural pmu on a host\n\t\t * with architectural pmu.\n\t\t */\n\t\tif (!cap.version)\n\t\t\tmemset(&cap, 0, sizeof(cap));\n\n\t\teax.split.version_id = min(cap.version, 2);\n\t\teax.split.num_counters = cap.num_counters_gp;\n\t\teax.split.bit_width = cap.bit_width_gp;\n\t\teax.split.mask_length = cap.events_mask_len;\n\n\t\tedx.split.num_counters_fixed = cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = cap.bit_width_fixed;\n\t\tedx.split.reserved = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\t/* function 0xb has additional index. */\n\tcase 0xb: {\n\t\tint i, level_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until level_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tlevel_type = entry[i - 1].ecx & 0xff00;\n\t\t\tif (!level_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0xd: {\n\t\tint idx, i;\n\t\tu64 supported = kvm_supported_xcr0();\n\n\t\tentry->eax &= supported;\n\t\tentry->ebx = xstate_required_size(supported, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= supported >> 32;\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\tif (!supported)\n\t\t\tbreak;\n\n\t\tfor (idx = 1, i = 1; idx < 64; ++idx) {\n\t\t\tu64 mask = ((u64)1 << idx);\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[i], function, idx);\n\t\t\tif (idx == 1) {\n\t\t\t\tentry[i].eax &= kvm_supported_word10_x86_features;\n\t\t\t\tentry[i].ebx = 0;\n\t\t\t\tif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\t\t\tentry[i].ebx =\n\t\t\t\t\t\txstate_required_size(supported,\n\t\t\t\t\t\t\t\t     true);\n\t\t\t} else {\n\t\t\t\tif (entry[i].eax == 0 || !(supported & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (WARN_ON_ONCE(entry[i].ecx & 1))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tentry[i].ecx = 0;\n\t\t\tentry[i].edx = 0;\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t\t++i;\n\t\t}\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\n\t\tconst u32 *sigptr = (const u32 *)signature;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x8000001a);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->edx &= kvm_supported_word1_x86_features;\n\t\tcpuid_mask(&entry->edx, 1);\n\t\tentry->ecx &= kvm_supported_word6_x86_features;\n\t\tcpuid_mask(&entry->ecx, 6);\n\t\tbreak;\n\tcase 0x80000007: /* Advanced power management */\n\t\t/* invariant TSC is CPUID.80000007H:EDX[8] */\n\t\tentry->edx &= (1 << 8);\n\t\t/* mask against host */\n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\tif (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->ebx = entry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\t\tbreak;\n\tcase 0x8000001d:\n\t\tbreak;\n\t/*Add support for Centaur's CPUID instruction*/\n\tcase 0xC0000000:\n\t\t/*Just support up to 0xC0000004 now*/\n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tentry->edx &= kvm_supported_word5_x86_features;\n\t\tcpuid_mask(&entry->edx, 5);\n\t\tbreak;\n\tcase 3: /* Processor serial number */\n\tcase 5: /* MONITOR/MWAIT */\n\tcase 6: /* Thermal management */\n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tkvm_x86_ops->set_supported_cpuid(function, entry);\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic inline int __do_cpuid_ent(struct kvm_cpuid_entry2 *entry, u32 function,\n\t\t\t\t u32 index, int *nent, int maxnent)\n{\n\tint r;\n\tunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\n\tunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n\t\t\t\t? F(GBPAGES) : 0;\n\tunsigned f_lm = F(LM);\n#else\n\tunsigned f_gbpages = 0;\n\tunsigned f_lm = 0;\n#endif\n\tunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\n\tunsigned f_invpcid = kvm_x86_ops->invpcid_supported() ? F(INVPCID) : 0;\n\tunsigned f_mpx = kvm_x86_ops->mpx_supported() ? F(MPX) : 0;\n\tunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\n\n\t/* cpuid 1.edx */\n\tconst u32 kvm_supported_word0_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |\n\t\t0 /* Reserved, DS, ACPI */ | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0 /* HTT, TM, Reserved, PBE */;\n\t/* cpuid 0x80000001.edx */\n\tconst u32 kvm_supported_word1_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* Reserved */ |\n\t\tf_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n\t\t0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW);\n\t/* cpuid 1.ecx */\n\tconst u32 kvm_supported_word4_x86_features =\n\t\t/* NOTE: MONITOR (and MWAIT) are emulated as NOP,\n\t\t * but *not* advertised to guests via CPUID ! */\n\t\tF(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |\n\t\t0 /* DS-CPL, VMX, SMX, EST */ |\n\t\t0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |\n\t\tF(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |\n\t\tF(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |\n\t\tF(F16C) | F(RDRAND);\n\t/* cpuid 0x80000001.ecx */\n\tconst u32 kvm_supported_word6_x86_features =\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |\n\t\t0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM);\n\n\t/* cpuid 0xC0000001.edx */\n\tconst u32 kvm_supported_word5_x86_features =\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN);\n\n\t/* cpuid 7.0.ebx */\n\tconst u32 kvm_supported_word9_x86_features =\n\t\tF(FSGSBASE) | F(BMI1) | F(HLE) | F(AVX2) | F(SMEP) |\n\t\tF(BMI2) | F(ERMS) | f_invpcid | F(RTM) | f_mpx | F(RDSEED) |\n\t\tF(ADX) | F(SMAP) | F(AVX512F) | F(AVX512PF) | F(AVX512ER) |\n\t\tF(AVX512CD);\n\n\t/* cpuid 0xD.1.eax */\n\tconst u32 kvm_supported_word10_x86_features =\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\n\n\t/* all calls to cpuid_count() should be made on the same cpu */\n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tif (*nent >= maxnent)\n\t\tgoto out;\n\n\tdo_cpuid_1_ent(entry, function, index);\n\t++*nent;\n\n\tswitch (function) {\n\tcase 0:\n\t\tentry->eax = min(entry->eax, (u32)0xd);\n\t\tbreak;\n\tcase 1:\n\t\tentry->edx &= kvm_supported_word0_x86_features;\n\t\tcpuid_mask(&entry->edx, 0);\n\t\tentry->ecx &= kvm_supported_word4_x86_features;\n\t\tcpuid_mask(&entry->ecx, 4);\n\t\t/* we support x2apic emulation even if host does not support\n\t\t * it since we emulate x2apic in software */\n\t\tentry->ecx |= F(X2APIC);\n\t\tbreak;\n\t/* function 2 entries are STATEFUL. That is, repeated cpuid commands\n\t * may return different values. This forces us to get_cpu() before\n\t * issuing the first command, and also to emulate this annoying behavior\n\t * in kvm_emulate_cpuid() using KVM_CPUID_FLAG_STATE_READ_NEXT */\n\tcase 2: {\n\t\tint t, times = entry->eax & 0xff;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\tentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\n\t\tfor (t = 1; t < times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[t], function, 0);\n\t\t\tentry[t].flags |= KVM_CPUID_FLAG_STATEFUL_FUNC;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\t/* function 4 has additional index. */\n\tcase 4: {\n\t\tint i, cache_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until cache_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tcache_type = entry[i - 1].eax & 0x1f;\n\t\t\tif (!cache_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 7: {\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* Mask ebx against host capability word 9 */\n\t\tif (index == 0) {\n\t\t\tentry->ebx &= kvm_supported_word9_x86_features;\n\t\t\tcpuid_mask(&entry->ebx, 9);\n\t\t\t// TSC_ADJUST is emulated\n\t\t\tentry->ebx |= F(TSC_ADJUST);\n\t\t} else\n\t\t\tentry->ebx = 0;\n\t\tentry->eax = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 9:\n\t\tbreak;\n\tcase 0xa: { /* Architectural Performance Monitoring */\n\t\tstruct x86_pmu_capability cap;\n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tperf_get_x86_pmu_capability(&cap);\n\n\t\t/*\n\t\t * Only support guest architectural pmu on a host\n\t\t * with architectural pmu.\n\t\t */\n\t\tif (!cap.version)\n\t\t\tmemset(&cap, 0, sizeof(cap));\n\n\t\teax.split.version_id = min(cap.version, 2);\n\t\teax.split.num_counters = cap.num_counters_gp;\n\t\teax.split.bit_width = cap.bit_width_gp;\n\t\teax.split.mask_length = cap.events_mask_len;\n\n\t\tedx.split.num_counters_fixed = cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = cap.bit_width_fixed;\n\t\tedx.split.reserved = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\t/* function 0xb has additional index. */\n\tcase 0xb: {\n\t\tint i, level_type;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t/* read more entries until level_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tlevel_type = entry[i - 1].ecx & 0xff00;\n\t\t\tif (!level_type)\n\t\t\t\tbreak;\n\t\t\tdo_cpuid_1_ent(&entry[i], function, i);\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0xd: {\n\t\tint idx, i;\n\t\tu64 supported = kvm_supported_xcr0();\n\n\t\tentry->eax &= supported;\n\t\tentry->ebx = xstate_required_size(supported, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= supported >> 32;\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\tif (!supported)\n\t\t\tbreak;\n\n\t\tfor (idx = 1, i = 1; idx < 64; ++idx) {\n\t\t\tu64 mask = ((u64)1 << idx);\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_cpuid_1_ent(&entry[i], function, idx);\n\t\t\tif (idx == 1) {\n\t\t\t\tentry[i].eax &= kvm_supported_word10_x86_features;\n\t\t\t\tcpuid_mask(&entry[i].eax, 10);\n\t\t\t\tentry[i].ebx = 0;\n\t\t\t\tif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\t\t\tentry[i].ebx =\n\t\t\t\t\t\txstate_required_size(supported,\n\t\t\t\t\t\t\t\t     true);\n\t\t\t} else {\n\t\t\t\tif (entry[i].eax == 0 || !(supported & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (WARN_ON_ONCE(entry[i].ecx & 1))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tentry[i].ecx = 0;\n\t\t\tentry[i].edx = 0;\n\t\t\tentry[i].flags |=\n\t\t\t       KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\t\t++*nent;\n\t\t\t++i;\n\t\t}\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\n\t\tconst u32 *sigptr = (const u32 *)signature;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x8000001a);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->edx &= kvm_supported_word1_x86_features;\n\t\tcpuid_mask(&entry->edx, 1);\n\t\tentry->ecx &= kvm_supported_word6_x86_features;\n\t\tcpuid_mask(&entry->ecx, 6);\n\t\tbreak;\n\tcase 0x80000007: /* Advanced power management */\n\t\t/* invariant TSC is CPUID.80000007H:EDX[8] */\n\t\tentry->edx &= (1 << 8);\n\t\t/* mask against host */\n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\tif (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->ebx = entry->edx = 0;\n\t\tbreak;\n\t}\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\t\tbreak;\n\tcase 0x8000001d:\n\t\tbreak;\n\t/*Add support for Centaur's CPUID instruction*/\n\tcase 0xC0000000:\n\t\t/*Just support up to 0xC0000004 now*/\n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tentry->edx &= kvm_supported_word5_x86_features;\n\t\tcpuid_mask(&entry->edx, 5);\n\t\tbreak;\n\tcase 3: /* Processor serial number */\n\tcase 5: /* MONITOR/MWAIT */\n\tcase 6: /* Thermal management */\n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tkvm_x86_ops->set_supported_cpuid(function, entry);\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}\n\nstatic void cpuid_mask(u32 *word, int wordnum)\n{\n\t*word &= boot_cpu_data.x86_capability[wordnum];\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nint\nSMB2_ioctl(const unsigned int xid, struct cifs_tcon *tcon, u64 persistent_fid,\n\t   u64 volatile_fid, u32 opcode, bool is_fsctl, bool use_ipc,\n\t   char *in_data, u32 indatalen,\n\t   char **out_data, u32 *plen /* returned data len */)\n{\n\tstruct smb2_ioctl_req *req;\n\tstruct smb2_ioctl_rsp *rsp;\n\tstruct smb2_sync_hdr *shdr;\n\tstruct cifs_ses *ses;\n\tstruct kvec iov[2];\n\tstruct kvec rsp_iov;\n\tint resp_buftype;\n\tint n_iov;\n\tint rc = 0;\n\tint flags = 0;\n\n\tcifs_dbg(FYI, \"SMB2 IOCTL\\n\");\n\n\tif (out_data != NULL)\n\t\t*out_data = NULL;\n\n\t/* zero out returned data len, in case of error */\n\tif (plen)\n\t\t*plen = 0;\n\n\tif (tcon)\n\t\tses = tcon->ses;\n\telse\n\t\treturn -EIO;\n\n\tif (!ses || !(ses->server))\n\t\treturn -EIO;\n\n\trc = small_smb2_init(SMB2_IOCTL, tcon, (void **) &req);\n\tif (rc)\n\t\treturn rc;\n\n\tif (use_ipc) {\n\t\tif (ses->ipc_tid == 0) {\n\t\t\tcifs_small_buf_release(req);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\n\t\tcifs_dbg(FYI, \"replacing tid 0x%x with IPC tid 0x%x\\n\",\n\t\t\t req->hdr.sync_hdr.TreeId, ses->ipc_tid);\n\t\treq->hdr.sync_hdr.TreeId = ses->ipc_tid;\n\t}\n\tif (encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->CtlCode = cpu_to_le32(opcode);\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\n\tif (indatalen) {\n\t\treq->InputCount = cpu_to_le32(indatalen);\n\t\t/* do not set InputOffset if no input data */\n\t\treq->InputOffset =\n\t\t       cpu_to_le32(offsetof(struct smb2_ioctl_req, Buffer) - 4);\n\t\tiov[1].iov_base = in_data;\n\t\tiov[1].iov_len = indatalen;\n\t\tn_iov = 2;\n\t} else\n\t\tn_iov = 1;\n\n\treq->OutputOffset = 0;\n\treq->OutputCount = 0; /* MBZ */\n\n\t/*\n\t * Could increase MaxOutputResponse, but that would require more\n\t * than one credit. Windows typically sets this smaller, but for some\n\t * ioctls it may be useful to allow server to send more. No point\n\t * limiting what the server can send as long as fits in one credit\n\t * Unfortunately - we can not handle more than CIFS_MAX_MSG_SIZE\n\t * (by default, note that it can be overridden to make max larger)\n\t * in responses (except for read responses which can be bigger.\n\t * We may want to bump this limit up\n\t */\n\treq->MaxOutputResponse = cpu_to_le32(CIFSMaxBufSize);\n\n\tif (is_fsctl)\n\t\treq->Flags = cpu_to_le32(SMB2_0_IOCTL_IS_FSCTL);\n\telse\n\t\treq->Flags = 0;\n\n\tiov[0].iov_base = (char *)req;\n\n\t/*\n\t * If no input data, the size of ioctl struct in\n\t * protocol spec still includes a 1 byte data buffer,\n\t * but if input data passed to ioctl, we do not\n\t * want to double count this, so we do not send\n\t * the dummy one byte of data in iovec[0] if sending\n\t * input data (in iovec[1]). We also must add 4 bytes\n\t * in first iovec to allow for rfc1002 length field.\n\t */\n\n\tif (indatalen) {\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4 - 1;\n\t\tinc_rfc1001_len(req, indatalen - 1);\n\t} else\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4;\n\n\n\trc = SendReceive2(xid, ses, iov, n_iov, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_ioctl_rsp *)rsp_iov.iov_base;\n\n\tif ((rc != 0) && (rc != -EINVAL)) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\tgoto ioctl_exit;\n\t} else if (rc == -EINVAL) {\n\t\tif ((opcode != FSCTL_SRV_COPYCHUNK_WRITE) &&\n\t\t    (opcode != FSCTL_SRV_COPYCHUNK)) {\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\t\tgoto ioctl_exit;\n\t\t}\n\t}\n\n\t/* check if caller wants to look at return data or just return rc */\n\tif ((plen == NULL) || (out_data == NULL))\n\t\tgoto ioctl_exit;\n\n\t*plen = le32_to_cpu(rsp->OutputCount);\n\n\t/* We check for obvious errors in the output buffer length and offset */\n\tif (*plen == 0)\n\t\tgoto ioctl_exit; /* server returned no data */\n\telse if (*plen > 0xFF00) {\n\t\tcifs_dbg(VFS, \"srv returned invalid ioctl length: %d\\n\", *plen);\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\tif (get_rfc1002_length(rsp) < le32_to_cpu(rsp->OutputOffset) + *plen) {\n\t\tcifs_dbg(VFS, \"Malformed ioctl resp: len %d offset %d\\n\", *plen,\n\t\t\tle32_to_cpu(rsp->OutputOffset));\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\t*out_data = kmalloc(*plen, GFP_KERNEL);\n\tif (*out_data == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto ioctl_exit;\n\t}\n\n\tshdr = get_sync_hdr(rsp);\n\tmemcpy(*out_data, (char *)shdr + le32_to_cpu(rsp->OutputOffset), *plen);\nioctl_exit:\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nint\nSMB2_ioctl(const unsigned int xid, struct cifs_tcon *tcon, u64 persistent_fid,\n\t   u64 volatile_fid, u32 opcode, bool is_fsctl, bool use_ipc,\n\t   char *in_data, u32 indatalen,\n\t   char **out_data, u32 *plen /* returned data len */)\n{\n\tstruct smb2_ioctl_req *req;\n\tstruct smb2_ioctl_rsp *rsp;\n\tstruct smb2_sync_hdr *shdr;\n\tstruct cifs_ses *ses;\n\tstruct kvec iov[2];\n\tstruct kvec rsp_iov;\n\tint resp_buftype;\n\tint n_iov;\n\tint rc = 0;\n\tint flags = 0;\n\n\tcifs_dbg(FYI, \"SMB2 IOCTL\\n\");\n\n\tif (out_data != NULL)\n\t\t*out_data = NULL;\n\n\t/* zero out returned data len, in case of error */\n\tif (plen)\n\t\t*plen = 0;\n\n\tif (tcon)\n\t\tses = tcon->ses;\n\telse\n\t\treturn -EIO;\n\n\tif (!ses || !(ses->server))\n\t\treturn -EIO;\n\n\trc = small_smb2_init(SMB2_IOCTL, tcon, (void **) &req);\n\tif (rc)\n\t\treturn rc;\n\n\tif (use_ipc) {\n\t\tif (ses->ipc_tid == 0) {\n\t\t\tcifs_small_buf_release(req);\n\t\t\treturn -ENOTCONN;\n\t\t}\n\n\t\tcifs_dbg(FYI, \"replacing tid 0x%x with IPC tid 0x%x\\n\",\n\t\t\t req->hdr.sync_hdr.TreeId, ses->ipc_tid);\n\t\treq->hdr.sync_hdr.TreeId = ses->ipc_tid;\n\t}\n\tif (encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->CtlCode = cpu_to_le32(opcode);\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\n\tif (indatalen) {\n\t\treq->InputCount = cpu_to_le32(indatalen);\n\t\t/* do not set InputOffset if no input data */\n\t\treq->InputOffset =\n\t\t       cpu_to_le32(offsetof(struct smb2_ioctl_req, Buffer) - 4);\n\t\tiov[1].iov_base = in_data;\n\t\tiov[1].iov_len = indatalen;\n\t\tn_iov = 2;\n\t} else\n\t\tn_iov = 1;\n\n\treq->OutputOffset = 0;\n\treq->OutputCount = 0; /* MBZ */\n\n\t/*\n\t * Could increase MaxOutputResponse, but that would require more\n\t * than one credit. Windows typically sets this smaller, but for some\n\t * ioctls it may be useful to allow server to send more. No point\n\t * limiting what the server can send as long as fits in one credit\n\t * Unfortunately - we can not handle more than CIFS_MAX_MSG_SIZE\n\t * (by default, note that it can be overridden to make max larger)\n\t * in responses (except for read responses which can be bigger.\n\t * We may want to bump this limit up\n\t */\n\treq->MaxOutputResponse = cpu_to_le32(CIFSMaxBufSize);\n\n\tif (is_fsctl)\n\t\treq->Flags = cpu_to_le32(SMB2_0_IOCTL_IS_FSCTL);\n\telse\n\t\treq->Flags = 0;\n\n\tiov[0].iov_base = (char *)req;\n\n\t/*\n\t * If no input data, the size of ioctl struct in\n\t * protocol spec still includes a 1 byte data buffer,\n\t * but if input data passed to ioctl, we do not\n\t * want to double count this, so we do not send\n\t * the dummy one byte of data in iovec[0] if sending\n\t * input data (in iovec[1]). We also must add 4 bytes\n\t * in first iovec to allow for rfc1002 length field.\n\t */\n\n\tif (indatalen) {\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4 - 1;\n\t\tinc_rfc1001_len(req, indatalen - 1);\n\t} else\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4;\n\n\t/* validate negotiate request must be signed - see MS-SMB2 3.2.5.5 */\n\tif (opcode == FSCTL_VALIDATE_NEGOTIATE_INFO)\n\t\treq->hdr.sync_hdr.Flags |= SMB2_FLAGS_SIGNED;\n\n\trc = SendReceive2(xid, ses, iov, n_iov, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_ioctl_rsp *)rsp_iov.iov_base;\n\n\tif ((rc != 0) && (rc != -EINVAL)) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\tgoto ioctl_exit;\n\t} else if (rc == -EINVAL) {\n\t\tif ((opcode != FSCTL_SRV_COPYCHUNK_WRITE) &&\n\t\t    (opcode != FSCTL_SRV_COPYCHUNK)) {\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\t\tgoto ioctl_exit;\n\t\t}\n\t}\n\n\t/* check if caller wants to look at return data or just return rc */\n\tif ((plen == NULL) || (out_data == NULL))\n\t\tgoto ioctl_exit;\n\n\t*plen = le32_to_cpu(rsp->OutputCount);\n\n\t/* We check for obvious errors in the output buffer length and offset */\n\tif (*plen == 0)\n\t\tgoto ioctl_exit; /* server returned no data */\n\telse if (*plen > 0xFF00) {\n\t\tcifs_dbg(VFS, \"srv returned invalid ioctl length: %d\\n\", *plen);\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\tif (get_rfc1002_length(rsp) < le32_to_cpu(rsp->OutputOffset) + *plen) {\n\t\tcifs_dbg(VFS, \"Malformed ioctl resp: len %d offset %d\\n\", *plen,\n\t\t\tle32_to_cpu(rsp->OutputOffset));\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\t*out_data = kmalloc(*plen, GFP_KERNEL);\n\tif (*out_data == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto ioctl_exit;\n\t}\n\n\tshdr = get_sync_hdr(rsp);\n\tmemcpy(*out_data, (char *)shdr + le32_to_cpu(rsp->OutputOffset), *plen);\nioctl_exit:\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nint\nSMB2_ioctl(const unsigned int xid, struct cifs_tcon *tcon, u64 persistent_fid,\n\t   u64 volatile_fid, u32 opcode, bool is_fsctl, char *in_data,\n\t   u32 indatalen, char **out_data, u32 *plen /* returned data len */)\n{\n\tstruct smb2_ioctl_req *req;\n\tstruct smb2_ioctl_rsp *rsp;\n\tstruct TCP_Server_Info *server;\n\tstruct cifs_ses *ses = tcon->ses;\n\tstruct kvec iov[2];\n\tint resp_buftype;\n\tint num_iovecs;\n\tint rc = 0;\n\n\tcifs_dbg(FYI, \"SMB2 IOCTL\\n\");\n\n\t*out_data = NULL;\n\t/* zero out returned data len, in case of error */\n\tif (plen)\n\t\t*plen = 0;\n\n\tif (ses && (ses->server))\n\t\tserver = ses->server;\n\telse\n\t\treturn -EIO;\n\n\trc = small_smb2_init(SMB2_IOCTL, tcon, (void **) &req);\n\tif (rc)\n\t\treturn rc;\n\n\treq->CtlCode = cpu_to_le32(opcode);\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\n\tif (indatalen) {\n\t\treq->InputCount = cpu_to_le32(indatalen);\n\t\t/* do not set InputOffset if no input data */\n\t\treq->InputOffset =\n\t\t       cpu_to_le32(offsetof(struct smb2_ioctl_req, Buffer) - 4);\n\t\tiov[1].iov_base = in_data;\n\t\tiov[1].iov_len = indatalen;\n\t\tnum_iovecs = 2;\n\t} else\n\t\tnum_iovecs = 1;\n\n\treq->OutputOffset = 0;\n\treq->OutputCount = 0; /* MBZ */\n\n\t/*\n\t * Could increase MaxOutputResponse, but that would require more\n\t * than one credit. Windows typically sets this smaller, but for some\n\t * ioctls it may be useful to allow server to send more. No point\n\t * limiting what the server can send as long as fits in one credit\n\t * Unfortunately - we can not handle more than CIFS_MAX_MSG_SIZE\n\t * (by default, note that it can be overridden to make max larger)\n\t * in responses (except for read responses which can be bigger.\n\t * We may want to bump this limit up\n\t */\n\treq->MaxOutputResponse = cpu_to_le32(CIFSMaxBufSize);\n\n\tif (is_fsctl)\n\t\treq->Flags = cpu_to_le32(SMB2_0_IOCTL_IS_FSCTL);\n\telse\n\t\treq->Flags = 0;\n\n\tiov[0].iov_base = (char *)req;\n\n\t/*\n\t * If no input data, the size of ioctl struct in\n\t * protocol spec still includes a 1 byte data buffer,\n\t * but if input data passed to ioctl, we do not\n\t * want to double count this, so we do not send\n\t * the dummy one byte of data in iovec[0] if sending\n\t * input data (in iovec[1]). We also must add 4 bytes\n\t * in first iovec to allow for rfc1002 length field.\n\t */\n\n\tif (indatalen) {\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4 - 1;\n\t\tinc_rfc1001_len(req, indatalen - 1);\n\t} else\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4;\n\n\n\trc = SendReceive2(xid, ses, iov, num_iovecs, &resp_buftype, 0);\n\trsp = (struct smb2_ioctl_rsp *)iov[0].iov_base;\n\n\tif ((rc != 0) && (rc != -EINVAL)) {\n\t\tif (tcon)\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\tgoto ioctl_exit;\n\t} else if (rc == -EINVAL) {\n\t\tif ((opcode != FSCTL_SRV_COPYCHUNK_WRITE) &&\n\t\t    (opcode != FSCTL_SRV_COPYCHUNK)) {\n\t\t\tif (tcon)\n\t\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\t\tgoto ioctl_exit;\n\t\t}\n\t}\n\n\t/* check if caller wants to look at return data or just return rc */\n\tif ((plen == NULL) || (out_data == NULL))\n\t\tgoto ioctl_exit;\n\n\t*plen = le32_to_cpu(rsp->OutputCount);\n\n\t/* We check for obvious errors in the output buffer length and offset */\n\tif (*plen == 0)\n\t\tgoto ioctl_exit; /* server returned no data */\n\telse if (*plen > 0xFF00) {\n\t\tcifs_dbg(VFS, \"srv returned invalid ioctl length: %d\\n\", *plen);\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\tif (get_rfc1002_length(rsp) < le32_to_cpu(rsp->OutputOffset) + *plen) {\n\t\tcifs_dbg(VFS, \"Malformed ioctl resp: len %d offset %d\\n\", *plen,\n\t\t\tle32_to_cpu(rsp->OutputOffset));\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\t*out_data = kmalloc(*plen, GFP_KERNEL);\n\tif (*out_data == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto ioctl_exit;\n\t}\n\n\tmemcpy(*out_data, rsp->hdr.ProtocolId + le32_to_cpu(rsp->OutputOffset),\n\t       *plen);\nioctl_exit:\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nint\nSMB2_ioctl(const unsigned int xid, struct cifs_tcon *tcon, u64 persistent_fid,\n\t   u64 volatile_fid, u32 opcode, bool is_fsctl, char *in_data,\n\t   u32 indatalen, char **out_data, u32 *plen /* returned data len */)\n{\n\tstruct smb2_ioctl_req *req;\n\tstruct smb2_ioctl_rsp *rsp;\n\tstruct TCP_Server_Info *server;\n\tstruct cifs_ses *ses = tcon->ses;\n\tstruct kvec iov[2];\n\tint resp_buftype;\n\tint num_iovecs;\n\tint rc = 0;\n\n\tcifs_dbg(FYI, \"SMB2 IOCTL\\n\");\n\n\t*out_data = NULL;\n\t/* zero out returned data len, in case of error */\n\tif (plen)\n\t\t*plen = 0;\n\n\tif (ses && (ses->server))\n\t\tserver = ses->server;\n\telse\n\t\treturn -EIO;\n\n\trc = small_smb2_init(SMB2_IOCTL, tcon, (void **) &req);\n\tif (rc)\n\t\treturn rc;\n\n\treq->CtlCode = cpu_to_le32(opcode);\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\n\tif (indatalen) {\n\t\treq->InputCount = cpu_to_le32(indatalen);\n\t\t/* do not set InputOffset if no input data */\n\t\treq->InputOffset =\n\t\t       cpu_to_le32(offsetof(struct smb2_ioctl_req, Buffer) - 4);\n\t\tiov[1].iov_base = in_data;\n\t\tiov[1].iov_len = indatalen;\n\t\tnum_iovecs = 2;\n\t} else\n\t\tnum_iovecs = 1;\n\n\treq->OutputOffset = 0;\n\treq->OutputCount = 0; /* MBZ */\n\n\t/*\n\t * Could increase MaxOutputResponse, but that would require more\n\t * than one credit. Windows typically sets this smaller, but for some\n\t * ioctls it may be useful to allow server to send more. No point\n\t * limiting what the server can send as long as fits in one credit\n\t * Unfortunately - we can not handle more than CIFS_MAX_MSG_SIZE\n\t * (by default, note that it can be overridden to make max larger)\n\t * in responses (except for read responses which can be bigger.\n\t * We may want to bump this limit up\n\t */\n\treq->MaxOutputResponse = cpu_to_le32(CIFSMaxBufSize);\n\n\tif (is_fsctl)\n\t\treq->Flags = cpu_to_le32(SMB2_0_IOCTL_IS_FSCTL);\n\telse\n\t\treq->Flags = 0;\n\n\tiov[0].iov_base = (char *)req;\n\n\t/*\n\t * If no input data, the size of ioctl struct in\n\t * protocol spec still includes a 1 byte data buffer,\n\t * but if input data passed to ioctl, we do not\n\t * want to double count this, so we do not send\n\t * the dummy one byte of data in iovec[0] if sending\n\t * input data (in iovec[1]). We also must add 4 bytes\n\t * in first iovec to allow for rfc1002 length field.\n\t */\n\n\tif (indatalen) {\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4 - 1;\n\t\tinc_rfc1001_len(req, indatalen - 1);\n\t} else\n\t\tiov[0].iov_len = get_rfc1002_length(req) + 4;\n\n\t/* validate negotiate request must be signed - see MS-SMB2 3.2.5.5 */\n\tif (opcode == FSCTL_VALIDATE_NEGOTIATE_INFO)\n\t\treq->hdr.Flags |= SMB2_FLAGS_SIGNED;\n\n\trc = SendReceive2(xid, ses, iov, num_iovecs, &resp_buftype, 0);\n\trsp = (struct smb2_ioctl_rsp *)iov[0].iov_base;\n\n\tif ((rc != 0) && (rc != -EINVAL)) {\n\t\tif (tcon)\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\tgoto ioctl_exit;\n\t} else if (rc == -EINVAL) {\n\t\tif ((opcode != FSCTL_SRV_COPYCHUNK_WRITE) &&\n\t\t    (opcode != FSCTL_SRV_COPYCHUNK)) {\n\t\t\tif (tcon)\n\t\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\t\tgoto ioctl_exit;\n\t\t}\n\t}\n\n\t/* check if caller wants to look at return data or just return rc */\n\tif ((plen == NULL) || (out_data == NULL))\n\t\tgoto ioctl_exit;\n\n\t*plen = le32_to_cpu(rsp->OutputCount);\n\n\t/* We check for obvious errors in the output buffer length and offset */\n\tif (*plen == 0)\n\t\tgoto ioctl_exit; /* server returned no data */\n\telse if (*plen > 0xFF00) {\n\t\tcifs_dbg(VFS, \"srv returned invalid ioctl length: %d\\n\", *plen);\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\tif (get_rfc1002_length(rsp) < le32_to_cpu(rsp->OutputOffset) + *plen) {\n\t\tcifs_dbg(VFS, \"Malformed ioctl resp: len %d offset %d\\n\", *plen,\n\t\t\tle32_to_cpu(rsp->OutputOffset));\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\t*out_data = kmalloc(*plen, GFP_KERNEL);\n\tif (*out_data == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto ioctl_exit;\n\t}\n\n\tmemcpy(*out_data, rsp->hdr.ProtocolId + le32_to_cpu(rsp->OutputOffset),\n\t       *plen);\nioctl_exit:\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int intel_pstate_cpu_init(struct cpufreq_policy *policy)\n{\n\tstruct cpudata *cpu;\n\tint rc;\n\n\trc = intel_pstate_init_cpu(policy->cpu);\n\tif (rc)\n\t\treturn rc;\n\n\tcpu = all_cpu_data[policy->cpu];\n\n\tif (limits->min_perf_pct == 100 && limits->max_perf_pct == 100)\n\t\tpolicy->policy = CPUFREQ_POLICY_PERFORMANCE;\n\telse\n\t\tpolicy->policy = CPUFREQ_POLICY_POWERSAVE;\n\n\tpolicy->min = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tpolicy->max = cpu->pstate.turbo_pstate * cpu->pstate.scaling;\n\n\t/* cpuinfo and default policy values */\n\tpolicy->cpuinfo.min_freq = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tpolicy->cpuinfo.max_freq =\n\t\tcpu->pstate.turbo_pstate * cpu->pstate.scaling;\n\tintel_pstate_init_acpi_perf_limits(policy);\n\tpolicy->cpuinfo.transition_latency = CPUFREQ_ETERNAL;\n\tcpumask_set_cpu(policy->cpu, policy->cpus);\n\n\treturn 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic inline void update_turbo_state(void)\n{\n\tu64 misc_en;\n\tstruct cpudata *cpu;\n\n\tcpu = all_cpu_data[0];\n\trdmsrl(MSR_IA32_MISC_ENABLE, misc_en);\n\tlimits->turbo_disabled =\n\t\t(misc_en & MSR_IA32_MISC_ENABLE_TURBO_DISABLE ||\n\t\t cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);\n}\n\nstatic int intel_pstate_cpu_init(struct cpufreq_policy *policy)\n{\n\tstruct cpudata *cpu;\n\tint rc;\n\n\trc = intel_pstate_init_cpu(policy->cpu);\n\tif (rc)\n\t\treturn rc;\n\n\tcpu = all_cpu_data[policy->cpu];\n\n\tif (limits->min_perf_pct == 100 && limits->max_perf_pct == 100)\n\t\tpolicy->policy = CPUFREQ_POLICY_PERFORMANCE;\n\telse\n\t\tpolicy->policy = CPUFREQ_POLICY_POWERSAVE;\n\n\tpolicy->min = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tpolicy->max = cpu->pstate.turbo_pstate * cpu->pstate.scaling;\n\n\t/* cpuinfo and default policy values */\n\tpolicy->cpuinfo.min_freq = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tupdate_turbo_state();\n\tpolicy->cpuinfo.max_freq = limits->turbo_disabled ?\n\t\t\tcpu->pstate.max_pstate : cpu->pstate.turbo_pstate;\n\tpolicy->cpuinfo.max_freq *= cpu->pstate.scaling;\n\n\tintel_pstate_init_acpi_perf_limits(policy);\n\tpolicy->cpuinfo.transition_latency = CPUFREQ_ETERNAL;\n\tcpumask_set_cpu(policy->cpu, policy->cpus);\n\n\treturn 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int intel_pstate_cpu_init(struct cpufreq_policy *policy)\n{\n\tstruct cpudata *cpu;\n\tint rc;\n\n\trc = intel_pstate_init_cpu(policy->cpu);\n\tif (rc)\n\t\treturn rc;\n\n\tcpu = all_cpu_data[policy->cpu];\n\n\tif (limits.min_perf_pct == 100 && limits.max_perf_pct == 100)\n\t\tpolicy->policy = CPUFREQ_POLICY_PERFORMANCE;\n\telse\n\t\tpolicy->policy = CPUFREQ_POLICY_POWERSAVE;\n\n\tpolicy->min = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tpolicy->max = cpu->pstate.turbo_pstate * cpu->pstate.scaling;\n\n\t/* cpuinfo and default policy values */\n\tpolicy->cpuinfo.min_freq = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tpolicy->cpuinfo.max_freq =\n\t\tcpu->pstate.turbo_pstate * cpu->pstate.scaling;\n\tpolicy->cpuinfo.transition_latency = CPUFREQ_ETERNAL;\n\tcpumask_set_cpu(policy->cpu, policy->cpus);\n\n\treturn 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic inline void update_turbo_state(void)\n{\n\tu64 misc_en;\n\tstruct cpudata *cpu;\n\n\tcpu = all_cpu_data[0];\n\trdmsrl(MSR_IA32_MISC_ENABLE, misc_en);\n\tlimits.turbo_disabled =\n\t\t(misc_en & MSR_IA32_MISC_ENABLE_TURBO_DISABLE ||\n\t\t cpu->pstate.max_pstate == cpu->pstate.turbo_pstate);\n}\n\nstatic int intel_pstate_cpu_init(struct cpufreq_policy *policy)\n{\n\tstruct cpudata *cpu;\n\tint rc;\n\n\trc = intel_pstate_init_cpu(policy->cpu);\n\tif (rc)\n\t\treturn rc;\n\n\tcpu = all_cpu_data[policy->cpu];\n\n\tif (limits.min_perf_pct == 100 && limits.max_perf_pct == 100)\n\t\tpolicy->policy = CPUFREQ_POLICY_PERFORMANCE;\n\telse\n\t\tpolicy->policy = CPUFREQ_POLICY_POWERSAVE;\n\n\tpolicy->min = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tpolicy->max = cpu->pstate.turbo_pstate * cpu->pstate.scaling;\n\n\t/* cpuinfo and default policy values */\n\tpolicy->cpuinfo.min_freq = cpu->pstate.min_pstate * cpu->pstate.scaling;\n\tupdate_turbo_state();\n\tpolicy->cpuinfo.max_freq = limits.turbo_disabled ?\n\t\t\tcpu->pstate.max_pstate : cpu->pstate.turbo_pstate;\n\tpolicy->cpuinfo.max_freq *= cpu->pstate.scaling;\n\n\tpolicy->cpuinfo.transition_latency = CPUFREQ_ETERNAL;\n\tcpumask_set_cpu(policy->cpu, policy->cpus);\n\n\treturn 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic bool fs_fully_visible(struct file_system_type *type, int *new_mnt_flags)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tif (unlikely(!ns))\n\t\treturn false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tint mnt_flags;\n\n\t\tif (mnt->mnt.mnt_sb->s_type != type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* Read the mount flags and filter out flags that\n\t\t * may safely be ignored.\n\t\t */\n\t\tmnt_flags = mnt->mnt.mnt_flags;\n\t\tif (mnt->mnt.mnt_sb->s_iflags & SB_I_NOEXEC)\n\t\t\tmnt_flags &= ~(MNT_LOCK_NOSUID | MNT_LOCK_NOEXEC);\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_NODEV) &&\n\t\t    !(new_flags & MNT_NODEV))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_NOSUID) &&\n\t\t    !(new_flags & MNT_NOSUID))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_NOEXEC) &&\n\t\t    !(new_flags & MNT_NOEXEC))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t       MNT_LOCK_NODEV    | \\\n\t\t\t\t\t       MNT_LOCK_NOSUID   | \\\n\t\t\t\t\t       MNT_LOCK_NOEXEC   | \\\n\t\t\t\t\t       MNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic bool fs_fully_visible(struct file_system_type *type, int *new_mnt_flags)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tif (unlikely(!ns))\n\t\treturn false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tint mnt_flags;\n\n\t\tif (mnt->mnt.mnt_sb->s_type != type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* Read the mount flags and filter out flags that\n\t\t * may safely be ignored.\n\t\t */\n\t\tmnt_flags = mnt->mnt.mnt_flags;\n\t\tif (mnt->mnt.mnt_sb->s_iflags & SB_I_NOEXEC)\n\t\t\tmnt_flags &= ~(MNT_LOCK_NOSUID | MNT_LOCK_NOEXEC);\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_NODEV) &&\n\t\t    !(new_flags & MNT_NODEV))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_NOSUID) &&\n\t\t    !(new_flags & MNT_NOSUID))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_NOEXEC) &&\n\t\t    !(new_flags & MNT_NOEXEC))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(child->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t       MNT_LOCK_NODEV    | \\\n\t\t\t\t\t       MNT_LOCK_NOSUID   | \\\n\t\t\t\t\t       MNT_LOCK_NOEXEC   | \\\n\t\t\t\t\t       MNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic bool fs_fully_visible(struct file_system_type *type, int *new_mnt_flags)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tif (unlikely(!ns))\n\t\treturn false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tif (mnt->mnt.mnt_sb->s_type != type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt->mnt.mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NODEV) &&\n\t\t    !(new_flags & MNT_NODEV))\n\t\t\tcontinue;\n\t\tif ((mnt->mnt.mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt->mnt.mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(mnt->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt->mnt.mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t\t\tMNT_LOCK_NODEV    | \\\n\t\t\t\t\t\t\tMNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic bool fs_fully_visible(struct file_system_type *type, int *new_mnt_flags)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tif (unlikely(!ns))\n\t\treturn false;\n\n\tdown_read(&namespace_sem);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tif (mnt->mnt.mnt_sb->s_type != type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt->mnt.mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt->mnt.mnt_flags & MNT_LOCK_NODEV) &&\n\t\t    !(new_flags & MNT_NODEV))\n\t\t\tcontinue;\n\t\tif ((mnt->mnt.mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt->mnt.mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(child->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt->mnt.mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t\t\tMNT_LOCK_NODEV    | \\\n\t\t\t\t\t\t\tMNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tup_read(&namespace_sem);\n\treturn visible;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nvoid gpiochip_remove(struct gpio_chip *chip)\n{\n\tunsigned long\tflags;\n\tunsigned\tid;\n\n\tgpiochip_irqchip_remove(chip);\n\n\tacpi_gpiochip_remove(chip);\n\tgpiochip_remove_pin_ranges(chip);\n\tof_gpiochip_remove(chip);\n\n\tspin_lock_irqsave(&gpio_lock, flags);\n\tfor (id = 0; id < chip->ngpio; id++) {\n\t\tif (test_bit(FLAG_REQUESTED, &chip->desc[id].flags))\n\t\t\tdev_crit(chip->dev, \"REMOVING GPIOCHIP WITH GPIOS STILL REQUESTED\\n\");\n\t}\n\tfor (id = 0; id < chip->ngpio; id++)\n\t\tchip->desc[id].chip = NULL;\n\n\tlist_del(&chip->list);\n\tspin_unlock_irqrestore(&gpio_lock, flags);\n\tgpiochip_unexport(chip);\n\n\tkfree(chip->desc);\n\tchip->desc = NULL;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nvoid gpiochip_remove(struct gpio_chip *chip)\n{\n\tunsigned long\tflags;\n\tunsigned\tid;\n\n\tgpiochip_unexport(chip);\n\n\tgpiochip_irqchip_remove(chip);\n\n\tacpi_gpiochip_remove(chip);\n\tgpiochip_remove_pin_ranges(chip);\n\tof_gpiochip_remove(chip);\n\n\tspin_lock_irqsave(&gpio_lock, flags);\n\tfor (id = 0; id < chip->ngpio; id++) {\n\t\tif (test_bit(FLAG_REQUESTED, &chip->desc[id].flags))\n\t\t\tdev_crit(chip->dev, \"REMOVING GPIOCHIP WITH GPIOS STILL REQUESTED\\n\");\n\t}\n\tfor (id = 0; id < chip->ngpio; id++)\n\t\tchip->desc[id].chip = NULL;\n\n\tlist_del(&chip->list);\n\tspin_unlock_irqrestore(&gpio_lock, flags);\n\n\tkfree(chip->desc);\n\tchip->desc = NULL;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nint gpiochip_remove(struct gpio_chip *chip)\n{\n\tunsigned long\tflags;\n\tint\t\tstatus = 0;\n\tunsigned\tid;\n\n\tspin_lock_irqsave(&gpio_lock, flags);\n\n\tof_gpiochip_remove(chip);\n\n\tfor (id = chip->base; id < chip->base + chip->ngpio; id++) {\n\t\tif (test_bit(FLAG_REQUESTED, &gpio_desc[id].flags)) {\n\t\t\tstatus = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (status == 0) {\n\t\tfor (id = chip->base; id < chip->base + chip->ngpio; id++)\n\t\t\tgpio_desc[id].chip = NULL;\n\t}\n\n\tspin_unlock_irqrestore(&gpio_lock, flags);\n\n\tif (status == 0)\n\t\tgpiochip_unexport(chip);\n\n\treturn status;\n}\n\nstatic inline void gpiochip_unexport(struct gpio_chip *chip)\n{\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nint gpiochip_remove(struct gpio_chip *chip)\n{\n\tunsigned long\tflags;\n\tint\t\tstatus = 0;\n\tunsigned\tid;\n\n\tgpiochip_unexport(chip);\n\n\tspin_lock_irqsave(&gpio_lock, flags);\n\n\tof_gpiochip_remove(chip);\n\n\tfor (id = chip->base; id < chip->base + chip->ngpio; id++) {\n\t\tif (test_bit(FLAG_REQUESTED, &gpio_desc[id].flags)) {\n\t\t\tstatus = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (status == 0) {\n\t\tfor (id = chip->base; id < chip->base + chip->ngpio; id++)\n\t\t\tgpio_desc[id].chip = NULL;\n\t}\n\n\tspin_unlock_irqrestore(&gpio_lock, flags);\n\n\treturn status;\n}\n\nstatic inline void gpiochip_unexport(struct gpio_chip *chip)\n{\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic struct sock *pppol2tp_session_get_sock(struct l2tp_session *session)\n{\n\tstruct pppol2tp_session *ps = l2tp_session_priv(session);\n\tstruct sock *sk;\n\n\trcu_read_lock();\n\tsk = rcu_dereference(ps->sk);\n\tif (sk)\n\t\tsock_hold(sk);\n\trcu_read_unlock();\n\n\treturn sk;\n}\n\nstatic int pppol2tp_xmit(struct ppp_channel *chan, struct sk_buff *skb)\n{\n\tstruct sock *sk = (struct sock *) chan->private;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel;\n\tint uhlen, headroom;\n\n\tif (sock_flag(sk, SOCK_DEAD) || !(sk->sk_state & PPPOX_CONNECTED))\n\t\tgoto abort;\n\n\t/* Get session and tunnel contexts from the socket */\n\tsession = pppol2tp_sock_to_session(sk);\n\tif (session == NULL)\n\t\tgoto abort;\n\n\ttunnel = session->tunnel;\n\n\tuhlen = (tunnel->encap == L2TP_ENCAPTYPE_UDP) ? sizeof(struct udphdr) : 0;\n\theadroom = NET_SKB_PAD +\n\t\t   sizeof(struct iphdr) + /* IP header */\n\t\t   uhlen +\t\t/* UDP header (if L2TP_ENCAPTYPE_UDP) */\n\t\t   session->hdr_len +\t/* L2TP header */\n\t\t   2;\t\t\t/* 2 bytes for PPP_ALLSTATIONS & PPP_UI */\n\tif (skb_cow_head(skb, headroom))\n\t\tgoto abort_put_sess;\n\n\t/* Setup PPP header */\n\t__skb_push(skb, 2);\n\tskb->data[0] = PPP_ALLSTATIONS;\n\tskb->data[1] = PPP_UI;\n\n\tlocal_bh_disable();\n\tl2tp_xmit_skb(session, skb, session->hdr_len);\n\tlocal_bh_enable();\n\n\tsock_put(sk);\n\n\treturn 1;\n\nabort_put_sess:\n\tsock_put(sk);\nabort:\n\t/* Free the original skb */\n\tkfree_skb(skb);\n\treturn 1;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int pppol2tp_xmit(struct ppp_channel *chan, struct sk_buff *skb)\n{\n\tstruct sock *sk = (struct sock *) chan->private;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel;\n\tint uhlen, headroom;\n\n\tif (sock_flag(sk, SOCK_DEAD) || !(sk->sk_state & PPPOX_CONNECTED))\n\t\tgoto abort;\n\n\t/* Get session and tunnel contexts from the socket */\n\tsession = pppol2tp_sock_to_session(sk);\n\tif (session == NULL)\n\t\tgoto abort;\n\n\ttunnel = session->tunnel;\n\n\tuhlen = (tunnel->encap == L2TP_ENCAPTYPE_UDP) ? sizeof(struct udphdr) : 0;\n\theadroom = NET_SKB_PAD +\n\t\t   sizeof(struct iphdr) + /* IP header */\n\t\t   uhlen +\t\t/* UDP header (if L2TP_ENCAPTYPE_UDP) */\n\t\t   session->hdr_len +\t/* L2TP header */\n\t\t   2;\t\t\t/* 2 bytes for PPP_ALLSTATIONS & PPP_UI */\n\tif (skb_cow_head(skb, headroom))\n\t\tgoto abort_put_sess;\n\n\t/* Setup PPP header */\n\t__skb_push(skb, 2);\n\tskb->data[0] = PPP_ALLSTATIONS;\n\tskb->data[1] = PPP_UI;\n\n\tlocal_bh_disable();\n\tl2tp_xmit_skb(session, skb, session->hdr_len);\n\tlocal_bh_enable();\n\n\tsock_put(sk);\n\n\treturn 1;\n\nabort_put_sess:\n\tsock_put(sk);\nabort:\n\t/* Free the original skb */\n\tkfree_skb(skb);\n\treturn 1;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic struct sock *pppol2tp_session_get_sock(struct l2tp_session *session)\n{\n\tstruct pppol2tp_session *ps = l2tp_session_priv(session);\n\tstruct sock *sk;\n\n\trcu_read_lock();\n\tsk = rcu_dereference(ps->sk);\n\tif (sk)\n\t\tsock_hold(sk);\n\trcu_read_unlock();\n\n\treturn sk;\n}\n\nstatic int pppol2tp_xmit(struct ppp_channel *chan, struct sk_buff *skb)\n{\n\tstatic const u8 ppph[2] = { 0xff, 0x03 };\n\tstruct sock *sk = (struct sock *) chan->private;\n\tstruct sock *sk_tun;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tint old_headroom;\n\tint new_headroom;\n\tint uhlen, headroom;\n\n\tif (sock_flag(sk, SOCK_DEAD) || !(sk->sk_state & PPPOX_CONNECTED))\n\t\tgoto abort;\n\n\t/* Get session and tunnel contexts from the socket */\n\tsession = pppol2tp_sock_to_session(sk);\n\tif (session == NULL)\n\t\tgoto abort;\n\n\tps = l2tp_session_priv(session);\n\tsk_tun = ps->tunnel_sock;\n\tif (sk_tun == NULL)\n\t\tgoto abort_put_sess;\n\ttunnel = l2tp_sock_to_tunnel(sk_tun);\n\tif (tunnel == NULL)\n\t\tgoto abort_put_sess;\n\n\told_headroom = skb_headroom(skb);\n\tuhlen = (tunnel->encap == L2TP_ENCAPTYPE_UDP) ? sizeof(struct udphdr) : 0;\n\theadroom = NET_SKB_PAD +\n\t\t   sizeof(struct iphdr) + /* IP header */\n\t\t   uhlen +\t\t/* UDP header (if L2TP_ENCAPTYPE_UDP) */\n\t\t   session->hdr_len +\t/* L2TP header */\n\t\t   sizeof(ppph);\t/* PPP header */\n\tif (skb_cow_head(skb, headroom))\n\t\tgoto abort_put_sess_tun;\n\n\tnew_headroom = skb_headroom(skb);\n\tskb->truesize += new_headroom - old_headroom;\n\n\t/* Setup PPP header */\n\t__skb_push(skb, sizeof(ppph));\n\tskb->data[0] = ppph[0];\n\tskb->data[1] = ppph[1];\n\n\tlocal_bh_disable();\n\tl2tp_xmit_skb(session, skb, session->hdr_len);\n\tlocal_bh_enable();\n\n\tsock_put(sk_tun);\n\tsock_put(sk);\n\treturn 1;\n\nabort_put_sess_tun:\n\tsock_put(sk_tun);\nabort_put_sess:\n\tsock_put(sk);\nabort:\n\t/* Free the original skb */\n\tkfree_skb(skb);\n\treturn 1;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int pppol2tp_xmit(struct ppp_channel *chan, struct sk_buff *skb)\n{\n\tstatic const u8 ppph[2] = { 0xff, 0x03 };\n\tstruct sock *sk = (struct sock *) chan->private;\n\tstruct sock *sk_tun;\n\tstruct l2tp_session *session;\n\tstruct l2tp_tunnel *tunnel;\n\tstruct pppol2tp_session *ps;\n\tint old_headroom;\n\tint new_headroom;\n\tint uhlen, headroom;\n\n\tif (sock_flag(sk, SOCK_DEAD) || !(sk->sk_state & PPPOX_CONNECTED))\n\t\tgoto abort;\n\n\t/* Get session and tunnel contexts from the socket */\n\tsession = pppol2tp_sock_to_session(sk);\n\tif (session == NULL)\n\t\tgoto abort;\n\n\tps = l2tp_session_priv(session);\n\tsk_tun = ps->tunnel_sock;\n\tif (sk_tun == NULL)\n\t\tgoto abort_put_sess;\n\ttunnel = l2tp_sock_to_tunnel(sk_tun);\n\tif (tunnel == NULL)\n\t\tgoto abort_put_sess;\n\n\told_headroom = skb_headroom(skb);\n\tuhlen = (tunnel->encap == L2TP_ENCAPTYPE_UDP) ? sizeof(struct udphdr) : 0;\n\theadroom = NET_SKB_PAD +\n\t\t   sizeof(struct iphdr) + /* IP header */\n\t\t   uhlen +\t\t/* UDP header (if L2TP_ENCAPTYPE_UDP) */\n\t\t   session->hdr_len +\t/* L2TP header */\n\t\t   sizeof(ppph);\t/* PPP header */\n\tif (skb_cow_head(skb, headroom))\n\t\tgoto abort_put_sess_tun;\n\n\tnew_headroom = skb_headroom(skb);\n\tskb->truesize += new_headroom - old_headroom;\n\n\t/* Setup PPP header */\n\t__skb_push(skb, sizeof(ppph));\n\tskb->data[0] = ppph[0];\n\tskb->data[1] = ppph[1];\n\n\tlocal_bh_disable();\n\tl2tp_xmit_skb(session, skb, session->hdr_len);\n\tlocal_bh_enable();\n\n\tsock_put(sk_tun);\n\tsock_put(sk);\n\treturn 1;\n\nabort_put_sess_tun:\n\tsock_put(sk_tun);\nabort_put_sess:\n\tsock_put(sk);\nabort:\n\t/* Free the original skb */\n\tkfree_skb(skb);\n\treturn 1;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int team_port_add(struct team *team, struct net_device *port_dev,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct net_device *dev = team->dev;\n\tstruct team_port *port;\n\tchar *portname = port_dev->name;\n\tint err;\n\n\tif (port_dev->flags & IFF_LOOPBACK) {\n\t\tNL_SET_ERR_MSG(extack, \"Loopback device can't be added as a team port\");\n\t\tnetdev_err(dev, \"Device %s is loopback device. Loopback devices can't be added as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EINVAL;\n\t}\n\n\tif (team_port_exists(port_dev)) {\n\t\tNL_SET_ERR_MSG(extack, \"Device is already a port of a team device\");\n\t\tnetdev_err(dev, \"Device %s is already a port \"\n\t\t\t\t\"of a team device\\n\", portname);\n\t\treturn -EBUSY;\n\t}\n\n\tif (port_dev->features & NETIF_F_VLAN_CHALLENGED &&\n\t    vlan_uses_dev(dev)) {\n\t\tNL_SET_ERR_MSG(extack, \"Device is VLAN challenged and team device has VLAN set up\");\n\t\tnetdev_err(dev, \"Device %s is VLAN challenged and team device has VLAN set up\\n\",\n\t\t\t   portname);\n\t\treturn -EPERM;\n\t}\n\n\terr = team_dev_type_check_change(dev, port_dev);\n\tif (err)\n\t\treturn err;\n\n\tif (port_dev->flags & IFF_UP) {\n\t\tNL_SET_ERR_MSG(extack, \"Device is up. Set it down before adding it as a team port\");\n\t\tnetdev_err(dev, \"Device %s is up. Set it down before adding it as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EBUSY;\n\t}\n\n\tport = kzalloc(sizeof(struct team_port) + team->mode->port_priv_size,\n\t\t       GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tport->dev = port_dev;\n\tport->team = team;\n\tINIT_LIST_HEAD(&port->qom_list);\n\n\tport->orig.mtu = port_dev->mtu;\n\terr = dev_set_mtu(port_dev, dev->mtu);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Error %d calling dev_set_mtu\\n\", err);\n\t\tgoto err_set_mtu;\n\t}\n\n\tmemcpy(port->orig.dev_addr, port_dev->dev_addr, port_dev->addr_len);\n\n\terr = team_port_enter(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to enter team mode\\n\",\n\t\t\t   portname);\n\t\tgoto err_port_enter;\n\t}\n\n\terr = dev_open(port_dev);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Device %s opening failed\\n\",\n\t\t\t   portname);\n\t\tgoto err_dev_open;\n\t}\n\n\terr = vlan_vids_add_by_dev(port_dev, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to add vlan ids to device %s\\n\",\n\t\t\t\tportname);\n\t\tgoto err_vids_add;\n\t}\n\n\terr = team_port_enable_netpoll(port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to enable netpoll on device %s\\n\",\n\t\t\t   portname);\n\t\tgoto err_enable_netpoll;\n\t}\n\n\tif (!(dev->features & NETIF_F_LRO))\n\t\tdev_disable_lro(port_dev);\n\n\terr = netdev_rx_handler_register(port_dev, team_handle_frame,\n\t\t\t\t\t port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to register rx_handler\\n\",\n\t\t\t   portname);\n\t\tgoto err_handler_register;\n\t}\n\n\terr = team_upper_dev_link(team, port, extack);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to set upper link\\n\",\n\t\t\t   portname);\n\t\tgoto err_set_upper_link;\n\t}\n\n\terr = __team_option_inst_add_port(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to add per-port options\\n\",\n\t\t\t   portname);\n\t\tgoto err_option_port_add;\n\t}\n\n\tnetif_addr_lock_bh(dev);\n\tdev_uc_sync_multiple(port_dev, dev);\n\tdev_mc_sync_multiple(port_dev, dev);\n\tnetif_addr_unlock_bh(dev);\n\n\tport->index = -1;\n\tlist_add_tail_rcu(&port->list, &team->port_list);\n\tteam_port_enable(team, port);\n\t__team_compute_features(team);\n\t__team_port_change_port_added(port, !!netif_carrier_ok(port_dev));\n\t__team_options_change_check(team);\n\n\tnetdev_info(dev, \"Port device %s added\\n\", portname);\n\n\treturn 0;\n\nerr_option_port_add:\n\tteam_upper_dev_unlink(team, port);\n\nerr_set_upper_link:\n\tnetdev_rx_handler_unregister(port_dev);\n\nerr_handler_register:\n\tteam_port_disable_netpoll(port);\n\nerr_enable_netpoll:\n\tvlan_vids_del_by_dev(port_dev, dev);\n\nerr_vids_add:\n\tdev_close(port_dev);\n\nerr_dev_open:\n\tteam_port_leave(team, port);\n\tteam_port_set_orig_dev_addr(port);\n\nerr_port_enter:\n\tdev_set_mtu(port_dev, port->orig.mtu);\n\nerr_set_mtu:\n\tkfree(port);\n\n\treturn err;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int team_port_add(struct team *team, struct net_device *port_dev,\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct net_device *dev = team->dev;\n\tstruct team_port *port;\n\tchar *portname = port_dev->name;\n\tint err;\n\n\tif (port_dev->flags & IFF_LOOPBACK) {\n\t\tNL_SET_ERR_MSG(extack, \"Loopback device can't be added as a team port\");\n\t\tnetdev_err(dev, \"Device %s is loopback device. Loopback devices can't be added as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EINVAL;\n\t}\n\n\tif (team_port_exists(port_dev)) {\n\t\tNL_SET_ERR_MSG(extack, \"Device is already a port of a team device\");\n\t\tnetdev_err(dev, \"Device %s is already a port \"\n\t\t\t\t\"of a team device\\n\", portname);\n\t\treturn -EBUSY;\n\t}\n\n\tif (dev == port_dev) {\n\t\tNL_SET_ERR_MSG(extack, \"Cannot enslave team device to itself\");\n\t\tnetdev_err(dev, \"Cannot enslave team device to itself\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (port_dev->features & NETIF_F_VLAN_CHALLENGED &&\n\t    vlan_uses_dev(dev)) {\n\t\tNL_SET_ERR_MSG(extack, \"Device is VLAN challenged and team device has VLAN set up\");\n\t\tnetdev_err(dev, \"Device %s is VLAN challenged and team device has VLAN set up\\n\",\n\t\t\t   portname);\n\t\treturn -EPERM;\n\t}\n\n\terr = team_dev_type_check_change(dev, port_dev);\n\tif (err)\n\t\treturn err;\n\n\tif (port_dev->flags & IFF_UP) {\n\t\tNL_SET_ERR_MSG(extack, \"Device is up. Set it down before adding it as a team port\");\n\t\tnetdev_err(dev, \"Device %s is up. Set it down before adding it as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EBUSY;\n\t}\n\n\tport = kzalloc(sizeof(struct team_port) + team->mode->port_priv_size,\n\t\t       GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tport->dev = port_dev;\n\tport->team = team;\n\tINIT_LIST_HEAD(&port->qom_list);\n\n\tport->orig.mtu = port_dev->mtu;\n\terr = dev_set_mtu(port_dev, dev->mtu);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Error %d calling dev_set_mtu\\n\", err);\n\t\tgoto err_set_mtu;\n\t}\n\n\tmemcpy(port->orig.dev_addr, port_dev->dev_addr, port_dev->addr_len);\n\n\terr = team_port_enter(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to enter team mode\\n\",\n\t\t\t   portname);\n\t\tgoto err_port_enter;\n\t}\n\n\terr = dev_open(port_dev);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Device %s opening failed\\n\",\n\t\t\t   portname);\n\t\tgoto err_dev_open;\n\t}\n\n\terr = vlan_vids_add_by_dev(port_dev, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to add vlan ids to device %s\\n\",\n\t\t\t\tportname);\n\t\tgoto err_vids_add;\n\t}\n\n\terr = team_port_enable_netpoll(port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to enable netpoll on device %s\\n\",\n\t\t\t   portname);\n\t\tgoto err_enable_netpoll;\n\t}\n\n\tif (!(dev->features & NETIF_F_LRO))\n\t\tdev_disable_lro(port_dev);\n\n\terr = netdev_rx_handler_register(port_dev, team_handle_frame,\n\t\t\t\t\t port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to register rx_handler\\n\",\n\t\t\t   portname);\n\t\tgoto err_handler_register;\n\t}\n\n\terr = team_upper_dev_link(team, port, extack);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to set upper link\\n\",\n\t\t\t   portname);\n\t\tgoto err_set_upper_link;\n\t}\n\n\terr = __team_option_inst_add_port(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to add per-port options\\n\",\n\t\t\t   portname);\n\t\tgoto err_option_port_add;\n\t}\n\n\tnetif_addr_lock_bh(dev);\n\tdev_uc_sync_multiple(port_dev, dev);\n\tdev_mc_sync_multiple(port_dev, dev);\n\tnetif_addr_unlock_bh(dev);\n\n\tport->index = -1;\n\tlist_add_tail_rcu(&port->list, &team->port_list);\n\tteam_port_enable(team, port);\n\t__team_compute_features(team);\n\t__team_port_change_port_added(port, !!netif_carrier_ok(port_dev));\n\t__team_options_change_check(team);\n\n\tnetdev_info(dev, \"Port device %s added\\n\", portname);\n\n\treturn 0;\n\nerr_option_port_add:\n\tteam_upper_dev_unlink(team, port);\n\nerr_set_upper_link:\n\tnetdev_rx_handler_unregister(port_dev);\n\nerr_handler_register:\n\tteam_port_disable_netpoll(port);\n\nerr_enable_netpoll:\n\tvlan_vids_del_by_dev(port_dev, dev);\n\nerr_vids_add:\n\tdev_close(port_dev);\n\nerr_dev_open:\n\tteam_port_leave(team, port);\n\tteam_port_set_orig_dev_addr(port);\n\nerr_port_enter:\n\tdev_set_mtu(port_dev, port->orig.mtu);\n\nerr_set_mtu:\n\tkfree(port);\n\n\treturn err;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int team_port_add(struct team *team, struct net_device *port_dev)\n{\n\tstruct net_device *dev = team->dev;\n\tstruct team_port *port;\n\tchar *portname = port_dev->name;\n\tint err;\n\n\tif (port_dev->flags & IFF_LOOPBACK) {\n\t\tnetdev_err(dev, \"Device %s is loopback device. Loopback devices can't be added as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EINVAL;\n\t}\n\n\tif (team_port_exists(port_dev)) {\n\t\tnetdev_err(dev, \"Device %s is already a port \"\n\t\t\t\t\"of a team device\\n\", portname);\n\t\treturn -EBUSY;\n\t}\n\n\tif (port_dev->features & NETIF_F_VLAN_CHALLENGED &&\n\t    vlan_uses_dev(dev)) {\n\t\tnetdev_err(dev, \"Device %s is VLAN challenged and team device has VLAN set up\\n\",\n\t\t\t   portname);\n\t\treturn -EPERM;\n\t}\n\n\terr = team_dev_type_check_change(dev, port_dev);\n\tif (err)\n\t\treturn err;\n\n\tif (port_dev->flags & IFF_UP) {\n\t\tnetdev_err(dev, \"Device %s is up. Set it down before adding it as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EBUSY;\n\t}\n\n\tport = kzalloc(sizeof(struct team_port) + team->mode->port_priv_size,\n\t\t       GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tport->dev = port_dev;\n\tport->team = team;\n\tINIT_LIST_HEAD(&port->qom_list);\n\n\tport->orig.mtu = port_dev->mtu;\n\terr = dev_set_mtu(port_dev, dev->mtu);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Error %d calling dev_set_mtu\\n\", err);\n\t\tgoto err_set_mtu;\n\t}\n\n\tmemcpy(port->orig.dev_addr, port_dev->dev_addr, port_dev->addr_len);\n\n\terr = team_port_enter(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to enter team mode\\n\",\n\t\t\t   portname);\n\t\tgoto err_port_enter;\n\t}\n\n\terr = dev_open(port_dev);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Device %s opening failed\\n\",\n\t\t\t   portname);\n\t\tgoto err_dev_open;\n\t}\n\n\terr = vlan_vids_add_by_dev(port_dev, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to add vlan ids to device %s\\n\",\n\t\t\t\tportname);\n\t\tgoto err_vids_add;\n\t}\n\n\terr = team_port_enable_netpoll(port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to enable netpoll on device %s\\n\",\n\t\t\t   portname);\n\t\tgoto err_enable_netpoll;\n\t}\n\n\terr = netdev_master_upper_dev_link(port_dev, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to set upper link\\n\",\n\t\t\t   portname);\n\t\tgoto err_set_upper_link;\n\t}\n\n\terr = netdev_rx_handler_register(port_dev, team_handle_frame,\n\t\t\t\t\t port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to register rx_handler\\n\",\n\t\t\t   portname);\n\t\tgoto err_handler_register;\n\t}\n\n\terr = __team_option_inst_add_port(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to add per-port options\\n\",\n\t\t\t   portname);\n\t\tgoto err_option_port_add;\n\t}\n\n\tport->index = -1;\n\tlist_add_tail_rcu(&port->list, &team->port_list);\n\tteam_port_enable(team, port);\n\t__team_compute_features(team);\n\t__team_port_change_port_added(port, !!netif_carrier_ok(port_dev));\n\t__team_options_change_check(team);\n\n\tnetdev_info(dev, \"Port device %s added\\n\", portname);\n\n\treturn 0;\n\nerr_option_port_add:\n\tnetdev_rx_handler_unregister(port_dev);\n\nerr_handler_register:\n\tnetdev_upper_dev_unlink(port_dev, dev);\n\nerr_set_upper_link:\n\tteam_port_disable_netpoll(port);\n\nerr_enable_netpoll:\n\tvlan_vids_del_by_dev(port_dev, dev);\n\nerr_vids_add:\n\tdev_close(port_dev);\n\nerr_dev_open:\n\tteam_port_leave(team, port);\n\tteam_port_set_orig_dev_addr(port);\n\nerr_port_enter:\n\tdev_set_mtu(port_dev, port->orig.mtu);\n\nerr_set_mtu:\n\tkfree(port);\n\n\treturn err;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int team_port_add(struct team *team, struct net_device *port_dev)\n{\n\tstruct net_device *dev = team->dev;\n\tstruct team_port *port;\n\tchar *portname = port_dev->name;\n\tint err;\n\n\tif (port_dev->flags & IFF_LOOPBACK) {\n\t\tnetdev_err(dev, \"Device %s is loopback device. Loopback devices can't be added as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EINVAL;\n\t}\n\n\tif (team_port_exists(port_dev)) {\n\t\tnetdev_err(dev, \"Device %s is already a port \"\n\t\t\t\t\"of a team device\\n\", portname);\n\t\treturn -EBUSY;\n\t}\n\n\tif (dev == port_dev) {\n\t\tnetdev_err(dev, \"Cannot enslave team device to itself\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (port_dev->features & NETIF_F_VLAN_CHALLENGED &&\n\t    vlan_uses_dev(dev)) {\n\t\tnetdev_err(dev, \"Device %s is VLAN challenged and team device has VLAN set up\\n\",\n\t\t\t   portname);\n\t\treturn -EPERM;\n\t}\n\n\terr = team_dev_type_check_change(dev, port_dev);\n\tif (err)\n\t\treturn err;\n\n\tif (port_dev->flags & IFF_UP) {\n\t\tnetdev_err(dev, \"Device %s is up. Set it down before adding it as a team port\\n\",\n\t\t\t   portname);\n\t\treturn -EBUSY;\n\t}\n\n\tport = kzalloc(sizeof(struct team_port) + team->mode->port_priv_size,\n\t\t       GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tport->dev = port_dev;\n\tport->team = team;\n\tINIT_LIST_HEAD(&port->qom_list);\n\n\tport->orig.mtu = port_dev->mtu;\n\terr = dev_set_mtu(port_dev, dev->mtu);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Error %d calling dev_set_mtu\\n\", err);\n\t\tgoto err_set_mtu;\n\t}\n\n\tmemcpy(port->orig.dev_addr, port_dev->dev_addr, port_dev->addr_len);\n\n\terr = team_port_enter(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to enter team mode\\n\",\n\t\t\t   portname);\n\t\tgoto err_port_enter;\n\t}\n\n\terr = dev_open(port_dev);\n\tif (err) {\n\t\tnetdev_dbg(dev, \"Device %s opening failed\\n\",\n\t\t\t   portname);\n\t\tgoto err_dev_open;\n\t}\n\n\terr = vlan_vids_add_by_dev(port_dev, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to add vlan ids to device %s\\n\",\n\t\t\t\tportname);\n\t\tgoto err_vids_add;\n\t}\n\n\terr = team_port_enable_netpoll(port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to enable netpoll on device %s\\n\",\n\t\t\t   portname);\n\t\tgoto err_enable_netpoll;\n\t}\n\n\terr = netdev_master_upper_dev_link(port_dev, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to set upper link\\n\",\n\t\t\t   portname);\n\t\tgoto err_set_upper_link;\n\t}\n\n\terr = netdev_rx_handler_register(port_dev, team_handle_frame,\n\t\t\t\t\t port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to register rx_handler\\n\",\n\t\t\t   portname);\n\t\tgoto err_handler_register;\n\t}\n\n\terr = __team_option_inst_add_port(team, port);\n\tif (err) {\n\t\tnetdev_err(dev, \"Device %s failed to add per-port options\\n\",\n\t\t\t   portname);\n\t\tgoto err_option_port_add;\n\t}\n\n\tport->index = -1;\n\tlist_add_tail_rcu(&port->list, &team->port_list);\n\tteam_port_enable(team, port);\n\t__team_compute_features(team);\n\t__team_port_change_port_added(port, !!netif_carrier_ok(port_dev));\n\t__team_options_change_check(team);\n\n\tnetdev_info(dev, \"Port device %s added\\n\", portname);\n\n\treturn 0;\n\nerr_option_port_add:\n\tnetdev_rx_handler_unregister(port_dev);\n\nerr_handler_register:\n\tnetdev_upper_dev_unlink(port_dev, dev);\n\nerr_set_upper_link:\n\tteam_port_disable_netpoll(port);\n\nerr_enable_netpoll:\n\tvlan_vids_del_by_dev(port_dev, dev);\n\nerr_vids_add:\n\tdev_close(port_dev);\n\nerr_dev_open:\n\tteam_port_leave(team, port);\n\tteam_port_set_orig_dev_addr(port);\n\nerr_port_enter:\n\tdev_set_mtu(port_dev, port->orig.mtu);\n\nerr_set_mtu:\n\tkfree(port);\n\n\treturn err;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nvoid xhci_mem_cleanup(struct xhci_hcd *xhci)\n{\n\tstruct device\t*dev = xhci_to_hcd(xhci)->self.controller;\n\tint size;\n\tint i, j, num_ports;\n\n\tdel_timer_sync(&xhci->cmd_timer);\n\n\t/* Free the Event Ring Segment Table and the actual Event Ring */\n\tsize = sizeof(struct xhci_erst_entry)*(xhci->erst.num_entries);\n\tif (xhci->erst.entries)\n\t\tdma_free_coherent(dev, size,\n\t\t\t\txhci->erst.entries, xhci->erst.erst_dma_addr);\n\txhci->erst.entries = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed ERST\");\n\tif (xhci->event_ring)\n\t\txhci_ring_free(xhci, xhci->event_ring);\n\txhci->event_ring = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed event ring\");\n\n\tif (xhci->lpm_command)\n\t\txhci_free_command(xhci, xhci->lpm_command);\n\txhci->lpm_command = NULL;\n\tif (xhci->cmd_ring)\n\t\txhci_ring_free(xhci, xhci->cmd_ring);\n\txhci->cmd_ring = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed command ring\");\n\txhci_cleanup_command_queue(xhci);\n\n\tnum_ports = HCS_MAX_PORTS(xhci->hcs_params1);\n\tfor (i = 0; i < num_ports && xhci->rh_bw; i++) {\n\t\tstruct xhci_interval_bw_table *bwt = &xhci->rh_bw[i].bw_table;\n\t\tfor (j = 0; j < XHCI_MAX_INTERVAL; j++) {\n\t\t\tstruct list_head *ep = &bwt->interval_bw[j].endpoints;\n\t\t\twhile (!list_empty(ep))\n\t\t\t\tlist_del_init(ep->next);\n\t\t}\n\t}\n\n\tfor (i = 1; i < MAX_HC_SLOTS; ++i)\n\t\txhci_free_virt_device(xhci, i);\n\n\tdma_pool_destroy(xhci->segment_pool);\n\txhci->segment_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed segment pool\");\n\n\tdma_pool_destroy(xhci->device_pool);\n\txhci->device_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed device context pool\");\n\n\tdma_pool_destroy(xhci->small_streams_pool);\n\txhci->small_streams_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init,\n\t\t\t\"Freed small stream array pool\");\n\n\tdma_pool_destroy(xhci->medium_streams_pool);\n\txhci->medium_streams_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init,\n\t\t\t\"Freed medium stream array pool\");\n\n\tif (xhci->dcbaa)\n\t\tdma_free_coherent(dev, sizeof(*xhci->dcbaa),\n\t\t\t\txhci->dcbaa, xhci->dcbaa->dma);\n\txhci->dcbaa = NULL;\n\n\tscratchpad_free(xhci);\n\n\tif (!xhci->rh_bw)\n\t\tgoto no_bw;\n\n\tfor (i = 0; i < num_ports; i++) {\n\t\tstruct xhci_tt_bw_info *tt, *n;\n\t\tlist_for_each_entry_safe(tt, n, &xhci->rh_bw[i].tts, tt_list) {\n\t\t\tlist_del(&tt->tt_list);\n\t\t\tkfree(tt);\n\t\t}\n\t}\n\nno_bw:\n\txhci->cmd_ring_reserved_trbs = 0;\n\txhci->num_usb2_ports = 0;\n\txhci->num_usb3_ports = 0;\n\txhci->num_active_eps = 0;\n\tkfree(xhci->usb2_ports);\n\tkfree(xhci->usb3_ports);\n\tkfree(xhci->port_array);\n\tkfree(xhci->rh_bw);\n\tkfree(xhci->ext_caps);\n\n\txhci->page_size = 0;\n\txhci->page_shift = 0;\n\txhci->bus_state[0].bus_suspended = 0;\n\txhci->bus_state[1].bus_suspended = 0;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nvoid xhci_mem_cleanup(struct xhci_hcd *xhci)\n{\n\tstruct device\t*dev = xhci_to_hcd(xhci)->self.controller;\n\tint size;\n\tint i, j, num_ports;\n\n\tdel_timer_sync(&xhci->cmd_timer);\n\n\t/* Free the Event Ring Segment Table and the actual Event Ring */\n\tsize = sizeof(struct xhci_erst_entry)*(xhci->erst.num_entries);\n\tif (xhci->erst.entries)\n\t\tdma_free_coherent(dev, size,\n\t\t\t\txhci->erst.entries, xhci->erst.erst_dma_addr);\n\txhci->erst.entries = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed ERST\");\n\tif (xhci->event_ring)\n\t\txhci_ring_free(xhci, xhci->event_ring);\n\txhci->event_ring = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed event ring\");\n\n\tif (xhci->lpm_command)\n\t\txhci_free_command(xhci, xhci->lpm_command);\n\txhci->lpm_command = NULL;\n\tif (xhci->cmd_ring)\n\t\txhci_ring_free(xhci, xhci->cmd_ring);\n\txhci->cmd_ring = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed command ring\");\n\txhci_cleanup_command_queue(xhci);\n\n\tnum_ports = HCS_MAX_PORTS(xhci->hcs_params1);\n\tfor (i = 0; i < num_ports && xhci->rh_bw; i++) {\n\t\tstruct xhci_interval_bw_table *bwt = &xhci->rh_bw[i].bw_table;\n\t\tfor (j = 0; j < XHCI_MAX_INTERVAL; j++) {\n\t\t\tstruct list_head *ep = &bwt->interval_bw[j].endpoints;\n\t\t\twhile (!list_empty(ep))\n\t\t\t\tlist_del_init(ep->next);\n\t\t}\n\t}\n\n\tfor (i = 1; i < MAX_HC_SLOTS; ++i)\n\t\txhci_free_virt_device(xhci, i);\n\n\tdma_pool_destroy(xhci->segment_pool);\n\txhci->segment_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed segment pool\");\n\n\tdma_pool_destroy(xhci->device_pool);\n\txhci->device_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init, \"Freed device context pool\");\n\n\tdma_pool_destroy(xhci->small_streams_pool);\n\txhci->small_streams_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init,\n\t\t\t\"Freed small stream array pool\");\n\n\tdma_pool_destroy(xhci->medium_streams_pool);\n\txhci->medium_streams_pool = NULL;\n\txhci_dbg_trace(xhci, trace_xhci_dbg_init,\n\t\t\t\"Freed medium stream array pool\");\n\n\tif (xhci->dcbaa)\n\t\tdma_free_coherent(dev, sizeof(*xhci->dcbaa),\n\t\t\t\txhci->dcbaa, xhci->dcbaa->dma);\n\txhci->dcbaa = NULL;\n\n\tscratchpad_free(xhci);\n\n\tif (!xhci->rh_bw)\n\t\tgoto no_bw;\n\n\tfor (i = 0; i < num_ports; i++) {\n\t\tstruct xhci_tt_bw_info *tt, *n;\n\t\tlist_for_each_entry_safe(tt, n, &xhci->rh_bw[i].tts, tt_list) {\n\t\t\tlist_del(&tt->tt_list);\n\t\t\tkfree(tt);\n\t\t}\n\t}\n\nno_bw:\n\txhci->cmd_ring_reserved_trbs = 0;\n\txhci->num_usb2_ports = 0;\n\txhci->num_usb3_ports = 0;\n\txhci->num_active_eps = 0;\n\tkfree(xhci->usb2_ports);\n\tkfree(xhci->usb3_ports);\n\tkfree(xhci->port_array);\n\tkfree(xhci->rh_bw);\n\tkfree(xhci->ext_caps);\n\n\txhci->usb2_ports = NULL;\n\txhci->usb3_ports = NULL;\n\txhci->port_array = NULL;\n\txhci->rh_bw = NULL;\n\txhci->ext_caps = NULL;\n\n\txhci->page_size = 0;\n\txhci->page_shift = 0;\n\txhci->bus_state[0].bus_suspended = 0;\n\txhci->bus_state[1].bus_suspended = 0;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nvoid xhci_mem_cleanup(struct xhci_hcd *xhci)\n{\n\tstruct pci_dev\t*pdev = to_pci_dev(xhci_to_hcd(xhci)->self.controller);\n\tstruct dev_info\t*dev_info, *next;\n\tstruct xhci_cd  *cur_cd, *next_cd;\n\tunsigned long\tflags;\n\tint size;\n\tint i, j, num_ports;\n\n\t/* Free the Event Ring Segment Table and the actual Event Ring */\n\tsize = sizeof(struct xhci_erst_entry)*(xhci->erst.num_entries);\n\tif (xhci->erst.entries)\n\t\tdma_free_coherent(&pdev->dev, size,\n\t\t\t\txhci->erst.entries, xhci->erst.erst_dma_addr);\n\txhci->erst.entries = NULL;\n\txhci_dbg(xhci, \"Freed ERST\\n\");\n\tif (xhci->event_ring)\n\t\txhci_ring_free(xhci, xhci->event_ring);\n\txhci->event_ring = NULL;\n\txhci_dbg(xhci, \"Freed event ring\\n\");\n\n\txhci->cmd_ring_reserved_trbs = 0;\n\tif (xhci->cmd_ring)\n\t\txhci_ring_free(xhci, xhci->cmd_ring);\n\txhci->cmd_ring = NULL;\n\txhci_dbg(xhci, \"Freed command ring\\n\");\n\tlist_for_each_entry_safe(cur_cd, next_cd,\n\t\t\t&xhci->cancel_cmd_list, cancel_cmd_list) {\n\t\tlist_del(&cur_cd->cancel_cmd_list);\n\t\tkfree(cur_cd);\n\t}\n\n\tnum_ports = HCS_MAX_PORTS(xhci->hcs_params1);\n\tfor (i = 0; i < num_ports && xhci->rh_bw; i++) {\n\t\tstruct xhci_interval_bw_table *bwt = &xhci->rh_bw[i].bw_table;\n\t\tfor (j = 0; j < XHCI_MAX_INTERVAL; j++) {\n\t\t\tstruct list_head *ep = &bwt->interval_bw[j].endpoints;\n\t\t\twhile (!list_empty(ep))\n\t\t\t\tlist_del_init(ep->next);\n\t\t}\n\t}\n\n\tfor (i = 1; i < MAX_HC_SLOTS; ++i)\n\t\txhci_free_virt_device(xhci, i);\n\n\tif (xhci->segment_pool)\n\t\tdma_pool_destroy(xhci->segment_pool);\n\txhci->segment_pool = NULL;\n\txhci_dbg(xhci, \"Freed segment pool\\n\");\n\n\tif (xhci->device_pool)\n\t\tdma_pool_destroy(xhci->device_pool);\n\txhci->device_pool = NULL;\n\txhci_dbg(xhci, \"Freed device context pool\\n\");\n\n\tif (xhci->small_streams_pool)\n\t\tdma_pool_destroy(xhci->small_streams_pool);\n\txhci->small_streams_pool = NULL;\n\txhci_dbg(xhci, \"Freed small stream array pool\\n\");\n\n\tif (xhci->medium_streams_pool)\n\t\tdma_pool_destroy(xhci->medium_streams_pool);\n\txhci->medium_streams_pool = NULL;\n\txhci_dbg(xhci, \"Freed medium stream array pool\\n\");\n\n\tif (xhci->dcbaa)\n\t\tdma_free_coherent(&pdev->dev, sizeof(*xhci->dcbaa),\n\t\t\t\txhci->dcbaa, xhci->dcbaa->dma);\n\txhci->dcbaa = NULL;\n\n\tscratchpad_free(xhci);\n\n\tspin_lock_irqsave(&xhci->lock, flags);\n\tlist_for_each_entry_safe(dev_info, next, &xhci->lpm_failed_devs, list) {\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n\tspin_unlock_irqrestore(&xhci->lock, flags);\n\n\tif (!xhci->rh_bw)\n\t\tgoto no_bw;\n\n\tfor (i = 0; i < num_ports; i++) {\n\t\tstruct xhci_tt_bw_info *tt, *n;\n\t\tlist_for_each_entry_safe(tt, n, &xhci->rh_bw[i].tts, tt_list) {\n\t\t\tlist_del(&tt->tt_list);\n\t\t\tkfree(tt);\n\t\t}\n\t}\n\nno_bw:\n\txhci->num_usb2_ports = 0;\n\txhci->num_usb3_ports = 0;\n\txhci->num_active_eps = 0;\n\tkfree(xhci->usb2_ports);\n\tkfree(xhci->usb3_ports);\n\tkfree(xhci->port_array);\n\tkfree(xhci->rh_bw);\n\n\txhci->page_size = 0;\n\txhci->page_shift = 0;\n\txhci->bus_state[0].bus_suspended = 0;\n\txhci->bus_state[1].bus_suspended = 0;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nvoid xhci_mem_cleanup(struct xhci_hcd *xhci)\n{\n\tstruct pci_dev\t*pdev = to_pci_dev(xhci_to_hcd(xhci)->self.controller);\n\tstruct dev_info\t*dev_info, *next;\n\tstruct xhci_cd  *cur_cd, *next_cd;\n\tunsigned long\tflags;\n\tint size;\n\tint i, j, num_ports;\n\n\t/* Free the Event Ring Segment Table and the actual Event Ring */\n\tsize = sizeof(struct xhci_erst_entry)*(xhci->erst.num_entries);\n\tif (xhci->erst.entries)\n\t\tdma_free_coherent(&pdev->dev, size,\n\t\t\t\txhci->erst.entries, xhci->erst.erst_dma_addr);\n\txhci->erst.entries = NULL;\n\txhci_dbg(xhci, \"Freed ERST\\n\");\n\tif (xhci->event_ring)\n\t\txhci_ring_free(xhci, xhci->event_ring);\n\txhci->event_ring = NULL;\n\txhci_dbg(xhci, \"Freed event ring\\n\");\n\n\txhci->cmd_ring_reserved_trbs = 0;\n\tif (xhci->cmd_ring)\n\t\txhci_ring_free(xhci, xhci->cmd_ring);\n\txhci->cmd_ring = NULL;\n\txhci_dbg(xhci, \"Freed command ring\\n\");\n\tlist_for_each_entry_safe(cur_cd, next_cd,\n\t\t\t&xhci->cancel_cmd_list, cancel_cmd_list) {\n\t\tlist_del(&cur_cd->cancel_cmd_list);\n\t\tkfree(cur_cd);\n\t}\n\n\tnum_ports = HCS_MAX_PORTS(xhci->hcs_params1);\n\tfor (i = 0; i < num_ports && xhci->rh_bw; i++) {\n\t\tstruct xhci_interval_bw_table *bwt = &xhci->rh_bw[i].bw_table;\n\t\tfor (j = 0; j < XHCI_MAX_INTERVAL; j++) {\n\t\t\tstruct list_head *ep = &bwt->interval_bw[j].endpoints;\n\t\t\twhile (!list_empty(ep))\n\t\t\t\tlist_del_init(ep->next);\n\t\t}\n\t}\n\n\tfor (i = 1; i < MAX_HC_SLOTS; ++i)\n\t\txhci_free_virt_device(xhci, i);\n\n\tif (xhci->segment_pool)\n\t\tdma_pool_destroy(xhci->segment_pool);\n\txhci->segment_pool = NULL;\n\txhci_dbg(xhci, \"Freed segment pool\\n\");\n\n\tif (xhci->device_pool)\n\t\tdma_pool_destroy(xhci->device_pool);\n\txhci->device_pool = NULL;\n\txhci_dbg(xhci, \"Freed device context pool\\n\");\n\n\tif (xhci->small_streams_pool)\n\t\tdma_pool_destroy(xhci->small_streams_pool);\n\txhci->small_streams_pool = NULL;\n\txhci_dbg(xhci, \"Freed small stream array pool\\n\");\n\n\tif (xhci->medium_streams_pool)\n\t\tdma_pool_destroy(xhci->medium_streams_pool);\n\txhci->medium_streams_pool = NULL;\n\txhci_dbg(xhci, \"Freed medium stream array pool\\n\");\n\n\tif (xhci->dcbaa)\n\t\tdma_free_coherent(&pdev->dev, sizeof(*xhci->dcbaa),\n\t\t\t\txhci->dcbaa, xhci->dcbaa->dma);\n\txhci->dcbaa = NULL;\n\n\tscratchpad_free(xhci);\n\n\tspin_lock_irqsave(&xhci->lock, flags);\n\tlist_for_each_entry_safe(dev_info, next, &xhci->lpm_failed_devs, list) {\n\t\tlist_del(&dev_info->list);\n\t\tkfree(dev_info);\n\t}\n\tspin_unlock_irqrestore(&xhci->lock, flags);\n\n\tif (!xhci->rh_bw)\n\t\tgoto no_bw;\n\n\tfor (i = 0; i < num_ports; i++) {\n\t\tstruct xhci_tt_bw_info *tt, *n;\n\t\tlist_for_each_entry_safe(tt, n, &xhci->rh_bw[i].tts, tt_list) {\n\t\t\tlist_del(&tt->tt_list);\n\t\t\tkfree(tt);\n\t\t}\n\t}\n\nno_bw:\n\txhci->num_usb2_ports = 0;\n\txhci->num_usb3_ports = 0;\n\txhci->num_active_eps = 0;\n\tkfree(xhci->usb2_ports);\n\tkfree(xhci->usb3_ports);\n\tkfree(xhci->port_array);\n\tkfree(xhci->rh_bw);\n\n\txhci->usb2_ports = NULL;\n\txhci->usb3_ports = NULL;\n\txhci->port_array = NULL;\n\txhci->rh_bw = NULL;\n\n\txhci->page_size = 0;\n\txhci->page_shift = 0;\n\txhci->bus_state[0].bus_suspended = 0;\n\txhci->bus_state[1].bus_suspended = 0;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void raid5_do_work(struct work_struct *work)\n{\n\tstruct r5worker *worker = container_of(work, struct r5worker, work);\n\tstruct r5worker_group *group = worker->group;\n\tstruct r5conf *conf = group->conf;\n\tstruct mddev *mddev = conf->mddev;\n\tint group_id = group - conf->worker_groups;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5worker active\\n\");\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tint batch_size, released;\n\n\t\treleased = release_stripe_list(conf, worker->temp_inactive_list);\n\n\t\tbatch_size = handle_active_stripes(conf, group_id, worker,\n\t\t\t\t\t\t   worker->temp_inactive_list);\n\t\tworker->working = false;\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\t\twait_event_lock_irq(mddev->sb_wait,\n\t\t\t!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags),\n\t\t\tconf->device_lock);\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5worker inactive\\n\");\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void raid5_do_work(struct work_struct *work)\n{\n\tstruct r5worker *worker = container_of(work, struct r5worker, work);\n\tstruct r5worker_group *group = worker->group;\n\tstruct r5conf *conf = group->conf;\n\tstruct mddev *mddev = conf->mddev;\n\tint group_id = group - conf->worker_groups;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5worker active\\n\");\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tint batch_size, released;\n\n\t\treleased = release_stripe_list(conf, worker->temp_inactive_list);\n\n\t\tbatch_size = handle_active_stripes(conf, group_id, worker,\n\t\t\t\t\t\t   worker->temp_inactive_list);\n\t\tworker->working = false;\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\t\twait_event_lock_irq(mddev->sb_wait,\n\t\t\t!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags),\n\t\t\tconf->device_lock);\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\tflush_deferred_bios(conf);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5worker inactive\\n\");\n}\n\nstatic void flush_deferred_bios(struct r5conf *conf)\n{\n\tstruct bio_list tmp = BIO_EMPTY_LIST;\n\n\tif (conf->pending_data_cnt == 0)\n\t\treturn;\n\n\tspin_lock(&conf->pending_bios_lock);\n\tdispatch_defer_bios(conf, conf->pending_data_cnt, &tmp);\n\tBUG_ON(conf->pending_data_cnt != 0);\n\tspin_unlock(&conf->pending_bios_lock);\n\n\tdispatch_bio_list(&tmp);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void raid5_do_work(struct work_struct *work)\n{\n\tstruct r5worker *worker = container_of(work, struct r5worker, work);\n\tstruct r5worker_group *group = worker->group;\n\tstruct r5conf *conf = group->conf;\n\tint group_id = group - conf->worker_groups;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5worker active\\n\");\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tint batch_size, released;\n\n\t\treleased = release_stripe_list(conf, worker->temp_inactive_list);\n\n\t\tbatch_size = handle_active_stripes(conf, group_id, worker,\n\t\t\t\t\t\t   worker->temp_inactive_list);\n\t\tworker->working = false;\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5worker inactive\\n\");\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void raid5_do_work(struct work_struct *work)\n{\n\tstruct r5worker *worker = container_of(work, struct r5worker, work);\n\tstruct r5worker_group *group = worker->group;\n\tstruct r5conf *conf = group->conf;\n\tint group_id = group - conf->worker_groups;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5worker active\\n\");\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tint batch_size, released;\n\n\t\treleased = release_stripe_list(conf, worker->temp_inactive_list);\n\n\t\tbatch_size = handle_active_stripes(conf, group_id, worker,\n\t\t\t\t\t\t   worker->temp_inactive_list);\n\t\tworker->working = false;\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5worker inactive\\n\");\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint time;\n\tint copied;\n\n\ttcp_mstamp_refresh(tp);\n\ttime = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcvq_space.time);\n\tif (time < (tp->rcv_rtt_est.rtt_us >> 3) || tp->rcv_rtt_est.rtt_us == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t/* A bit of theory :\n\t * copied = bytes received in previous RTT, our base window\n\t * To cope with packet losses, we need a 2x factor\n\t * To cope with slow start, and sender growing its cwin by 100 %\n\t * every RTT, we need a 4x factor, because the ACK we are sending\n\t * now is for the next RTT, not the current one :\n\t * <prev RTT . ><current RTT .. ><next RTT .... >\n\t */\n\n\tif (sock_net(sk)->ipv4.sysctl_tcp_moderate_rcvbuf &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tint rcvwin, rcvmem, rcvbuf;\n\n\t\t/* minimal window to cope with packet losses, assuming\n\t\t * steady state. Add some cushion because of small variations.\n\t\t */\n\t\trcvwin = (copied << 1) + 16 * tp->advmss;\n\n\t\t/* If rate increased by 25%,\n\t\t *\tassume slow start, rcvwin = 3 * copied\n\t\t * If rate increased by 50%,\n\t\t *\tassume sender can use 2x growth, rcvwin = 4 * copied\n\t\t */\n\t\tif (copied >=\n\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 2)) {\n\t\t\tif (copied >=\n\t\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 1))\n\t\t\t\trcvwin <<= 1;\n\t\t\telse\n\t\t\t\trcvwin += (rcvwin >> 1);\n\t\t}\n\n\t\trcvmem = SKB_TRUESIZE(tp->advmss + MAX_TCP_HEADER);\n\t\twhile (tcp_win_from_space(sk, rcvmem) < tp->advmss)\n\t\t\trcvmem += 128;\n\n\t\trcvbuf = min(rcvwin / tp->advmss * rcvmem,\n\t\t\t     sock_net(sk)->ipv4.sysctl_tcp_rmem[2]);\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tsk->sk_rcvbuf = rcvbuf;\n\n\t\t\t/* Make the window clamp follow along.  */\n\t\t\ttp->window_clamp = rcvwin;\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint time;\n\tint copied;\n\n\ttcp_mstamp_refresh(tp);\n\ttime = tcp_stamp_us_delta(tp->tcp_mstamp, tp->rcvq_space.time);\n\tif (time < (tp->rcv_rtt_est.rtt_us >> 3) || tp->rcv_rtt_est.rtt_us == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t/* A bit of theory :\n\t * copied = bytes received in previous RTT, our base window\n\t * To cope with packet losses, we need a 2x factor\n\t * To cope with slow start, and sender growing its cwin by 100 %\n\t * every RTT, we need a 4x factor, because the ACK we are sending\n\t * now is for the next RTT, not the current one :\n\t * <prev RTT . ><current RTT .. ><next RTT .... >\n\t */\n\n\tif (sock_net(sk)->ipv4.sysctl_tcp_moderate_rcvbuf &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tint rcvwin, rcvmem, rcvbuf;\n\n\t\t/* minimal window to cope with packet losses, assuming\n\t\t * steady state. Add some cushion because of small variations.\n\t\t */\n\t\trcvwin = (copied << 1) + 16 * tp->advmss;\n\n\t\t/* If rate increased by 25%,\n\t\t *\tassume slow start, rcvwin = 3 * copied\n\t\t * If rate increased by 50%,\n\t\t *\tassume sender can use 2x growth, rcvwin = 4 * copied\n\t\t */\n\t\tif (copied >=\n\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 2)) {\n\t\t\tif (copied >=\n\t\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 1))\n\t\t\t\trcvwin <<= 1;\n\t\t\telse\n\t\t\t\trcvwin += (rcvwin >> 1);\n\t\t}\n\n\t\trcvmem = SKB_TRUESIZE(tp->advmss + MAX_TCP_HEADER);\n\t\twhile (tcp_win_from_space(sk, rcvmem) < tp->advmss)\n\t\t\trcvmem += 128;\n\n\t\trcvbuf = min(rcvwin / tp->advmss * rcvmem,\n\t\t\t     sock_net(sk)->ipv4.sysctl_tcp_rmem[2]);\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tsk->sk_rcvbuf = rcvbuf;\n\n\t\t\t/* Make the window clamp follow along.  */\n\t\t\ttp->window_clamp = tcp_win_from_space(sk, rcvbuf);\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tp->tcp_mstamp;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 copied;\n\tint time;\n\n\ttime = tcp_time_stamp - tp->rcvq_space.time;\n\tif (time < (tp->rcv_rtt_est.rtt >> 3) || tp->rcv_rtt_est.rtt == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t/* A bit of theory :\n\t * copied = bytes received in previous RTT, our base window\n\t * To cope with packet losses, we need a 2x factor\n\t * To cope with slow start, and sender growing its cwin by 100 %\n\t * every RTT, we need a 4x factor, because the ACK we are sending\n\t * now is for the next RTT, not the current one :\n\t * <prev RTT . ><current RTT .. ><next RTT .... >\n\t */\n\n\tif (sysctl_tcp_moderate_rcvbuf &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tint rcvmem, rcvbuf;\n\t\tu64 rcvwin;\n\n\t\t/* minimal window to cope with packet losses, assuming\n\t\t * steady state. Add some cushion because of small variations.\n\t\t */\n\t\trcvwin = ((u64)copied << 1) + 16 * tp->advmss;\n\n\t\t/* If rate increased by 25%,\n\t\t *\tassume slow start, rcvwin = 3 * copied\n\t\t * If rate increased by 50%,\n\t\t *\tassume sender can use 2x growth, rcvwin = 4 * copied\n\t\t */\n\t\tif (copied >=\n\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 2)) {\n\t\t\tif (copied >=\n\t\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 1))\n\t\t\t\trcvwin <<= 1;\n\t\t\telse\n\t\t\t\trcvwin += (rcvwin >> 1);\n\t\t}\n\n\t\trcvmem = SKB_TRUESIZE(tp->advmss + MAX_TCP_HEADER);\n\t\twhile (tcp_win_from_space(rcvmem) < tp->advmss)\n\t\t\trcvmem += 128;\n\n\t\tdo_div(rcvwin, tp->advmss);\n\t\trcvbuf = min_t(u64, rcvwin * rcvmem, sysctl_tcp_rmem[2]);\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tsk->sk_rcvbuf = rcvbuf;\n\n\t\t\t/* Make the window clamp follow along.  */\n\t\t\ttp->window_clamp = rcvwin;\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tcp_time_stamp;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nvoid tcp_rcv_space_adjust(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 copied;\n\tint time;\n\n\ttime = tcp_time_stamp - tp->rcvq_space.time;\n\tif (time < (tp->rcv_rtt_est.rtt >> 3) || tp->rcv_rtt_est.rtt == 0)\n\t\treturn;\n\n\t/* Number of bytes copied to user in last RTT */\n\tcopied = tp->copied_seq - tp->rcvq_space.seq;\n\tif (copied <= tp->rcvq_space.space)\n\t\tgoto new_measure;\n\n\t/* A bit of theory :\n\t * copied = bytes received in previous RTT, our base window\n\t * To cope with packet losses, we need a 2x factor\n\t * To cope with slow start, and sender growing its cwin by 100 %\n\t * every RTT, we need a 4x factor, because the ACK we are sending\n\t * now is for the next RTT, not the current one :\n\t * <prev RTT . ><current RTT .. ><next RTT .... >\n\t */\n\n\tif (sysctl_tcp_moderate_rcvbuf &&\n\t    !(sk->sk_userlocks & SOCK_RCVBUF_LOCK)) {\n\t\tint rcvmem, rcvbuf;\n\t\tu64 rcvwin;\n\n\t\t/* minimal window to cope with packet losses, assuming\n\t\t * steady state. Add some cushion because of small variations.\n\t\t */\n\t\trcvwin = ((u64)copied << 1) + 16 * tp->advmss;\n\n\t\t/* If rate increased by 25%,\n\t\t *\tassume slow start, rcvwin = 3 * copied\n\t\t * If rate increased by 50%,\n\t\t *\tassume sender can use 2x growth, rcvwin = 4 * copied\n\t\t */\n\t\tif (copied >=\n\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 2)) {\n\t\t\tif (copied >=\n\t\t\t    tp->rcvq_space.space + (tp->rcvq_space.space >> 1))\n\t\t\t\trcvwin <<= 1;\n\t\t\telse\n\t\t\t\trcvwin += (rcvwin >> 1);\n\t\t}\n\n\t\trcvmem = SKB_TRUESIZE(tp->advmss + MAX_TCP_HEADER);\n\t\twhile (tcp_win_from_space(rcvmem) < tp->advmss)\n\t\t\trcvmem += 128;\n\n\t\tdo_div(rcvwin, tp->advmss);\n\t\trcvbuf = min_t(u64, rcvwin * rcvmem, sysctl_tcp_rmem[2]);\n\t\tif (rcvbuf > sk->sk_rcvbuf) {\n\t\t\tsk->sk_rcvbuf = rcvbuf;\n\n\t\t\t/* Make the window clamp follow along.  */\n\t\t\ttp->window_clamp = tcp_win_from_space(rcvbuf);\n\t\t}\n\t}\n\ttp->rcvq_space.space = copied;\n\nnew_measure:\n\ttp->rcvq_space.seq = tp->copied_seq;\n\ttp->rcvq_space.time = tcp_time_stamp;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int ip_frag_reasm(struct ipq *qp, struct sk_buff *prev,\n\t\t\t struct net_device *dev)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct iphdr *iph;\n\tstruct sk_buff *fp, *head = qp->q.fragments;\n\tint len;\n\tint ihlen;\n\tint err;\n\tint sum_truesize;\n\tu8 ecn;\n\n\tipq_kill(qp);\n\n\tecn = ip_frag_ecn_table[qp->ecn];\n\tif (unlikely(ecn == 0xff)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fail;\n\t}\n\t/* Make the one we just received the head. */\n\tif (prev) {\n\t\thead = prev->next;\n\t\tfp = skb_clone(head, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\tgoto out_nomem;\n\n\t\tfp->next = head->next;\n\t\tif (!fp->next)\n\t\t\tqp->q.fragments_tail = fp;\n\t\tprev->next = fp;\n\n\t\tskb_morph(head, qp->q.fragments);\n\t\thead->next = qp->q.fragments->next;\n\n\t\tconsume_skb(qp->q.fragments);\n\t\tqp->q.fragments = head;\n\t}\n\n\tWARN_ON(!head);\n\tWARN_ON(FRAG_CB(head)->offset != 0);\n\n\t/* Allocate a new buffer for the datagram. */\n\tihlen = ip_hdrlen(head);\n\tlen = ihlen + qp->q.len;\n\n\terr = -E2BIG;\n\tif (len > 65535)\n\t\tgoto out_oversize;\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\tgoto out_nomem;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!clone)\n\t\t\tgoto out_nomem;\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\t\tadd_frag_mem_limit(&qp->q, clone->truesize);\n\t}\n\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tsum_truesize = head->truesize;\n\tfor (fp = head->next; fp;) {\n\t\tbool headstolen;\n\t\tint delta;\n\t\tstruct sk_buff *next = fp->next;\n\n\t\tsum_truesize += fp->truesize;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\n\t\tif (skb_try_coalesce(head, fp, &headstolen, &delta)) {\n\t\t\tkfree_skb_partial(fp, headstolen);\n\t\t} else {\n\t\t\tif (!skb_shinfo(head)->frag_list)\n\t\t\t\tskb_shinfo(head)->frag_list = fp;\n\t\t\thead->data_len += fp->len;\n\t\t\thead->len += fp->len;\n\t\t\thead->truesize += fp->truesize;\n\t\t}\n\t\tfp = next;\n\t}\n\tsub_frag_mem_limit(&qp->q, sum_truesize);\n\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = qp->q.stamp;\n\tIPCB(head)->frag_max_size = max(qp->max_df_size, qp->q.max_size);\n\n\tiph = ip_hdr(head);\n\tiph->tot_len = htons(len);\n\tiph->tos |= ecn;\n\n\t/* When we set IP_DF on a refragmented skb we must also force a\n\t * call to ip_fragment to avoid forwarding a DF-skb of size s while\n\t * original sender only sent fragments of size f (where f < s).\n\t *\n\t * We only set DF/IPSKB_FRAG_PMTU if such DF fragment was the largest\n\t * frag seen to avoid sending tiny DF-fragments in case skb was built\n\t * from one very small df-fragment and one large non-df frag.\n\t */\n\tif (qp->max_df_size == qp->q.max_size) {\n\t\tIPCB(head)->flags |= IPSKB_FRAG_PMTU;\n\t\tiph->frag_off = htons(IP_DF);\n\t} else {\n\t\tiph->frag_off = 0;\n\t}\n\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMOKS);\n\tqp->q.fragments = NULL;\n\tqp->q.fragments_tail = NULL;\n\treturn 0;\n\nout_nomem:\n\tnet_dbg_ratelimited(\"queue_glue: no memory for gluing queue %p\\n\", qp);\n\terr = -ENOMEM;\n\tgoto out_fail;\nout_oversize:\n\tnet_info_ratelimited(\"Oversized IP packet from %pI4\\n\", &qp->saddr);\nout_fail:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);\n\treturn err;\n}\n\nstatic int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n{\n\tstruct sk_buff *prev, *next;\n\tstruct net_device *dev;\n\tunsigned int fragsize;\n\tint flags, offset;\n\tint ihl, end;\n\tint err = -ENOENT;\n\tu8 ecn;\n\n\tif (qp->q.flags & INET_FRAG_COMPLETE)\n\t\tgoto err;\n\n\tif (!(IPCB(skb)->flags & IPSKB_FRAG_COMPLETE) &&\n\t    unlikely(ip_frag_too_far(qp)) &&\n\t    unlikely(err = ip_frag_reinit(qp))) {\n\t\tipq_kill(qp);\n\t\tgoto err;\n\t}\n\n\tecn = ip4_frag_ecn(ip_hdr(skb)->tos);\n\toffset = ntohs(ip_hdr(skb)->frag_off);\n\tflags = offset & ~IP_OFFSET;\n\toffset &= IP_OFFSET;\n\toffset <<= 3;\t\t/* offset is in 8-byte chunks */\n\tihl = ip_hdrlen(skb);\n\n\t/* Determine the position of this fragment. */\n\tend = offset + skb->len - ihl;\n\terr = -EINVAL;\n\n\t/* Is this the final fragment? */\n\tif ((flags & IP_MF) == 0) {\n\t\t/* If we already have some bits beyond end\n\t\t * or have different end, the segment is corrupted.\n\t\t */\n\t\tif (end < qp->q.len ||\n\t\t    ((qp->q.flags & INET_FRAG_LAST_IN) && end != qp->q.len))\n\t\t\tgoto err;\n\t\tqp->q.flags |= INET_FRAG_LAST_IN;\n\t\tqp->q.len = end;\n\t} else {\n\t\tif (end&7) {\n\t\t\tend &= ~7;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t\tif (end > qp->q.len) {\n\t\t\t/* Some bits beyond end -> corruption. */\n\t\t\tif (qp->q.flags & INET_FRAG_LAST_IN)\n\t\t\t\tgoto err;\n\t\t\tqp->q.len = end;\n\t\t}\n\t}\n\tif (end == offset)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (!pskb_pull(skb, ihl))\n\t\tgoto err;\n\n\terr = pskb_trim_rcsum(skb, end - offset);\n\tif (err)\n\t\tgoto err;\n\n\t/* Find out which fragments are in front and at the back of us\n\t * in the chain of fragments so far.  We must know where to put\n\t * this fragment, right?\n\t */\n\tprev = qp->q.fragments_tail;\n\tif (!prev || FRAG_CB(prev)->offset < offset) {\n\t\tnext = NULL;\n\t\tgoto found;\n\t}\n\tprev = NULL;\n\tfor (next = qp->q.fragments; next != NULL; next = next->next) {\n\t\tif (FRAG_CB(next)->offset >= offset)\n\t\t\tbreak;\t/* bingo! */\n\t\tprev = next;\n\t}\n\nfound:\n\t/* We found where to put this one.  Check for overlap with\n\t * preceding fragment, and, if needed, align things so that\n\t * any overlaps are eliminated.\n\t */\n\tif (prev) {\n\t\tint i = (FRAG_CB(prev)->offset + prev->len) - offset;\n\n\t\tif (i > 0) {\n\t\t\toffset += i;\n\t\t\terr = -EINVAL;\n\t\t\tif (end <= offset)\n\t\t\t\tgoto err;\n\t\t\terr = -ENOMEM;\n\t\t\tif (!pskb_pull(skb, i))\n\t\t\t\tgoto err;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\twhile (next && FRAG_CB(next)->offset < end) {\n\t\tint i = end - FRAG_CB(next)->offset; /* overlap is 'i' bytes */\n\n\t\tif (i < next->len) {\n\t\t\t/* Eat head of the next overlapped fragment\n\t\t\t * and leave the loop. The next ones cannot overlap.\n\t\t\t */\n\t\t\tif (!pskb_pull(next, i))\n\t\t\t\tgoto err;\n\t\t\tFRAG_CB(next)->offset += i;\n\t\t\tqp->q.meat -= i;\n\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tnext->ip_summed = CHECKSUM_NONE;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *free_it = next;\n\n\t\t\t/* Old fragment is completely overridden with\n\t\t\t * new one drop it.\n\t\t\t */\n\t\t\tnext = next->next;\n\n\t\t\tif (prev)\n\t\t\t\tprev->next = next;\n\t\t\telse\n\t\t\t\tqp->q.fragments = next;\n\n\t\t\tqp->q.meat -= free_it->len;\n\t\t\tsub_frag_mem_limit(&qp->q, free_it->truesize);\n\t\t\tkfree_skb(free_it);\n\t\t}\n\t}\n\n\tFRAG_CB(skb)->offset = offset;\n\n\t/* Insert this fragment in the chain of fragments. */\n\tskb->next = next;\n\tif (!next)\n\t\tqp->q.fragments_tail = skb;\n\tif (prev)\n\t\tprev->next = skb;\n\telse\n\t\tqp->q.fragments = skb;\n\n\tdev = skb->dev;\n\tif (dev) {\n\t\tqp->iif = dev->ifindex;\n\t\tskb->dev = NULL;\n\t}\n\tqp->q.stamp = skb->tstamp;\n\tqp->q.meat += skb->len;\n\tqp->ecn |= ecn;\n\tadd_frag_mem_limit(&qp->q, skb->truesize);\n\tif (offset == 0)\n\t\tqp->q.flags |= INET_FRAG_FIRST_IN;\n\n\tfragsize = skb->len + ihl;\n\n\tif (fragsize > qp->q.max_size)\n\t\tqp->q.max_size = fragsize;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_DF) &&\n\t    fragsize > qp->max_df_size)\n\t\tqp->max_df_size = fragsize;\n\n\tif (qp->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    qp->q.meat == qp->q.len) {\n\t\tunsigned long orefdst = skb->_skb_refdst;\n\n\t\tskb->_skb_refdst = 0UL;\n\t\terr = ip_frag_reasm(qp, prev, dev);\n\t\tskb->_skb_refdst = orefdst;\n\t\treturn err;\n\t}\n\n\tskb_dst_drop(skb);\n\treturn -EINPROGRESS;\n\nerr:\n\tkfree_skb(skb);\n\treturn err;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n{\n\tstruct sk_buff *prev, *next;\n\tstruct net_device *dev;\n\tunsigned int fragsize;\n\tint flags, offset;\n\tint ihl, end;\n\tint err = -ENOENT;\n\tu8 ecn;\n\n\tif (qp->q.flags & INET_FRAG_COMPLETE)\n\t\tgoto err;\n\n\tif (!(IPCB(skb)->flags & IPSKB_FRAG_COMPLETE) &&\n\t    unlikely(ip_frag_too_far(qp)) &&\n\t    unlikely(err = ip_frag_reinit(qp))) {\n\t\tipq_kill(qp);\n\t\tgoto err;\n\t}\n\n\tecn = ip4_frag_ecn(ip_hdr(skb)->tos);\n\toffset = ntohs(ip_hdr(skb)->frag_off);\n\tflags = offset & ~IP_OFFSET;\n\toffset &= IP_OFFSET;\n\toffset <<= 3;\t\t/* offset is in 8-byte chunks */\n\tihl = ip_hdrlen(skb);\n\n\t/* Determine the position of this fragment. */\n\tend = offset + skb->len - skb_network_offset(skb) - ihl;\n\terr = -EINVAL;\n\n\t/* Is this the final fragment? */\n\tif ((flags & IP_MF) == 0) {\n\t\t/* If we already have some bits beyond end\n\t\t * or have different end, the segment is corrupted.\n\t\t */\n\t\tif (end < qp->q.len ||\n\t\t    ((qp->q.flags & INET_FRAG_LAST_IN) && end != qp->q.len))\n\t\t\tgoto err;\n\t\tqp->q.flags |= INET_FRAG_LAST_IN;\n\t\tqp->q.len = end;\n\t} else {\n\t\tif (end&7) {\n\t\t\tend &= ~7;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t\tif (end > qp->q.len) {\n\t\t\t/* Some bits beyond end -> corruption. */\n\t\t\tif (qp->q.flags & INET_FRAG_LAST_IN)\n\t\t\t\tgoto err;\n\t\t\tqp->q.len = end;\n\t\t}\n\t}\n\tif (end == offset)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (!pskb_pull(skb, skb_network_offset(skb) + ihl))\n\t\tgoto err;\n\n\terr = pskb_trim_rcsum(skb, end - offset);\n\tif (err)\n\t\tgoto err;\n\n\t/* Find out which fragments are in front and at the back of us\n\t * in the chain of fragments so far.  We must know where to put\n\t * this fragment, right?\n\t */\n\tprev = qp->q.fragments_tail;\n\tif (!prev || FRAG_CB(prev)->offset < offset) {\n\t\tnext = NULL;\n\t\tgoto found;\n\t}\n\tprev = NULL;\n\tfor (next = qp->q.fragments; next != NULL; next = next->next) {\n\t\tif (FRAG_CB(next)->offset >= offset)\n\t\t\tbreak;\t/* bingo! */\n\t\tprev = next;\n\t}\n\nfound:\n\t/* We found where to put this one.  Check for overlap with\n\t * preceding fragment, and, if needed, align things so that\n\t * any overlaps are eliminated.\n\t */\n\tif (prev) {\n\t\tint i = (FRAG_CB(prev)->offset + prev->len) - offset;\n\n\t\tif (i > 0) {\n\t\t\toffset += i;\n\t\t\terr = -EINVAL;\n\t\t\tif (end <= offset)\n\t\t\t\tgoto err;\n\t\t\terr = -ENOMEM;\n\t\t\tif (!pskb_pull(skb, i))\n\t\t\t\tgoto err;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\twhile (next && FRAG_CB(next)->offset < end) {\n\t\tint i = end - FRAG_CB(next)->offset; /* overlap is 'i' bytes */\n\n\t\tif (i < next->len) {\n\t\t\t/* Eat head of the next overlapped fragment\n\t\t\t * and leave the loop. The next ones cannot overlap.\n\t\t\t */\n\t\t\tif (!pskb_pull(next, i))\n\t\t\t\tgoto err;\n\t\t\tFRAG_CB(next)->offset += i;\n\t\t\tqp->q.meat -= i;\n\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tnext->ip_summed = CHECKSUM_NONE;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *free_it = next;\n\n\t\t\t/* Old fragment is completely overridden with\n\t\t\t * new one drop it.\n\t\t\t */\n\t\t\tnext = next->next;\n\n\t\t\tif (prev)\n\t\t\t\tprev->next = next;\n\t\t\telse\n\t\t\t\tqp->q.fragments = next;\n\n\t\t\tqp->q.meat -= free_it->len;\n\t\t\tsub_frag_mem_limit(&qp->q, free_it->truesize);\n\t\t\tkfree_skb(free_it);\n\t\t}\n\t}\n\n\tFRAG_CB(skb)->offset = offset;\n\n\t/* Insert this fragment in the chain of fragments. */\n\tskb->next = next;\n\tif (!next)\n\t\tqp->q.fragments_tail = skb;\n\tif (prev)\n\t\tprev->next = skb;\n\telse\n\t\tqp->q.fragments = skb;\n\n\tdev = skb->dev;\n\tif (dev) {\n\t\tqp->iif = dev->ifindex;\n\t\tskb->dev = NULL;\n\t}\n\tqp->q.stamp = skb->tstamp;\n\tqp->q.meat += skb->len;\n\tqp->ecn |= ecn;\n\tadd_frag_mem_limit(&qp->q, skb->truesize);\n\tif (offset == 0)\n\t\tqp->q.flags |= INET_FRAG_FIRST_IN;\n\n\tfragsize = skb->len + ihl;\n\n\tif (fragsize > qp->q.max_size)\n\t\tqp->q.max_size = fragsize;\n\n\tif (ip_hdr(skb)->frag_off & htons(IP_DF) &&\n\t    fragsize > qp->max_df_size)\n\t\tqp->max_df_size = fragsize;\n\n\tif (qp->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    qp->q.meat == qp->q.len) {\n\t\tunsigned long orefdst = skb->_skb_refdst;\n\n\t\tskb->_skb_refdst = 0UL;\n\t\terr = ip_frag_reasm(qp, prev, dev);\n\t\tskb->_skb_refdst = orefdst;\n\t\treturn err;\n\t}\n\n\tskb_dst_drop(skb);\n\treturn -EINPROGRESS;\n\nerr:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic int ip_frag_reasm(struct ipq *qp, struct sk_buff *prev,\n\t\t\t struct net_device *dev)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct iphdr *iph;\n\tstruct sk_buff *fp, *head = qp->q.fragments;\n\tint len;\n\tint ihlen;\n\tint err;\n\tint sum_truesize;\n\tu8 ecn;\n\n\tipq_kill(qp);\n\n\tecn = ip_frag_ecn_table[qp->ecn];\n\tif (unlikely(ecn == 0xff)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fail;\n\t}\n\t/* Make the one we just received the head. */\n\tif (prev) {\n\t\thead = prev->next;\n\t\tfp = skb_clone(head, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\tgoto out_nomem;\n\n\t\tfp->next = head->next;\n\t\tif (!fp->next)\n\t\t\tqp->q.fragments_tail = fp;\n\t\tprev->next = fp;\n\n\t\tskb_morph(head, qp->q.fragments);\n\t\thead->next = qp->q.fragments->next;\n\n\t\tconsume_skb(qp->q.fragments);\n\t\tqp->q.fragments = head;\n\t}\n\n\tWARN_ON(!head);\n\tWARN_ON(FRAG_CB(head)->offset != 0);\n\n\t/* Allocate a new buffer for the datagram. */\n\tihlen = ip_hdrlen(head);\n\tlen = ihlen + qp->q.len;\n\n\terr = -E2BIG;\n\tif (len > 65535)\n\t\tgoto out_oversize;\n\n\t/* Head of list must not be cloned. */\n\tif (skb_unclone(head, GFP_ATOMIC))\n\t\tgoto out_nomem;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tclone = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!clone)\n\t\t\tgoto out_nomem;\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\t\tadd_frag_mem_limit(&qp->q, clone->truesize);\n\t}\n\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tsum_truesize = head->truesize;\n\tfor (fp = head->next; fp;) {\n\t\tbool headstolen;\n\t\tint delta;\n\t\tstruct sk_buff *next = fp->next;\n\n\t\tsum_truesize += fp->truesize;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\n\t\tif (skb_try_coalesce(head, fp, &headstolen, &delta)) {\n\t\t\tkfree_skb_partial(fp, headstolen);\n\t\t} else {\n\t\t\tif (!skb_shinfo(head)->frag_list)\n\t\t\t\tskb_shinfo(head)->frag_list = fp;\n\t\t\thead->data_len += fp->len;\n\t\t\thead->len += fp->len;\n\t\t\thead->truesize += fp->truesize;\n\t\t}\n\t\tfp = next;\n\t}\n\tsub_frag_mem_limit(&qp->q, sum_truesize);\n\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = qp->q.stamp;\n\tIPCB(head)->frag_max_size = max(qp->max_df_size, qp->q.max_size);\n\n\tiph = ip_hdr(head);\n\tiph->tot_len = htons(len);\n\tiph->tos |= ecn;\n\n\t/* When we set IP_DF on a refragmented skb we must also force a\n\t * call to ip_fragment to avoid forwarding a DF-skb of size s while\n\t * original sender only sent fragments of size f (where f < s).\n\t *\n\t * We only set DF/IPSKB_FRAG_PMTU if such DF fragment was the largest\n\t * frag seen to avoid sending tiny DF-fragments in case skb was built\n\t * from one very small df-fragment and one large non-df frag.\n\t */\n\tif (qp->max_df_size == qp->q.max_size) {\n\t\tIPCB(head)->flags |= IPSKB_FRAG_PMTU;\n\t\tiph->frag_off = htons(IP_DF);\n\t} else {\n\t\tiph->frag_off = 0;\n\t}\n\n\tip_send_check(iph);\n\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMOKS);\n\tqp->q.fragments = NULL;\n\tqp->q.fragments_tail = NULL;\n\treturn 0;\n\nout_nomem:\n\tnet_dbg_ratelimited(\"queue_glue: no memory for gluing queue %p\\n\", qp);\n\terr = -ENOMEM;\n\tgoto out_fail;\nout_oversize:\n\tnet_info_ratelimited(\"Oversized IP packet from %pI4\\n\", &qp->saddr);\nout_fail:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);\n\treturn err;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int ip_frag_reasm(struct ipq *qp, struct sk_buff *prev,\n\t\t\t struct net_device *dev)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct iphdr *iph;\n\tstruct sk_buff *fp, *head = qp->q.fragments;\n\tint len;\n\tint ihlen;\n\tint err;\n\tu8 ecn;\n\n\tipq_kill(qp);\n\n\tecn = ip4_frag_ecn_table[qp->ecn];\n\tif (unlikely(ecn == 0xff)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fail;\n\t}\n\t/* Make the one we just received the head. */\n\tif (prev) {\n\t\thead = prev->next;\n\t\tfp = skb_clone(head, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\tgoto out_nomem;\n\n\t\tfp->next = head->next;\n\t\tif (!fp->next)\n\t\t\tqp->q.fragments_tail = fp;\n\t\tprev->next = fp;\n\n\t\tskb_morph(head, qp->q.fragments);\n\t\thead->next = qp->q.fragments->next;\n\n\t\tkfree_skb(qp->q.fragments);\n\t\tqp->q.fragments = head;\n\t}\n\n\tWARN_ON(head == NULL);\n\tWARN_ON(FRAG_CB(head)->offset != 0);\n\n\t/* Allocate a new buffer for the datagram. */\n\tihlen = ip_hdrlen(head);\n\tlen = ihlen + qp->q.len;\n\n\terr = -E2BIG;\n\tif (len > 65535)\n\t\tgoto out_oversize;\n\n\t/* Head of list must not be cloned. */\n\tif (skb_cloned(head) && pskb_expand_head(head, 0, 0, GFP_ATOMIC))\n\t\tgoto out_nomem;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tif ((clone = alloc_skb(0, GFP_ATOMIC)) == NULL)\n\t\t\tgoto out_nomem;\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\t\tatomic_add(clone->truesize, &qp->q.net->mem);\n\t}\n\n\tskb_shinfo(head)->frag_list = head->next;\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tfor (fp=head->next; fp; fp = fp->next) {\n\t\thead->data_len += fp->len;\n\t\thead->len += fp->len;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\thead->truesize += fp->truesize;\n\t}\n\tatomic_sub(head->truesize, &qp->q.net->mem);\n\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = qp->q.stamp;\n\n\tiph = ip_hdr(head);\n\tiph->frag_off = 0;\n\tiph->tot_len = htons(len);\n\tiph->tos |= ecn;\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMOKS);\n\tqp->q.fragments = NULL;\n\tqp->q.fragments_tail = NULL;\n\treturn 0;\n\nout_nomem:\n\tLIMIT_NETDEBUG(KERN_ERR \"IP: queue_glue: no memory for gluing \"\n\t\t\t      \"queue %p\\n\", qp);\n\terr = -ENOMEM;\n\tgoto out_fail;\nout_oversize:\n\tif (net_ratelimit())\n\t\tprintk(KERN_INFO \"Oversized IP packet from %pI4.\\n\",\n\t\t\t&qp->saddr);\nout_fail:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);\n\treturn err;\n}\n\nstatic int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n{\n\tstruct sk_buff *prev, *next;\n\tstruct net_device *dev;\n\tint flags, offset;\n\tint ihl, end;\n\tint err = -ENOENT;\n\tu8 ecn;\n\n\tif (qp->q.last_in & INET_FRAG_COMPLETE)\n\t\tgoto err;\n\n\tif (!(IPCB(skb)->flags & IPSKB_FRAG_COMPLETE) &&\n\t    unlikely(ip_frag_too_far(qp)) &&\n\t    unlikely(err = ip_frag_reinit(qp))) {\n\t\tipq_kill(qp);\n\t\tgoto err;\n\t}\n\n\tecn = ip4_frag_ecn(ip_hdr(skb)->tos);\n\toffset = ntohs(ip_hdr(skb)->frag_off);\n\tflags = offset & ~IP_OFFSET;\n\toffset &= IP_OFFSET;\n\toffset <<= 3;\t\t/* offset is in 8-byte chunks */\n\tihl = ip_hdrlen(skb);\n\n\t/* Determine the position of this fragment. */\n\tend = offset + skb->len - ihl;\n\terr = -EINVAL;\n\n\t/* Is this the final fragment? */\n\tif ((flags & IP_MF) == 0) {\n\t\t/* If we already have some bits beyond end\n\t\t * or have different end, the segment is corrrupted.\n\t\t */\n\t\tif (end < qp->q.len ||\n\t\t    ((qp->q.last_in & INET_FRAG_LAST_IN) && end != qp->q.len))\n\t\t\tgoto err;\n\t\tqp->q.last_in |= INET_FRAG_LAST_IN;\n\t\tqp->q.len = end;\n\t} else {\n\t\tif (end&7) {\n\t\t\tend &= ~7;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t\tif (end > qp->q.len) {\n\t\t\t/* Some bits beyond end -> corruption. */\n\t\t\tif (qp->q.last_in & INET_FRAG_LAST_IN)\n\t\t\t\tgoto err;\n\t\t\tqp->q.len = end;\n\t\t}\n\t}\n\tif (end == offset)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (pskb_pull(skb, ihl) == NULL)\n\t\tgoto err;\n\n\terr = pskb_trim_rcsum(skb, end - offset);\n\tif (err)\n\t\tgoto err;\n\n\t/* Find out which fragments are in front and at the back of us\n\t * in the chain of fragments so far.  We must know where to put\n\t * this fragment, right?\n\t */\n\tprev = qp->q.fragments_tail;\n\tif (!prev || FRAG_CB(prev)->offset < offset) {\n\t\tnext = NULL;\n\t\tgoto found;\n\t}\n\tprev = NULL;\n\tfor (next = qp->q.fragments; next != NULL; next = next->next) {\n\t\tif (FRAG_CB(next)->offset >= offset)\n\t\t\tbreak;\t/* bingo! */\n\t\tprev = next;\n\t}\n\nfound:\n\t/* We found where to put this one.  Check for overlap with\n\t * preceding fragment, and, if needed, align things so that\n\t * any overlaps are eliminated.\n\t */\n\tif (prev) {\n\t\tint i = (FRAG_CB(prev)->offset + prev->len) - offset;\n\n\t\tif (i > 0) {\n\t\t\toffset += i;\n\t\t\terr = -EINVAL;\n\t\t\tif (end <= offset)\n\t\t\t\tgoto err;\n\t\t\terr = -ENOMEM;\n\t\t\tif (!pskb_pull(skb, i))\n\t\t\t\tgoto err;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\twhile (next && FRAG_CB(next)->offset < end) {\n\t\tint i = end - FRAG_CB(next)->offset; /* overlap is 'i' bytes */\n\n\t\tif (i < next->len) {\n\t\t\t/* Eat head of the next overlapped fragment\n\t\t\t * and leave the loop. The next ones cannot overlap.\n\t\t\t */\n\t\t\tif (!pskb_pull(next, i))\n\t\t\t\tgoto err;\n\t\t\tFRAG_CB(next)->offset += i;\n\t\t\tqp->q.meat -= i;\n\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tnext->ip_summed = CHECKSUM_NONE;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *free_it = next;\n\n\t\t\t/* Old fragment is completely overridden with\n\t\t\t * new one drop it.\n\t\t\t */\n\t\t\tnext = next->next;\n\n\t\t\tif (prev)\n\t\t\t\tprev->next = next;\n\t\t\telse\n\t\t\t\tqp->q.fragments = next;\n\n\t\t\tqp->q.meat -= free_it->len;\n\t\t\tfrag_kfree_skb(qp->q.net, free_it);\n\t\t}\n\t}\n\n\tFRAG_CB(skb)->offset = offset;\n\n\t/* Insert this fragment in the chain of fragments. */\n\tskb->next = next;\n\tif (!next)\n\t\tqp->q.fragments_tail = skb;\n\tif (prev)\n\t\tprev->next = skb;\n\telse\n\t\tqp->q.fragments = skb;\n\n\tdev = skb->dev;\n\tif (dev) {\n\t\tqp->iif = dev->ifindex;\n\t\tskb->dev = NULL;\n\t}\n\tqp->q.stamp = skb->tstamp;\n\tqp->q.meat += skb->len;\n\tqp->ecn |= ecn;\n\tatomic_add(skb->truesize, &qp->q.net->mem);\n\tif (offset == 0)\n\t\tqp->q.last_in |= INET_FRAG_FIRST_IN;\n\n\tif (qp->q.last_in == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    qp->q.meat == qp->q.len) {\n\t\tunsigned long orefdst = skb->_skb_refdst;\n\n\t\tskb->_skb_refdst = 0UL;\n\t\terr = ip_frag_reasm(qp, prev, dev);\n\t\tskb->_skb_refdst = orefdst;\n\t\treturn err;\n\t}\n\n\tskb_dst_drop(skb);\n\n\twrite_lock(&ip4_frags.lock);\n\tlist_move_tail(&qp->q.lru_list, &qp->q.net->lru_list);\n\twrite_unlock(&ip4_frags.lock);\n\treturn -EINPROGRESS;\n\nerr:\n\tkfree_skb(skb);\n\treturn err;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)\n{\n\tstruct sk_buff *prev, *next;\n\tstruct net_device *dev;\n\tint flags, offset;\n\tint ihl, end;\n\tint err = -ENOENT;\n\tu8 ecn;\n\n\tif (qp->q.last_in & INET_FRAG_COMPLETE)\n\t\tgoto err;\n\n\tif (!(IPCB(skb)->flags & IPSKB_FRAG_COMPLETE) &&\n\t    unlikely(ip_frag_too_far(qp)) &&\n\t    unlikely(err = ip_frag_reinit(qp))) {\n\t\tipq_kill(qp);\n\t\tgoto err;\n\t}\n\n\tecn = ip4_frag_ecn(ip_hdr(skb)->tos);\n\toffset = ntohs(ip_hdr(skb)->frag_off);\n\tflags = offset & ~IP_OFFSET;\n\toffset &= IP_OFFSET;\n\toffset <<= 3;\t\t/* offset is in 8-byte chunks */\n\tihl = ip_hdrlen(skb);\n\n\t/* Determine the position of this fragment. */\n\tend = offset + skb->len - skb_network_offset(skb) - ihl;\n\terr = -EINVAL;\n\n\t/* Is this the final fragment? */\n\tif ((flags & IP_MF) == 0) {\n\t\t/* If we already have some bits beyond end\n\t\t * or have different end, the segment is corrrupted.\n\t\t */\n\t\tif (end < qp->q.len ||\n\t\t    ((qp->q.last_in & INET_FRAG_LAST_IN) && end != qp->q.len))\n\t\t\tgoto err;\n\t\tqp->q.last_in |= INET_FRAG_LAST_IN;\n\t\tqp->q.len = end;\n\t} else {\n\t\tif (end&7) {\n\t\t\tend &= ~7;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t\tif (end > qp->q.len) {\n\t\t\t/* Some bits beyond end -> corruption. */\n\t\t\tif (qp->q.last_in & INET_FRAG_LAST_IN)\n\t\t\t\tgoto err;\n\t\t\tqp->q.len = end;\n\t\t}\n\t}\n\tif (end == offset)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (!pskb_pull(skb, skb_network_offset(skb) + ihl))\n\t\tgoto err;\n\n\terr = pskb_trim_rcsum(skb, end - offset);\n\tif (err)\n\t\tgoto err;\n\n\t/* Find out which fragments are in front and at the back of us\n\t * in the chain of fragments so far.  We must know where to put\n\t * this fragment, right?\n\t */\n\tprev = qp->q.fragments_tail;\n\tif (!prev || FRAG_CB(prev)->offset < offset) {\n\t\tnext = NULL;\n\t\tgoto found;\n\t}\n\tprev = NULL;\n\tfor (next = qp->q.fragments; next != NULL; next = next->next) {\n\t\tif (FRAG_CB(next)->offset >= offset)\n\t\t\tbreak;\t/* bingo! */\n\t\tprev = next;\n\t}\n\nfound:\n\t/* We found where to put this one.  Check for overlap with\n\t * preceding fragment, and, if needed, align things so that\n\t * any overlaps are eliminated.\n\t */\n\tif (prev) {\n\t\tint i = (FRAG_CB(prev)->offset + prev->len) - offset;\n\n\t\tif (i > 0) {\n\t\t\toffset += i;\n\t\t\terr = -EINVAL;\n\t\t\tif (end <= offset)\n\t\t\t\tgoto err;\n\t\t\terr = -ENOMEM;\n\t\t\tif (!pskb_pull(skb, i))\n\t\t\t\tgoto err;\n\t\t\tif (skb->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\twhile (next && FRAG_CB(next)->offset < end) {\n\t\tint i = end - FRAG_CB(next)->offset; /* overlap is 'i' bytes */\n\n\t\tif (i < next->len) {\n\t\t\t/* Eat head of the next overlapped fragment\n\t\t\t * and leave the loop. The next ones cannot overlap.\n\t\t\t */\n\t\t\tif (!pskb_pull(next, i))\n\t\t\t\tgoto err;\n\t\t\tFRAG_CB(next)->offset += i;\n\t\t\tqp->q.meat -= i;\n\t\t\tif (next->ip_summed != CHECKSUM_UNNECESSARY)\n\t\t\t\tnext->ip_summed = CHECKSUM_NONE;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct sk_buff *free_it = next;\n\n\t\t\t/* Old fragment is completely overridden with\n\t\t\t * new one drop it.\n\t\t\t */\n\t\t\tnext = next->next;\n\n\t\t\tif (prev)\n\t\t\t\tprev->next = next;\n\t\t\telse\n\t\t\t\tqp->q.fragments = next;\n\n\t\t\tqp->q.meat -= free_it->len;\n\t\t\tfrag_kfree_skb(qp->q.net, free_it);\n\t\t}\n\t}\n\n\tFRAG_CB(skb)->offset = offset;\n\n\t/* Insert this fragment in the chain of fragments. */\n\tskb->next = next;\n\tif (!next)\n\t\tqp->q.fragments_tail = skb;\n\tif (prev)\n\t\tprev->next = skb;\n\telse\n\t\tqp->q.fragments = skb;\n\n\tdev = skb->dev;\n\tif (dev) {\n\t\tqp->iif = dev->ifindex;\n\t\tskb->dev = NULL;\n\t}\n\tqp->q.stamp = skb->tstamp;\n\tqp->q.meat += skb->len;\n\tqp->ecn |= ecn;\n\tatomic_add(skb->truesize, &qp->q.net->mem);\n\tif (offset == 0)\n\t\tqp->q.last_in |= INET_FRAG_FIRST_IN;\n\n\tif (qp->q.last_in == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    qp->q.meat == qp->q.len) {\n\t\tunsigned long orefdst = skb->_skb_refdst;\n\n\t\tskb->_skb_refdst = 0UL;\n\t\terr = ip_frag_reasm(qp, prev, dev);\n\t\tskb->_skb_refdst = orefdst;\n\t\treturn err;\n\t}\n\n\tskb_dst_drop(skb);\n\n\twrite_lock(&ip4_frags.lock);\n\tlist_move_tail(&qp->q.lru_list, &qp->q.net->lru_list);\n\twrite_unlock(&ip4_frags.lock);\n\treturn -EINPROGRESS;\n\nerr:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic int ip_frag_reasm(struct ipq *qp, struct sk_buff *prev,\n\t\t\t struct net_device *dev)\n{\n\tstruct net *net = container_of(qp->q.net, struct net, ipv4.frags);\n\tstruct iphdr *iph;\n\tstruct sk_buff *fp, *head = qp->q.fragments;\n\tint len;\n\tint ihlen;\n\tint err;\n\tu8 ecn;\n\n\tipq_kill(qp);\n\n\tecn = ip4_frag_ecn_table[qp->ecn];\n\tif (unlikely(ecn == 0xff)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fail;\n\t}\n\t/* Make the one we just received the head. */\n\tif (prev) {\n\t\thead = prev->next;\n\t\tfp = skb_clone(head, GFP_ATOMIC);\n\t\tif (!fp)\n\t\t\tgoto out_nomem;\n\n\t\tfp->next = head->next;\n\t\tif (!fp->next)\n\t\t\tqp->q.fragments_tail = fp;\n\t\tprev->next = fp;\n\n\t\tskb_morph(head, qp->q.fragments);\n\t\thead->next = qp->q.fragments->next;\n\n\t\tkfree_skb(qp->q.fragments);\n\t\tqp->q.fragments = head;\n\t}\n\n\tWARN_ON(head == NULL);\n\tWARN_ON(FRAG_CB(head)->offset != 0);\n\n\t/* Allocate a new buffer for the datagram. */\n\tihlen = ip_hdrlen(head);\n\tlen = ihlen + qp->q.len;\n\n\terr = -E2BIG;\n\tif (len > 65535)\n\t\tgoto out_oversize;\n\n\t/* Head of list must not be cloned. */\n\tif (skb_cloned(head) && pskb_expand_head(head, 0, 0, GFP_ATOMIC))\n\t\tgoto out_nomem;\n\n\t/* If the first fragment is fragmented itself, we split\n\t * it to two chunks: the first with data and paged part\n\t * and the second, holding only fragments. */\n\tif (skb_has_frag_list(head)) {\n\t\tstruct sk_buff *clone;\n\t\tint i, plen = 0;\n\n\t\tif ((clone = alloc_skb(0, GFP_ATOMIC)) == NULL)\n\t\t\tgoto out_nomem;\n\t\tclone->next = head->next;\n\t\thead->next = clone;\n\t\tskb_shinfo(clone)->frag_list = skb_shinfo(head)->frag_list;\n\t\tskb_frag_list_init(head);\n\t\tfor (i = 0; i < skb_shinfo(head)->nr_frags; i++)\n\t\t\tplen += skb_frag_size(&skb_shinfo(head)->frags[i]);\n\t\tclone->len = clone->data_len = head->data_len - plen;\n\t\thead->data_len -= clone->len;\n\t\thead->len -= clone->len;\n\t\tclone->csum = 0;\n\t\tclone->ip_summed = head->ip_summed;\n\t\tatomic_add(clone->truesize, &qp->q.net->mem);\n\t}\n\n\tskb_shinfo(head)->frag_list = head->next;\n\tskb_push(head, head->data - skb_network_header(head));\n\n\tfor (fp=head->next; fp; fp = fp->next) {\n\t\thead->data_len += fp->len;\n\t\thead->len += fp->len;\n\t\tif (head->ip_summed != fp->ip_summed)\n\t\t\thead->ip_summed = CHECKSUM_NONE;\n\t\telse if (head->ip_summed == CHECKSUM_COMPLETE)\n\t\t\thead->csum = csum_add(head->csum, fp->csum);\n\t\thead->truesize += fp->truesize;\n\t}\n\tatomic_sub(head->truesize, &qp->q.net->mem);\n\n\thead->next = NULL;\n\thead->dev = dev;\n\thead->tstamp = qp->q.stamp;\n\n\tiph = ip_hdr(head);\n\tiph->frag_off = 0;\n\tiph->tot_len = htons(len);\n\tiph->tos |= ecn;\n\tip_send_check(iph);\n\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMOKS);\n\tqp->q.fragments = NULL;\n\tqp->q.fragments_tail = NULL;\n\treturn 0;\n\nout_nomem:\n\tLIMIT_NETDEBUG(KERN_ERR \"IP: queue_glue: no memory for gluing \"\n\t\t\t      \"queue %p\\n\", qp);\n\terr = -ENOMEM;\n\tgoto out_fail;\nout_oversize:\n\tif (net_ratelimit())\n\t\tprintk(KERN_INFO \"Oversized IP packet from %pI4.\\n\",\n\t\t\t&qp->saddr);\nout_fail:\n\tIP_INC_STATS_BH(net, IPSTATS_MIB_REASMFAILS);\n\treturn err;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void xs_reset_transport(struct sock_xprt *transport)\n{\n\tstruct socket *sock = transport->sock;\n\tstruct sock *sk = transport->inet;\n\tstruct rpc_xprt *xprt = &transport->xprt;\n\n\tif (sk == NULL)\n\t\treturn;\n\n\tif (atomic_read(&transport->xprt.swapper))\n\t\tsk_clear_memalloc(sk);\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\ttransport->inet = NULL;\n\ttransport->sock = NULL;\n\n\tsk->sk_user_data = NULL;\n\n\txs_restore_old_callbacks(transport, sk);\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\txs_sock_reset_connection_flags(xprt);\n\n\ttrace_rpc_socket_close(xprt, sock);\n\tsock_release(sock);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void xs_reset_transport(struct sock_xprt *transport)\n{\n\tstruct socket *sock = transport->sock;\n\tstruct sock *sk = transport->inet;\n\tstruct rpc_xprt *xprt = &transport->xprt;\n\n\tif (sk == NULL)\n\t\treturn;\n\n\tif (atomic_read(&transport->xprt.swapper))\n\t\tsk_clear_memalloc(sk);\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\ttransport->inet = NULL;\n\ttransport->sock = NULL;\n\n\tsk->sk_user_data = NULL;\n\n\txs_restore_old_callbacks(transport, sk);\n\txprt_clear_connected(xprt);\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\txs_sock_reset_connection_flags(xprt);\n\n\ttrace_rpc_socket_close(xprt, sock);\n\tsock_release(sock);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void xs_reset_transport(struct sock_xprt *transport)\n{\n\tstruct socket *sock = transport->sock;\n\tstruct sock *sk = transport->inet;\n\n\tif (sk == NULL)\n\t\treturn;\n\n\ttransport->srcport = 0;\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\ttransport->inet = NULL;\n\ttransport->sock = NULL;\n\n\tsk->sk_user_data = NULL;\n\n\txs_restore_old_callbacks(transport, sk);\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\n\tsk->sk_no_check = 0;\n\n\tsock_release(sock);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void xs_reset_transport(struct sock_xprt *transport)\n{\n\tstruct socket *sock = transport->sock;\n\tstruct sock *sk = transport->inet;\n\tstruct rpc_xprt *xprt = &transport->xprt;\n\n\tif (sk == NULL)\n\t\treturn;\n\n\ttransport->srcport = 0;\n\n\twrite_lock_bh(&sk->sk_callback_lock);\n\ttransport->inet = NULL;\n\ttransport->sock = NULL;\n\n\tsk->sk_user_data = NULL;\n\n\txs_restore_old_callbacks(transport, sk);\n\txprt_clear_connected(xprt);\n\twrite_unlock_bh(&sk->sk_callback_lock);\n\n\tsk->sk_no_check = 0;\n\n\tsock_release(sock);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void pool_postsuspend(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tcancel_delayed_work(&pool->waker);\n\tcancel_delayed_work(&pool->no_space_timeout);\n\tflush_workqueue(pool->wq);\n\t(void) commit(pool);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void pool_postsuspend(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tcancel_delayed_work_sync(&pool->waker);\n\tcancel_delayed_work_sync(&pool->no_space_timeout);\n\tflush_workqueue(pool->wq);\n\t(void) commit(pool);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic void pool_postsuspend(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tcancel_delayed_work(&pool->waker);\n\tflush_workqueue(pool->wq);\n\t(void) commit_or_fallback(pool);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void pool_postsuspend(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tcancel_delayed_work_sync(&pool->waker);\n\tflush_workqueue(pool->wq);\n\t(void) commit_or_fallback(pool);\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int fastop(struct x86_emulate_ctxt *ctxt, void (*fop)(struct fastop *))\n{\n\tulong flags = (ctxt->eflags & EFLAGS_MASK) | X86_EFLAGS_IF;\n\n\tif (!(ctxt->d & ByteOp))\n\t\tfop += __ffs(ctxt->dst.bytes) * FASTOP_SIZE;\n\n\tasm(\"push %[flags]; popf; call *%[fastop]; pushf; pop %[flags]\\n\"\n\t    : \"+a\"(ctxt->dst.val), \"+d\"(ctxt->src.val), [flags]\"+D\"(flags),\n\t      [fastop]\"+S\"(fop), ASM_CALL_CONSTRAINT\n\t    : \"c\"(ctxt->src2.val));\n\n\tctxt->eflags = (ctxt->eflags & ~EFLAGS_MASK) | (flags & EFLAGS_MASK);\n\tif (!fop) /* exception is returned in fop variable */\n\t\treturn emulate_de(ctxt);\n\treturn X86EMUL_CONTINUE;\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\n#include <asm/nospec-branch.h>\n\n\nstatic int fastop(struct x86_emulate_ctxt *ctxt, void (*fop)(struct fastop *))\n{\n\tulong flags = (ctxt->eflags & EFLAGS_MASK) | X86_EFLAGS_IF;\n\n\tif (!(ctxt->d & ByteOp))\n\t\tfop += __ffs(ctxt->dst.bytes) * FASTOP_SIZE;\n\n\tasm(\"push %[flags]; popf; \" CALL_NOSPEC \" ; pushf; pop %[flags]\\n\"\n\t    : \"+a\"(ctxt->dst.val), \"+d\"(ctxt->src.val), [flags]\"+D\"(flags),\n\t      [thunk_target]\"+S\"(fop), ASM_CALL_CONSTRAINT\n\t    : \"c\"(ctxt->src2.val));\n\n\tctxt->eflags = (ctxt->eflags & ~EFLAGS_MASK) | (flags & EFLAGS_MASK);\n\tif (!fop) /* exception is returned in fop variable */\n\t\treturn emulate_de(ctxt);\n\treturn X86EMUL_CONTINUE;\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic u8 test_cc(unsigned int condition, unsigned long flags)\n{\n\tu8 rc;\n\tvoid (*fop)(void) = (void *)em_setcc + 4 * (condition & 0xf);\n\n\tflags = (flags & EFLAGS_MASK) | X86_EFLAGS_IF;\n\tasm(\"push %[flags]; popf; call *%[fastop]\"\n\t    : \"=a\"(rc) : [fastop]\"r\"(fop), [flags]\"r\"(flags));\n\treturn rc;\n}\n\nstatic int fastop(struct x86_emulate_ctxt *ctxt, void (*fop)(struct fastop *))\n{\n\tulong flags = (ctxt->eflags & EFLAGS_MASK) | X86_EFLAGS_IF;\n\tif (!(ctxt->d & ByteOp))\n\t\tfop += __ffs(ctxt->dst.bytes) * FASTOP_SIZE;\n\tasm(\"push %[flags]; popf; call *%[fastop]; pushf; pop %[flags]\\n\"\n\t    : \"+a\"(ctxt->dst.val), \"+d\"(ctxt->src.val), [flags]\"+D\"(flags),\n\t      [fastop]\"+S\"(fop)\n\t    : \"c\"(ctxt->src2.val));\n\tctxt->eflags = (ctxt->eflags & ~EFLAGS_MASK) | (flags & EFLAGS_MASK);\n\tif (!fop) /* exception is returned in fop variable */\n\t\treturn emulate_de(ctxt);\n\treturn X86EMUL_CONTINUE;\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\n#include <asm/nospec-branch.h>\n\n\nstatic u8 test_cc(unsigned int condition, unsigned long flags)\n{\n\tu8 rc;\n\tvoid (*fop)(void) = (void *)em_setcc + 4 * (condition & 0xf);\n\n\tflags = (flags & EFLAGS_MASK) | X86_EFLAGS_IF;\n\tasm(\"push %[flags]; popf; \" CALL_NOSPEC\n\t    : \"=a\"(rc) : [thunk_target]\"r\"(fop), [flags]\"r\"(flags));\n\treturn rc;\n}\n\nstatic int fastop(struct x86_emulate_ctxt *ctxt, void (*fop)(struct fastop *))\n{\n\tulong flags = (ctxt->eflags & EFLAGS_MASK) | X86_EFLAGS_IF;\n\tif (!(ctxt->d & ByteOp))\n\t\tfop += __ffs(ctxt->dst.bytes) * FASTOP_SIZE;\n\tasm(\"push %[flags]; popf; \" CALL_NOSPEC \" ; pushf; pop %[flags]\\n\"\n\t    : \"+a\"(ctxt->dst.val), \"+d\"(ctxt->src.val), [flags]\"+D\"(flags),\n\t      [thunk_target]\"+S\"(fop), ASM_CALL_CONSTRAINT\n\t    : \"c\"(ctxt->src2.val));\n\tctxt->eflags = (ctxt->eflags & ~EFLAGS_MASK) | (flags & EFLAGS_MASK);\n\tif (!fop) /* exception is returned in fop variable */\n\t\treturn emulate_de(ctxt);\n\treturn X86EMUL_CONTINUE;\n}'''\n"}]}
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "\u539f\u59cb\u4ee3\u7801 (A \u7248\u672c) \u5b58\u5728\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic int autofs4_write(struct autofs_sb_info *sbi,\n\t\t\t struct file *file, const void *addr, int bytes)\n{\n\tunsigned long sigpipe, flags;\n\tconst char *data = (const char *)addr;\n\tssize_t wr = 0;\n\n\tsigpipe = sigismember(&current->pending.signal, SIGPIPE);\n\n\tmutex_lock(&sbi->pipe_mutex);\n\twhile (bytes) {\n\t\twr = __kernel_write(file, data, bytes, &file->f_pos);\n\t\tif (wr <= 0)\n\t\t\tbreak;\n\t\tdata += wr;\n\t\tbytes -= wr;\n\t}\n\tmutex_unlock(&sbi->pipe_mutex);\n\n\t/* Keep the currently executing process from receiving a\n\t * SIGPIPE unless it was already supposed to get one\n\t */\n\tif (wr == -EPIPE && !sigpipe) {\n\t\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\t\tsigdelset(&current->pending.signal, SIGPIPE);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n\t}\n\n\t/* if 'wr' returned 0 (impossible) we assume -EIO (safe) */\n\treturn bytes == 0 ? 0 : wr < 0 ? wr : -EIO;\n}\n\nstatic void autofs4_notify_daemon(struct autofs_sb_info *sbi,\n\t\t\t\t struct autofs_wait_queue *wq,\n\t\t\t\t int type)\n{\n\tunion {\n\t\tstruct autofs_packet_hdr hdr;\n\t\tunion autofs_packet_union v4_pkt;\n\t\tunion autofs_v5_packet_union v5_pkt;\n\t} pkt;\n\tstruct file *pipe = NULL;\n\tsize_t pktsz;\n\tint ret;\n\n\tpr_debug(\"wait id = 0x%08lx, name = %.*s, type=%d\\n\",\n\t\t (unsigned long) wq->wait_queue_token,\n\t\t wq->name.len, wq->name.name, type);\n\n\tmemset(&pkt, 0, sizeof(pkt)); /* For security reasons */\n\n\tpkt.hdr.proto_version = sbi->version;\n\tpkt.hdr.type = type;\n\n\tswitch (type) {\n\t/* Kernel protocol v4 missing and expire packets */\n\tcase autofs_ptype_missing:\n\t{\n\t\tstruct autofs_packet_missing *mp = &pkt.v4_pkt.missing;\n\n\t\tpktsz = sizeof(*mp);\n\n\t\tmp->wait_queue_token = wq->wait_queue_token;\n\t\tmp->len = wq->name.len;\n\t\tmemcpy(mp->name, wq->name.name, wq->name.len);\n\t\tmp->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\tcase autofs_ptype_expire_multi:\n\t{\n\t\tstruct autofs_packet_expire_multi *ep =\n\t\t\t\t\t&pkt.v4_pkt.expire_multi;\n\n\t\tpktsz = sizeof(*ep);\n\n\t\tep->wait_queue_token = wq->wait_queue_token;\n\t\tep->len = wq->name.len;\n\t\tmemcpy(ep->name, wq->name.name, wq->name.len);\n\t\tep->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\t/*\n\t * Kernel protocol v5 packet for handling indirect and direct\n\t * mount missing and expire requests\n\t */\n\tcase autofs_ptype_missing_indirect:\n\tcase autofs_ptype_expire_indirect:\n\tcase autofs_ptype_missing_direct:\n\tcase autofs_ptype_expire_direct:\n\t{\n\t\tstruct autofs_v5_packet *packet = &pkt.v5_pkt.v5_packet;\n\t\tstruct user_namespace *user_ns = sbi->pipe->f_cred->user_ns;\n\n\t\tpktsz = sizeof(*packet);\n\n\t\tpacket->wait_queue_token = wq->wait_queue_token;\n\t\tpacket->len = wq->name.len;\n\t\tmemcpy(packet->name, wq->name.name, wq->name.len);\n\t\tpacket->name[wq->name.len] = '\\0';\n\t\tpacket->dev = wq->dev;\n\t\tpacket->ino = wq->ino;\n\t\tpacket->uid = from_kuid_munged(user_ns, wq->uid);\n\t\tpacket->gid = from_kgid_munged(user_ns, wq->gid);\n\t\tpacket->pid = wq->pid;\n\t\tpacket->tgid = wq->tgid;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpr_warn(\"bad type %d!\\n\", type);\n\t\tmutex_unlock(&sbi->wq_mutex);\n\t\treturn;\n\t}\n\n\tpipe = get_file(sbi->pipe);\n\n\tmutex_unlock(&sbi->wq_mutex);\n\n\tif (autofs4_write(sbi, pipe, &pkt, pktsz))\n\tswitch (ret = autofs4_write(sbi, pipe, &pkt, pktsz)) {\n\tcase 0:\n\t\tbreak;\n\tcase -ENOMEM:\n\tcase -ERESTARTSYS:\n\t\t/* Just fail this one */\n\t\tautofs4_wait_release(sbi, wq->wait_queue_token, ret);\n\t\tbreak;\n\tdefault:\n\t\tautofs4_catatonic_mode(sbi);\n\t\tbreak;\n\t}\n\tfput(pipe);\n}'''\n\u4fee\u590d\u4ee3\u7801 (B \u7248\u672c) \u4fee\u590d\u4e86\u8be5\u6f0f\u6d1e\uff1a\n'''c\n\n\nstatic void autofs4_notify_daemon(struct autofs_sb_info *sbi,\n\t\t\t\t struct autofs_wait_queue *wq,\n\t\t\t\t int type)\n{\n\tunion {\n\t\tstruct autofs_packet_hdr hdr;\n\t\tunion autofs_packet_union v4_pkt;\n\t\tunion autofs_v5_packet_union v5_pkt;\n\t} pkt;\n\tstruct file *pipe = NULL;\n\tsize_t pktsz;\n\tint ret;\n\n\tpr_debug(\"wait id = 0x%08lx, name = %.*s, type=%d\\n\",\n\t\t (unsigned long) wq->wait_queue_token,\n\t\t wq->name.len, wq->name.name, type);\n\n\tmemset(&pkt, 0, sizeof(pkt)); /* For security reasons */\n\n\tpkt.hdr.proto_version = sbi->version;\n\tpkt.hdr.type = type;\n\n\tswitch (type) {\n\t/* Kernel protocol v4 missing and expire packets */\n\tcase autofs_ptype_missing:\n\t{\n\t\tstruct autofs_packet_missing *mp = &pkt.v4_pkt.missing;\n\n\t\tpktsz = sizeof(*mp);\n\n\t\tmp->wait_queue_token = wq->wait_queue_token;\n\t\tmp->len = wq->name.len;\n\t\tmemcpy(mp->name, wq->name.name, wq->name.len);\n\t\tmp->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\tcase autofs_ptype_expire_multi:\n\t{\n\t\tstruct autofs_packet_expire_multi *ep =\n\t\t\t\t\t&pkt.v4_pkt.expire_multi;\n\n\t\tpktsz = sizeof(*ep);\n\n\t\tep->wait_queue_token = wq->wait_queue_token;\n\t\tep->len = wq->name.len;\n\t\tmemcpy(ep->name, wq->name.name, wq->name.len);\n\t\tep->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\t/*\n\t * Kernel protocol v5 packet for handling indirect and direct\n\t * mount missing and expire requests\n\t */\n\tcase autofs_ptype_missing_indirect:\n\tcase autofs_ptype_expire_indirect:\n\tcase autofs_ptype_missing_direct:\n\tcase autofs_ptype_expire_direct:\n\t{\n\t\tstruct autofs_v5_packet *packet = &pkt.v5_pkt.v5_packet;\n\t\tstruct user_namespace *user_ns = sbi->pipe->f_cred->user_ns;\n\n\t\tpktsz = sizeof(*packet);\n\n\t\tpacket->wait_queue_token = wq->wait_queue_token;\n\t\tpacket->len = wq->name.len;\n\t\tmemcpy(packet->name, wq->name.name, wq->name.len);\n\t\tpacket->name[wq->name.len] = '\\0';\n\t\tpacket->dev = wq->dev;\n\t\tpacket->ino = wq->ino;\n\t\tpacket->uid = from_kuid_munged(user_ns, wq->uid);\n\t\tpacket->gid = from_kgid_munged(user_ns, wq->gid);\n\t\tpacket->pid = wq->pid;\n\t\tpacket->tgid = wq->tgid;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpr_warn(\"bad type %d!\\n\", type);\n\t\tmutex_unlock(&sbi->wq_mutex);\n\t\treturn;\n\t}\n\n\tpipe = get_file(sbi->pipe);\n\n\tmutex_unlock(&sbi->wq_mutex);\n\n\tswitch (ret = autofs4_write(sbi, pipe, &pkt, pktsz)) {\n\tcase 0:\n\t\tbreak;\n\tcase -ENOMEM:\n\tcase -ERESTARTSYS:\n\t\t/* Just fail this one */\n\t\tautofs4_wait_release(sbi, wq->wait_queue_token, ret);\n\t\tbreak;\n\tdefault:\n\t\tautofs4_catatonic_mode(sbi);\n\t\tbreak;\n\t}\n\tfput(pipe);\n}'''\n\u6700\u65b0\u4ee3\u7801 (C \u7248\u672c) \u662f\uff1a\n'''c\n\n\nstatic int autofs4_write(struct file *file, const void *addr, int bytes)\n{\n\tunsigned long sigpipe, flags;\n\tmm_segment_t fs;\n\tconst char *data = (const char *)addr;\n\tssize_t wr = 0;\n\n\t/** WARNING: this is not safe for writing more than PIPE_BUF bytes! **/\n\n\tsigpipe = sigismember(&current->pending.signal, SIGPIPE);\n\n\t/* Save pointer to user space and point back to kernel space */\n\tfs = get_fs();\n\tset_fs(KERNEL_DS);\n\n\twhile (bytes &&\n\t       (wr = file->f_op->write(file,data,bytes,&file->f_pos)) > 0) {\n\t\tdata += wr;\n\t\tbytes -= wr;\n\t}\n\n\tset_fs(fs);\n\n\t/* Keep the currently executing process from receiving a\n\t   SIGPIPE unless it was already supposed to get one */\n\tif (wr == -EPIPE && !sigpipe) {\n\t\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\t\tsigdelset(&current->pending.signal, SIGPIPE);\n\t\trecalc_sigpending();\n\t\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n\t}\n\n\t/* if 'wr' returned 0 (impossible) we assume -EIO (safe) */\n\treturn bytes == 0 ? 0 : wr < 0 ? wr : -EIO;\n}\n\nstatic void autofs4_notify_daemon(struct autofs_sb_info *sbi,\n\t\t\t\t struct autofs_wait_queue *wq,\n\t\t\t\t int type)\n{\n\tunion {\n\t\tstruct autofs_packet_hdr hdr;\n\t\tunion autofs_packet_union v4_pkt;\n\t\tunion autofs_v5_packet_union v5_pkt;\n\t} pkt;\n\tstruct file *pipe = NULL;\n\tsize_t pktsz;\n\tint ret;\n\n\tDPRINTK(\"wait id = 0x%08lx, name = %.*s, type=%d\",\n\t\t(unsigned long) wq->wait_queue_token, wq->name.len, wq->name.name, type);\n\n\tmemset(&pkt,0,sizeof pkt); /* For security reasons */\n\n\tpkt.hdr.proto_version = sbi->version;\n\tpkt.hdr.type = type;\n\tmutex_lock(&sbi->wq_mutex);\n\n\t/* Check if we have become catatonic */\n\tif (sbi->catatonic) {\n\t\tmutex_unlock(&sbi->wq_mutex);\n\t\treturn;\n\t}\n\tswitch (type) {\n\t/* Kernel protocol v4 missing and expire packets */\n\tcase autofs_ptype_missing:\n\t{\n\t\tstruct autofs_packet_missing *mp = &pkt.v4_pkt.missing;\n\n\t\tpktsz = sizeof(*mp);\n\n\t\tmp->wait_queue_token = wq->wait_queue_token;\n\t\tmp->len = wq->name.len;\n\t\tmemcpy(mp->name, wq->name.name, wq->name.len);\n\t\tmp->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\tcase autofs_ptype_expire_multi:\n\t{\n\t\tstruct autofs_packet_expire_multi *ep = &pkt.v4_pkt.expire_multi;\n\n\t\tpktsz = sizeof(*ep);\n\n\t\tep->wait_queue_token = wq->wait_queue_token;\n\t\tep->len = wq->name.len;\n\t\tmemcpy(ep->name, wq->name.name, wq->name.len);\n\t\tep->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\t/*\n\t * Kernel protocol v5 packet for handling indirect and direct\n\t * mount missing and expire requests\n\t */\n\tcase autofs_ptype_missing_indirect:\n\tcase autofs_ptype_expire_indirect:\n\tcase autofs_ptype_missing_direct:\n\tcase autofs_ptype_expire_direct:\n\t{\n\t\tstruct autofs_v5_packet *packet = &pkt.v5_pkt.v5_packet;\n\n\t\tpktsz = sizeof(*packet);\n\n\t\tpacket->wait_queue_token = wq->wait_queue_token;\n\t\tpacket->len = wq->name.len;\n\t\tmemcpy(packet->name, wq->name.name, wq->name.len);\n\t\tpacket->name[wq->name.len] = '\\0';\n\t\tpacket->dev = wq->dev;\n\t\tpacket->ino = wq->ino;\n\t\tpacket->uid = wq->uid;\n\t\tpacket->gid = wq->gid;\n\t\tpacket->pid = wq->pid;\n\t\tpacket->tgid = wq->tgid;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tprintk(\"autofs4_notify_daemon: bad type %d!\\n\", type);\n\t\tmutex_unlock(&sbi->wq_mutex);\n\t\treturn;\n\t}\n\n\tpipe = sbi->pipe;\n\tget_file(pipe);\n\n\tmutex_unlock(&sbi->wq_mutex);\n\n\tif (autofs4_write(pipe, &pkt, pktsz))\n\tswitch (ret = autofs4_write(pipe, &pkt, pktsz)) {\n\tcase 0:\n\t\tbreak;\n\tcase -ENOMEM:\n\tcase -ERESTARTSYS:\n\t\t/* Just fail this one */\n\t\tautofs4_wait_release(sbi, wq->wait_queue_token, ret);\n\t\tbreak;\n\tdefault:\n\t\tautofs4_catatonic_mode(sbi);\n\t\tbreak;\n\t}\n\tfput(pipe);\n}'''\n\u8bf7\u5c06 A \u7248\u672c\u5230 B \u7248\u672c\u7684\u8865\u4e01\u5e94\u7528\u5230 C \u7248\u672c\u3002"}, {"role": "assistant", "content": "\u5c06\u8865\u4e01\u5e94\u7528\u5230\u6700\u65b0\u4ee3\u7801 (C\u7248\u672c) \u540e\u7684\u7ed3\u679c\uff1a\n'''c\n\n\nstatic void autofs4_notify_daemon(struct autofs_sb_info *sbi,\n\t\t\t\t struct autofs_wait_queue *wq,\n\t\t\t\t int type)\n{\n\tunion {\n\t\tstruct autofs_packet_hdr hdr;\n\t\tunion autofs_packet_union v4_pkt;\n\t\tunion autofs_v5_packet_union v5_pkt;\n\t} pkt;\n\tstruct file *pipe = NULL;\n\tsize_t pktsz;\n\tint ret;\n\n\tDPRINTK(\"wait id = 0x%08lx, name = %.*s, type=%d\",\n\t\t(unsigned long) wq->wait_queue_token, wq->name.len, wq->name.name, type);\n\n\tmemset(&pkt,0,sizeof pkt); /* For security reasons */\n\n\tpkt.hdr.proto_version = sbi->version;\n\tpkt.hdr.type = type;\n\tmutex_lock(&sbi->wq_mutex);\n\n\t/* Check if we have become catatonic */\n\tif (sbi->catatonic) {\n\t\tmutex_unlock(&sbi->wq_mutex);\n\t\treturn;\n\t}\n\tswitch (type) {\n\t/* Kernel protocol v4 missing and expire packets */\n\tcase autofs_ptype_missing:\n\t{\n\t\tstruct autofs_packet_missing *mp = &pkt.v4_pkt.missing;\n\n\t\tpktsz = sizeof(*mp);\n\n\t\tmp->wait_queue_token = wq->wait_queue_token;\n\t\tmp->len = wq->name.len;\n\t\tmemcpy(mp->name, wq->name.name, wq->name.len);\n\t\tmp->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\tcase autofs_ptype_expire_multi:\n\t{\n\t\tstruct autofs_packet_expire_multi *ep = &pkt.v4_pkt.expire_multi;\n\n\t\tpktsz = sizeof(*ep);\n\n\t\tep->wait_queue_token = wq->wait_queue_token;\n\t\tep->len = wq->name.len;\n\t\tmemcpy(ep->name, wq->name.name, wq->name.len);\n\t\tep->name[wq->name.len] = '\\0';\n\t\tbreak;\n\t}\n\t/*\n\t * Kernel protocol v5 packet for handling indirect and direct\n\t * mount missing and expire requests\n\t */\n\tcase autofs_ptype_missing_indirect:\n\tcase autofs_ptype_expire_indirect:\n\tcase autofs_ptype_missing_direct:\n\tcase autofs_ptype_expire_direct:\n\t{\n\t\tstruct autofs_v5_packet *packet = &pkt.v5_pkt.v5_packet;\n\n\t\tpktsz = sizeof(*packet);\n\n\t\tpacket->wait_queue_token = wq->wait_queue_token;\n\t\tpacket->len = wq->name.len;\n\t\tmemcpy(packet->name, wq->name.name, wq->name.len);\n\t\tpacket->name[wq->name.len] = '\\0';\n\t\tpacket->dev = wq->dev;\n\t\tpacket->ino = wq->ino;\n\t\tpacket->uid = wq->uid;\n\t\tpacket->gid = wq->gid;\n\t\tpacket->pid = wq->pid;\n\t\tpacket->tgid = wq->tgid;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tprintk(\"autofs4_notify_daemon: bad type %d!\\n\", type);\n\t\tmutex_unlock(&sbi->wq_mutex);\n\t\treturn;\n\t}\n\n\tpipe = sbi->pipe;\n\tget_file(pipe);\n\n\tmutex_unlock(&sbi->wq_mutex);\n\n\tswitch (ret = autofs4_write(pipe, &pkt, pktsz)) {\n\tcase 0:\n\t\tbreak;\n\tcase -ENOMEM:\n\tcase -ERESTARTSYS:\n\t\t/* Just fail this one */\n\t\tautofs4_wait_release(sbi, wq->wait_queue_token, ret);\n\t\tbreak;\n\tdefault:\n\t\tautofs4_catatonic_mode(sbi);\n\t\tbreak;\n\t}\n\tfput(pipe);\n}'''\n"}]}